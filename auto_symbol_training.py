#!/usr/bin/env python3
"""
éŠ˜æŸ„è¿½åŠ æ™‚ã®è‡ªå‹•å­¦ç¿’ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
éŠ˜æŸ„ã‚’è¿½åŠ ã™ã‚‹ã¨å…¨æ™‚é–“è¶³ãƒ»å…¨æˆ¦ç•¥ã§è‡ªå‹•åˆ†æã‚’å®Ÿè¡Œ
"""

import sys
import os
import asyncio
import uuid
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import traceback

# ãƒ‘ã‚¹è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from real_time_system.utils.colored_log import get_colored_logger
from scalable_analysis_system import ScalableAnalysisSystem
from execution_log_database import ExecutionLogDatabase, ExecutionType, ExecutionStatus
from engines.leverage_decision_engine import InsufficientMarketDataError, InsufficientConfigurationError, LeverageAnalysisError
from engines.filtering_framework import FilteringFramework, FilteringStatistics
from symbol_early_fail_validator import SymbolEarlyFailValidator

# progress_trackerçµ±åˆ
try:
    from web_dashboard.analysis_progress import progress_tracker
    PROGRESS_TRACKER_AVAILABLE = True
    print("âœ… progress_tracker ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    PROGRESS_TRACKER_AVAILABLE = False
    print(f"âš ï¸ progress_tracker ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


class AutoSymbolTrainer:
    """éŠ˜æŸ„è¿½åŠ æ™‚ã®è‡ªå‹•å­¦ç¿’ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = get_colored_logger(__name__)
        # Force use of root large_scale_analysis directory to prevent DB fragmentation
        self.analysis_system = ScalableAnalysisSystem("large_scale_analysis") 
        self.execution_db = ExecutionLogDatabase()
        # å®Ÿè¡Œãƒ­ã‚°ã®ä¸€æ™‚ä¿å­˜ã¯å»ƒæ­¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ï¼‰
        
        # Early Failæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.early_fail_validator = SymbolEarlyFailValidator()
        self.logger.info("âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        
    async def add_symbol_with_training(self, symbol: str, execution_id: str = None, selected_strategies: list = None, selected_timeframes: list = None, strategy_configs: list = None, skip_pretask_creation: bool = False, custom_period_settings: dict = None, filter_params: dict = None) -> str:
        """
        éŠ˜æŸ„ã‚’è¿½åŠ ã—ã¦æŒ‡å®šæˆ¦ç•¥ãƒ»æ™‚é–“è¶³ã§è‡ªå‹•å­¦ç¿’ãƒ»ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        Args:
            symbol: éŠ˜æŸ„å (ä¾‹: "HYPE")
            execution_id: äº‹å‰å®šç¾©ã•ã‚ŒãŸå®Ÿè¡ŒIDï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            selected_strategies: é¸æŠã•ã‚ŒãŸæˆ¦ç•¥ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            selected_timeframes: é¸æŠã•ã‚ŒãŸæ™‚é–“è¶³ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            strategy_configs: ã‚«ã‚¹ã‚¿ãƒ æˆ¦ç•¥è¨­å®šãƒªã‚¹ãƒˆï¼ˆstrategy_configurationsãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰
            skip_pretask_creation: Pre-taskä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆæ—¢ã«ä½œæˆæ¸ˆã¿ã®å ´åˆï¼‰
            
        Returns:
            execution_id: å®Ÿè¡ŒIDï¼ˆé€²æ—è¿½è·¡ç”¨ï¼‰
        """
        try:
            self.logger.info(f"Starting automatic training for symbol: {symbol}")
            
            # jsonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é–¢æ•°å†…ã§ã‚‚ç¢ºå®Ÿã«åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹
            import json
            
            # ğŸš€ Early Failæ¤œè¨¼å®Ÿè¡Œ
            self.logger.info(f"ğŸ” Early Failæ¤œè¨¼é–‹å§‹: {symbol}")
            early_fail_result = await self._run_early_fail_validation(symbol)
            if not early_fail_result.passed:
                error_msg = f"Early Failæ¤œè¨¼å¤±æ•—: {early_fail_result.fail_reason} - {early_fail_result.error_message}"
                self.logger.error(error_msg)
                raise ValueError(error_msg)
            
            self.logger.info(f"âœ… Early Failæ¤œè¨¼åˆæ ¼: {symbol}")
            
            # é‡è¤‡å®Ÿè¡Œãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜execution_idã¯é™¤å¤–ï¼‰
            existing_executions = self.execution_db.list_executions(limit=20)
            running_symbols = [
                exec_item for exec_item in existing_executions 
                if (exec_item.get('status') == 'RUNNING' and 
                    exec_item.get('symbol') == symbol and 
                    exec_item.get('execution_id') != execution_id)  # åŒã˜execution_idã¯é™¤å¤–
            ]
            
            if running_symbols:
                error_msg = f"Symbol {symbol} is already being processed. Cancel existing execution first."
                self.logger.error(error_msg)
                raise ValueError(error_msg)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å®Ÿè¡Œè¨˜éŒ²ã‚’ä½œæˆ
            if execution_id is None:
                execution_id = self.execution_db.create_execution(
                    ExecutionType.SYMBOL_ADDITION,
                    symbol=symbol,
                    triggered_by="USER",
                    metadata={"auto_training": True, "all_strategies": True, "all_timeframes": True}
                )
            else:
                # äº‹å‰å®šç¾©ã•ã‚ŒãŸIDã®å ´åˆã€æ—¢å­˜ã®è¨˜éŒ²ã‚’ãƒã‚§ãƒƒã‚¯
                existing_record = self.execution_db.get_execution(execution_id)
                if not existing_record:
                    # è¨˜éŒ²ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ä½œæˆ
                    self.execution_db.create_execution_with_id(
                        execution_id,
                        ExecutionType.SYMBOL_ADDITION,
                        symbol=symbol,
                        triggered_by="USER",
                        metadata={
                            "auto_training": True, 
                            "selected_strategies": selected_strategies or "all",
                            "selected_timeframes": selected_timeframes or "all",
                            "custom_strategy_configs": len(strategy_configs) if strategy_configs else 0
                        }
                    )
                    self.logger.info(f"ğŸ“ Created new execution record: {execution_id}")
                else:
                    self.logger.info(f"ğŸ“‹ Using existing execution record: {execution_id}")
            
            self.logger.info(f"Execution ID: {execution_id}")
            
            # å®Ÿè¡ŒIDã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆé€²æ—ãƒ­ã‚¬ãƒ¼ç”¨ï¼‰
            self._current_execution_id = execution_id
            
            # å®Ÿè¡ŒIDã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ç”¨ï¼‰
            os.environ['CURRENT_EXECUTION_ID'] = execution_id
            self.logger.info(f"ğŸ“ å®Ÿè¡ŒIDã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š: {execution_id}")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
            if filter_params:
                os.environ['FILTER_PARAMS'] = json.dumps(filter_params)
                self.logger.info(f"ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š: {filter_params}")
            
            # progress_trackeråˆæœŸåŒ–
            if PROGRESS_TRACKER_AVAILABLE:
                self.logger.info(f"ğŸ“Š progress_trackeråˆæœŸåŒ–é–‹å§‹: {symbol}, {execution_id}")
                progress_tracker.start_analysis(symbol, execution_id)
                progress_tracker.update_stage(execution_id, "initializing")
                self.logger.info(f"âœ… progress_trackeråˆæœŸåŒ–å®Œäº†")
            else:
                self.logger.warning("âš ï¸ progress_trackeråˆ©ç”¨ä¸å¯")
            
            # å®Ÿè¡Œé–‹å§‹
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.RUNNING,
                current_operation='é–‹å§‹ä¸­',
                progress_percentage=0,
                total_tasks=4
            )
            
            # Step 1: ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨æ¤œè¨¼
            await self._execute_step(execution_id, 'data_fetch', 
                                   self._fetch_and_validate_data, symbol, custom_period_settings)
            
            # Step 2: é¸æŠã•ã‚ŒãŸæˆ¦ç•¥ãƒ»æ™‚é–“è¶³ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            await self._execute_step(execution_id, 'backtest', 
                                   self._run_comprehensive_backtest, symbol, selected_strategies, selected_timeframes, strategy_configs, skip_pretask_creation, custom_period_settings)
            
            # Step 3: MLå­¦ç¿’å®Ÿè¡Œ
            await self._execute_step(execution_id, 'ml_training', 
                                   self._train_ml_models, symbol)
            
            # Step 4: çµæœä¿å­˜ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ›´æ–°
            await self._execute_step(execution_id, 'result_save', 
                                   self._save_results, symbol)
            
            # å®Ÿè¡Œå®Œäº†å‰ã«åˆ†æçµæœã®å­˜åœ¨ç¢ºèª
            analysis_results_exist = self._verify_analysis_results(symbol, execution_id)
            
            if analysis_results_exist:
                # åˆ†æçµæœãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿SUCCESS
                self.execution_db.update_execution_status(
                    execution_id,
                    ExecutionStatus.SUCCESS,
                    current_operation='å®Œäº†',
                    progress_percentage=100
                )
                
                # progress_trackeræœ€çµ‚æ›´æ–°ï¼ˆæˆåŠŸï¼‰
                if PROGRESS_TRACKER_AVAILABLE:
                    progress_tracker.complete_analysis(execution_id, "signal_detected", "Analysis completed successfully")
                    
                self.logger.success(f"Symbol {symbol} training completed successfully!")
            else:
                # åˆ†æçµæœãŒå­˜åœ¨ã—ãªã„å ´åˆã¯FAILED
                self.execution_db.update_execution_status(
                    execution_id,
                    ExecutionStatus.FAILED,
                    current_operation='åˆ†æçµæœãªã— - å‡¦ç†å¤±æ•—',
                    progress_percentage=100,
                    error_message="No analysis results found despite successful steps"
                )
                
                # progress_trackeræœ€çµ‚æ›´æ–°ï¼ˆå¤±æ•—ï¼‰
                if PROGRESS_TRACKER_AVAILABLE:
                    self.logger.info(f"ğŸ“Š progress_trackerå¤±æ•—æ›´æ–°é–‹å§‹: {execution_id}")
                    progress_tracker.fail_analysis(execution_id, "result_validation", "No analysis results found despite successful execution steps")
                    self.logger.info(f"âœ… progress_trackerå¤±æ•—æ›´æ–°å®Œäº†")
                else:
                    self.logger.warning("âš ï¸ progress_trackeråˆ©ç”¨ä¸å¯ã®ãŸã‚å¤±æ•—æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—")
                    
                self.logger.error(f"âŒ Symbol {symbol} training failed: No analysis results found")
                raise ValueError(f"No analysis results found for {symbol} despite successful execution steps")
            
            return execution_id
            
        except Exception as e:
            self.logger.error(f"Error in symbol training: {e}")
            
            # progress_trackeræœ€çµ‚æ›´æ–°ï¼ˆä¾‹å¤–ç™ºç”Ÿï¼‰
            if PROGRESS_TRACKER_AVAILABLE:
                progress_tracker.fail_analysis(execution_id, "exception", f"Training failed with exception: {str(e)}")
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
            self.execution_db.add_execution_error(execution_id, {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'traceback': traceback.format_exc(),
                'step': 'general'
            })
            
            # å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å¤±æ•—ã«æ›´æ–°
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.FAILED,
                current_operation=f'ã‚¨ãƒ©ãƒ¼: {str(e)}'
            )
            
            raise
    
    async def _execute_step(self, execution_id: str, step_name: str, 
                          step_function, *args, **kwargs):
        """å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ã®å…±é€šå‡¦ç†"""
        try:
            self.logger.info(f"Executing step: {step_name}")
            step_start = datetime.now()
            
            # é€²æ—æ›´æ–°
            exchange = self._get_exchange_from_config()
            step_display_names = {
                'data_fetch': f'{exchange.capitalize()}éŠ˜æŸ„ç¢ºèªãƒ»ãƒ‡ãƒ¼ã‚¿å–å¾—',
                'backtest': 'å…¨æˆ¦ç•¥ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ',
                'ml_training': 'MLå­¦ç¿’å®Ÿè¡Œ',
                'result_save': 'çµæœä¿å­˜ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ›´æ–°'
            }
            
            current_operation = step_display_names.get(step_name, step_name)
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.RUNNING,
                current_operation=current_operation
            )
            
            # progress_trackeræ®µéšæ›´æ–°
            if PROGRESS_TRACKER_AVAILABLE:
                stage_mapping = {
                    'data_fetch': 'data_validation',
                    'backtest': 'backtest_analysis', 
                    'ml_training': 'ml_training',
                    'result_save': 'finalizing'
                }
                stage = stage_mapping.get(step_name, step_name)
                self.logger.info(f"ğŸ“Š progress_trackeræ®µéšæ›´æ–°: {step_name} -> {stage} (execution_id: {execution_id})")
                progress_tracker.update_stage(execution_id, stage)
                self.logger.info(f"âœ… progress_trackeræ®µéšæ›´æ–°å®Œäº†")
            else:
                self.logger.warning(f"âš ï¸ progress_trackeråˆ©ç”¨ä¸å¯ã®ãŸã‚æ®µéšæ›´æ–°ã‚¹ã‚­ãƒƒãƒ—: {step_name}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            result = await step_function(*args, **kwargs)
            
            step_end = datetime.now()
            duration = (step_end - step_start).total_seconds()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†ã‚’è¨˜éŒ²
            self.execution_db.add_execution_step(
                execution_id,
                step_name,
                'SUCCESS',
                result_data=result,
                duration_seconds=duration
            )
            
            self.logger.success(f"Step {step_name} completed in {duration:.2f}s")
            
        except InsufficientMarketDataError as e:
            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
            self.logger.error(f"âŒ å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚ŠéŠ˜æŸ„è¿½åŠ ã‚’ä¸­æ­¢: {e}")
            self.logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {e.error_type}")
            self.logger.error(f"   ä¸è¶³ãƒ‡ãƒ¼ã‚¿: {e.missing_data}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç‰¹åˆ¥ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§è¨˜éŒ²
            self.execution_db.add_execution_step(
                execution_id,
                step_name,
                'FAILED_INSUFFICIENT_DATA',
                error_message=f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {e}",
                error_traceback=traceback.format_exc()
            )
            
            # å®Ÿè¡Œã‚’å¤±æ•—ã¨ã—ã¦çµ‚äº†
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.FAILED,
                current_operation=f'å¸‚å ´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šä¸­æ­¢: {e.error_type}',
                error_message=str(e)
            )
            
            raise  # ä¸Šä½ã«ä¼æ’­ã—ã¦éŠ˜æŸ„è¿½åŠ ã‚’å®Œå…¨ã«åœæ­¢
            
        except InsufficientConfigurationError as e:
            # è¨­å®šä¸è¶³ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
            self.logger.error(f"âŒ è¨­å®šä¸è¶³ã«ã‚ˆã‚ŠéŠ˜æŸ„è¿½åŠ ã‚’ä¸­æ­¢: {e}")
            self.logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {e.error_type}")
            self.logger.error(f"   ä¸è¶³è¨­å®š: {e.missing_config}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç‰¹åˆ¥ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§è¨˜éŒ²
            self.execution_db.add_execution_step(
                execution_id,
                step_name,
                'FAILED_INSUFFICIENT_CONFIG',
                error_message=f"è¨­å®šä¸è¶³: {e}",
                error_traceback=traceback.format_exc()
            )
            
            # å®Ÿè¡Œã‚’å¤±æ•—ã¨ã—ã¦çµ‚äº†
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.FAILED,
                current_operation=f'è¨­å®šä¸è¶³ã«ã‚ˆã‚Šä¸­æ­¢: {e.error_type}',
                error_message=str(e)
            )
            
            raise  # ä¸Šä½ã«ä¼æ’­ã—ã¦éŠ˜æŸ„è¿½åŠ ã‚’å®Œå…¨ã«åœæ­¢
            
        except LeverageAnalysisError as e:
            # ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
            self.logger.error(f"âŒ ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æå¤±æ•—ã«ã‚ˆã‚ŠéŠ˜æŸ„è¿½åŠ ã‚’ä¸­æ­¢: {e}")
            self.logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {e.error_type}")
            self.logger.error(f"   åˆ†ææ®µéš: {e.analysis_stage}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç‰¹åˆ¥ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§è¨˜éŒ²
            self.execution_db.add_execution_step(
                execution_id,
                step_name,
                'FAILED_LEVERAGE_ANALYSIS',
                error_message=f"ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æå¤±æ•—: {e}",
                error_traceback=traceback.format_exc()
            )
            
            # å®Ÿè¡Œã‚’å¤±æ•—ã¨ã—ã¦çµ‚äº†
            self.execution_db.update_execution_status(
                execution_id,
                ExecutionStatus.FAILED,
                current_operation=f'ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æå¤±æ•—ã«ã‚ˆã‚Šä¸­æ­¢: {e.error_type}',
                error_message=str(e)
            )
            
            raise  # ä¸Šä½ã«ä¼æ’­ã—ã¦éŠ˜æŸ„è¿½åŠ ã‚’å®Œå…¨ã«åœæ­¢
            
        except Exception as e:
            self.logger.error(f"Step {step_name} failed: {e}")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—ã‚’è¨˜éŒ²
            self.execution_db.add_execution_step(
                execution_id,
                step_name,
                'FAILED',
                error_message=str(e),
                error_traceback=traceback.format_exc()
            )
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚‚è¿½åŠ 
            self.execution_db.add_execution_error(execution_id, {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'traceback': traceback.format_exc(),
                'step': step_name
            })
            
            raise
    
    def _get_exchange_from_config(self) -> str:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¼•æ‰€ã‚’å–å¾—"""
        import json
        import os
        
        # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        try:
            if os.path.exists('exchange_config.json'):
                with open('exchange_config.json', 'r') as f:
                    config = json.load(f)
                    return config.get('default_exchange', 'hyperliquid').lower()
        except Exception as e:
            self.logger.warning(f"Failed to load exchange config: {e}")
        
        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿
        env_exchange = os.getenv('EXCHANGE_TYPE', '').lower()
        if env_exchange in ['hyperliquid', 'gateio']:
            return env_exchange
        
        # 3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Hyperliquid
        return 'hyperliquid'
    
    async def _fetch_and_validate_data(self, symbol: str, custom_period_settings: dict = None) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨æ¤œè¨¼ï¼ˆHyperliquidãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆï¼‰"""
        
        try:
            # 1. ãƒãƒ«ãƒå–å¼•æ‰€éŠ˜æŸ„ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
            exchange = self._get_exchange_from_config()
            
            self.logger.info(f"Validating {symbol} on {exchange}...")
            
            # ãƒãƒ«ãƒå–å¼•æ‰€å¯¾å¿œãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            from hyperliquid_api_client import MultiExchangeAPIClient
            api_client = MultiExchangeAPIClient(exchange_type=exchange)
            
            validation_result = await api_client.validate_symbol_real(symbol)
            
            if not validation_result['valid']:
                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—æ™‚ã¯æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹
                status = validation_result.get('status', 'unknown')
                reason = validation_result.get('reason', 'Unknown error')
                
                if status == "invalid":
                    raise ValueError(f"{symbol}: {reason}")
                elif status == "inactive":
                    raise ValueError(f"{symbol}: {reason}")
                else:
                    # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                    raise ValueError(f"Validation failed for {symbol}: {reason}")
            
            self.logger.success(f"âœ… {symbol} validated on {exchange}")
            
            # 2. å±¥æ­´ãƒ‡ãƒ¼ã‚¿å–å¾—
            self.logger.info(f"Fetching historical data for {symbol}")
            
            try:
                self.logger.info(f"ğŸš€ STARTING OHLCV DATA VALIDATION for {symbol}")
                self.logger.info(f"ğŸ“… ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šå—ä¿¡: {custom_period_settings}")
                
                # æœŸé–“è¨­å®šã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—
                if custom_period_settings and custom_period_settings.get('mode') == 'custom':
                    self.logger.info(f"ğŸ“… ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šä½¿ç”¨: {custom_period_settings}")
                    
                    # ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ã«200æœ¬å‰ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿å–å¾—
                    from datetime import datetime, timedelta, timezone
                    import dateutil.parser
                    
                    try:
                        start_date_str = custom_period_settings.get('start_date')
                        end_date_str = custom_period_settings.get('end_date')
                        
                        start_time = dateutil.parser.parse(start_date_str).replace(tzinfo=timezone.utc)
                        end_time = dateutil.parser.parse(end_date_str).replace(tzinfo=timezone.utc)
                        
                        # 200æœ¬å‰ãƒ‡ãƒ¼ã‚¿ï¼ˆ1æ™‚é–“è¶³ã§200æ™‚é–“å‰ï¼‰
                        pre_period_hours = 200
                        adjusted_start_time = start_time - timedelta(hours=pre_period_hours)
                        
                        self.logger.info(f"ğŸ“… ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ãƒ‡ãƒ¼ã‚¿å–å¾—: {adjusted_start_time.strftime('%Y-%m-%d %H:%M')} ï½ {end_time.strftime('%Y-%m-%d %H:%M')}")
                        
                        # ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ã§ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
                        ohlcv_data = await api_client.get_ohlcv_data(symbol, '1h', adjusted_start_time, end_time)
                        
                    except Exception as e:
                        self.logger.error(f"ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90æ—¥é–“
                        ohlcv_data = await api_client.get_ohlcv_data_with_period(symbol, '1h', days=90)
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1æ™‚é–“è¶³ã€90æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    ohlcv_data = await api_client.get_ohlcv_data_with_period(symbol, '1h', days=90)
                
                data_info = {
                    'records': len(ohlcv_data),
                    'date_range': {
                        'start': ohlcv_data['timestamp'].min().strftime('%Y-%m-%d') if len(ohlcv_data) > 0 else 'N/A',
                        'end': ohlcv_data['timestamp'].max().strftime('%Y-%m-%d') if len(ohlcv_data) > 0 else 'N/A'
                    }
                }
                
                # 3. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šæ™‚ã¯æŸ”è»Ÿã«èª¿æ•´ï¼‰
                if custom_period_settings and custom_period_settings.get('mode') == 'custom':
                    # ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šæ™‚ã¯æœ€ä½100ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆï¼ˆç´„4æ—¥åˆ†ï¼‰
                    minimum_required = 100
                    if data_info['records'] < minimum_required:
                        raise ValueError(f"{symbol}: Only {data_info['records']} data points available (minimum for custom period: {minimum_required})")
                    self.logger.info(f"âœ… ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ãƒ‡ãƒ¼ã‚¿å“è³ªOK: {data_info['records']}ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“ï¼ˆ90æ—¥ï¼‰ã®å ´åˆã¯å¾“æ¥é€šã‚Š1000ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå¿…è¦
                    if data_info['records'] < 1000:
                        raise ValueError(f"{symbol}: Only {data_info['records']} data points available (minimum: 1000)")
                
                self.logger.success(f"ğŸ“Š Data fetched: {data_info['records']} records for {symbol}")
                
                return {
                    'symbol': symbol,
                    'validation_status': validation_result.get('status', 'valid'),
                    'validation_valid': validation_result.get('valid', True),
                    'records_fetched': data_info['records'],
                    'date_range': data_info['date_range'],
                    'data_quality': 'HIGH',
                    'leverage_limit': validation_result.get('market_info', {}).get('leverage_limit', 10)
                }
                
            except Exception as api_error:
                self.logger.error(f"âŒ Failed to fetch OHLCV data for {symbol}: {api_error}")
                
                # ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã¯è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å‡¦ç†ã‚’åœæ­¢
                error_msg = str(api_error)
                if "No data retrieved" in error_msg:
                    raise ValueError(f"{symbol}ã®OHLCVãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚{exchange}ã§å–å¼•ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                elif "Too many failed requests" in error_msg:
                    raise ValueError(f"{symbol}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã§å¤šæ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚éŠ˜æŸ„ãŒå­˜åœ¨ã—ãªã„ã‹ã€ä¸€æ™‚çš„ã«ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                else:
                    raise ValueError(f"{symbol}ã®OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {error_msg}")
                
        except Exception as e:
            self.logger.error(f"âŒ Data fetch/validation failed for {symbol}: {e}")
            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            error_detail = {
                'symbol': symbol,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'step': 'data_fetch_validation'
            }
            
            # ã‚¨ãƒ©ãƒ¼è©³ç´°ã¯æ—¢ã«add_execution_errorã§è¨˜éŒ²æ¸ˆã¿
            
            raise
    
    async def _run_comprehensive_backtest(self, symbol: str, selected_strategies: list = None, selected_timeframes: list = None, strategy_configs: list = None, skip_pretask_creation: bool = False, custom_period_settings: dict = None) -> Dict:
        """å…¨æˆ¦ç•¥ãƒ»å…¨æ™‚é–“è¶³ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆé¸æŠçš„å®Ÿè¡Œå¯¾å¿œï¼‰"""
        
        try:
            self.logger.info(f"Running comprehensive backtest for {symbol}")
            
            # ã‚«ã‚¹ã‚¿ãƒ æˆ¦ç•¥è¨­å®šãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if strategy_configs:
                configs = []
                for config in strategy_configs:
                    # è¨­å®šãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–
                    if not config.get('base_strategy'):
                        self.logger.warning(f"Skipping config without base_strategy: {config}")
                        continue
                    configs.append({
                        'symbol': symbol,
                        'timeframe': config['timeframe'],
                        'strategy': config['base_strategy'],
                        'strategy_config_id': config.get('id'),
                        'strategy_name': config.get('name'),
                        'custom_parameters': config.get('parameters', {})
                    })
                self.logger.info(f"Using {len(configs)} custom strategy configurations")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šç”Ÿæˆ
                timeframes = selected_timeframes or ['1m', '3m', '5m', '15m', '30m', '1h']
                strategies = selected_strategies or ['Conservative_ML', 'Aggressive_Traditional', 'Full_ML']
                
                configs = []
                for timeframe in timeframes:
                    for strategy in strategies:
                        configs.append({
                            'symbol': symbol,
                            'timeframe': timeframe,
                            'strategy': strategy
                        })
                self.logger.info(f"Using default strategy combinations: {len(strategies)} strategies Ã— {len(timeframes)} timeframes = {len(configs)} configs")
            
            self.logger.info(f"Generated {len(configs)} backtest configurations")
            
            # execution_idå–å¾—
            current_execution_id = getattr(self, '_current_execution_id', None)
            
            # æ³¨æ„: 9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯å®Ÿéš›ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå‡¦ç†å†…ã§æ™‚ç³»åˆ—å®Ÿè¡Œã•ã‚Œã‚‹
            # ã“ã“ã§ã¯è¨­å®šãƒ¬ãƒ™ãƒ«ã§ã®äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è¡Œã‚ãªã„
            self.logger.info(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®šæ•°: {len(configs)}")
            
            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆScalableAnalysisSystemã‚’ä½¿ç”¨ + é€²æ—ãƒ­ã‚¬ãƒ¼çµ±åˆï¼‰
            # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯ã‚·ã‚°ãƒŠãƒ«ãªã—ã¨ã—ã¦ç¶™ç¶š
            # ğŸ”§ ä¿®æ­£: æˆ¦ç•¥åˆ¥ç‹¬ç«‹å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼éš”é›¢ã‚’å®Ÿç¾
            
            # execution_idãŒç’°å¢ƒå¤‰æ•°ã«ã‚‚è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            import os
            env_execution_id = os.environ.get('CURRENT_EXECUTION_ID')
            if current_execution_id and not env_execution_id:
                os.environ['CURRENT_EXECUTION_ID'] = current_execution_id
                self.logger.info(f"ğŸ“ å®Ÿè¡ŒIDã‚’ç’°å¢ƒå¤‰æ•°ã«å†è¨­å®š: {current_execution_id}")
            
            processed_count = self._execute_strategies_independently(
                configs, 
                symbol, 
                current_execution_id,
                custom_period_settings
            )
            
            # çµæœã®é›†è¨ˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼‰
            results = []
            best_performance = None
            failed_tests = 0
            low_performance_count = 0
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰çµæœã‚’å–å¾—
            for config in configs:
                try:
                    query_results = self.analysis_system.query_analyses(
                        filters={
                            'symbol': config['symbol'],
                            'timeframe': config['timeframe'], 
                            'config': config['strategy']
                        },
                        limit=1
                    )
                    
                    if query_results and len(query_results) > 0:
                        result = query_results[0] if isinstance(query_results, list) else query_results.iloc[0].to_dict()
                        result['status'] = 'success'
                        results.append(result)
                        
                        if result.get('sharpe_ratio', 0) < 1.0:
                            low_performance_count += 1
                        
                        if best_performance is None or result.get('sharpe_ratio', 0) > best_performance.get('sharpe_ratio', 0):
                            best_performance = result
                    else:
                        failed_tests += 1
                        results.append({'status': 'failed', 'config': config})
                        
                except Exception as e:
                    self.logger.warning(f"Failed to retrieve result for {config}: {e}")
                    failed_tests += 1
                    results.append({'status': 'failed', 'config': config})
            
            successful_tests = len(configs) - failed_tests
            
            # ã‚·ã‚°ãƒŠãƒ«ãªã—ï¼ˆno_signalï¼‰ã®å ´åˆã‚‚æˆåŠŸã¨ã—ã¦æ‰±ã†
            # processed_count > 0 ãªã‚‰åˆ†æãŒå®Ÿè¡Œã•ã‚ŒãŸã¨åˆ¤å®š
            if successful_tests == 0 and processed_count == 0:
                error_msg = f"å…¨æˆ¦ç•¥ã®åˆ†æãŒå¤±æ•—ã—ã¾ã—ãŸã€‚{failed_tests}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã€‚"
                self.logger.error(f"âŒ {symbol}: {error_msg}")
                raise ValueError(error_msg)
            elif processed_count > 0 and successful_tests == 0:
                # åˆ†æã¯å®Ÿè¡Œã•ã‚ŒãŸãŒã€é€šå¸¸ã®æˆåŠŸçµæœãŒãªã„å ´åˆï¼ˆã‚·ã‚°ãƒŠãƒ«ãªã—ã®å ´åˆï¼‰
                self.logger.info(f"ğŸ“Š {symbol}: åˆ†æå®Œäº†ï¼ˆã‚·ã‚°ãƒŠãƒ«ãªã—ï¼‰ - {processed_count}æˆ¦ç•¥å®Ÿè¡Œ, {failed_tests}ä»¶ã¯çµæœå–å¾—å¤±æ•—")
            
            self.logger.success(f"Backtest completed: {len(configs)} configurations tested")
            
            return {
                'total_combinations': len(configs),
                'successful_tests': successful_tests,
                'failed_tests': failed_tests,
                'low_performance_count': low_performance_count,
                'best_performance': best_performance,
                'results_summary': {
                    'avg_sharpe': sum(r.get('sharpe_ratio', 0) for r in results) / len(results) if results else 0,
                    'max_sharpe': max(r.get('sharpe_ratio', 0) for r in results) if results else 0,
                    'min_sharpe': min(r.get('sharpe_ratio', 0) for r in results) if results else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"Backtest failed for {symbol}: {e}")
            raise
    
    async def _train_ml_models(self, symbol: str) -> Dict:
        """MLå­¦ç¿’å®Ÿè¡Œ"""
        
        try:
            self.logger.info(f"Training ML models for {symbol}")
            
            # MLå­¦ç¿’ã®å®Ÿè£…
            # TODO: å®Ÿéš›ã®MLå­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            
            # TODO: ãƒ©ãƒ³ãƒ€ãƒ MLçµæœç”Ÿæˆã¯å“è³ªå•é¡Œã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ (2024-06-18)
            # å®Ÿéš›ã®MLå­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…ãŒå¿…è¦
            import time
            # import random
            
            # å­¦ç¿’æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            await asyncio.sleep(2)
            
            # TODO: ä»¥ä¸‹ã®ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆã¯å‰Šé™¤ã—ã€å®Ÿéš›ã®MLå­¦ç¿’çµæœã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£å¿…è¦
            # models_trained = {
            #     'support_resistance_ml': {
            #         'accuracy': round(random.uniform(0.65, 0.85), 3),
            #         'f1_score': round(random.uniform(0.6, 0.8), 3),
            #         'training_samples': random.randint(5000, 15000)
            #     },
            #     'breakout_predictor': {
            #         'accuracy': round(random.uniform(0.6, 0.8), 3),
            #         'precision': round(random.uniform(0.65, 0.85), 3),
            #         'training_samples': random.randint(3000, 10000)
            #     },
            #     'btc_correlation': {
            #         'r_squared': round(random.uniform(0.7, 0.9), 3),
            #         'mae': round(random.uniform(0.02, 0.08), 4),
            #         'training_samples': random.randint(8000, 20000)
            #     }
            # }
            
            # æš«å®š: å®Ÿéš›ã®MLå­¦ç¿’æœªå®Ÿè£…ã®ãŸã‚å›ºå®šå€¤ã‚’è¿”ã™
            models_trained = {
                'support_resistance_ml': {
                    'accuracy': 0.000,  # æœªå­¦ç¿’çŠ¶æ…‹ã‚’ç¤ºã™
                    'f1_score': 0.000,
                    'training_samples': 0,
                    'status': 'not_implemented'
                },
                'breakout_predictor': {
                    'accuracy': 0.000,
                    'precision': 0.000,
                    'training_samples': 0,
                    'status': 'not_implemented'
                },
                'btc_correlation': {
                    'r_squared': 0.000,
                    'mae': 0.000,
                    'training_samples': 0,
                    'status': 'not_implemented'
                }
            }
            
            self.logger.success(f"ML training completed for {symbol}")
            
            return {
                'models_trained': models_trained,
                'total_training_time': 120,  # seconds
                'avg_accuracy': 0.000,  # MLæœªå®Ÿè£…ã®ãŸã‚0
                'status': 'ml_training_not_implemented'
            }
            
        except Exception as e:
            self.logger.error(f"ML training failed for {symbol}: {e}")
            raise
    
    async def _save_results(self, symbol: str) -> Dict:
        """çµæœä¿å­˜ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ›´æ–°"""
        
        try:
            self.logger.info(f"Saving results for {symbol}")
            
            # çµæœã®ä¿å­˜
            # TODO: æˆ¦ç•¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°DBã¸ã®ä¿å­˜å®Ÿè£…
            
            # ä¸€æ™‚çš„ãªå®Ÿè£…
            await asyncio.sleep(1)
            
            self.logger.success(f"Results saved for {symbol}")
            
            return {
                'symbol': symbol,
                'ranking_entries_created': 18,  # 3 strategies Ã— 6 timeframes
                'database_updated': True,
                'available_for_selection': True
            }
            
        except Exception as e:
            self.logger.error(f"Result saving failed for {symbol}: {e}")
            raise
    
    def _create_no_signal_record(self, symbol: str, config: Dict, execution_id: str, error_message: str = None):
        """ã‚·ã‚°ãƒŠãƒ«ãªã—ã®åˆ†æãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
        try:
            import sqlite3
            from pathlib import Path
            from datetime import datetime, timezone
            import json
            
            analysis_db_path = Path(__file__).parent / "large_scale_analysis" / "analysis.db"
            
            with sqlite3.connect(analysis_db_path) as conn:
                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè©³ç´°æƒ…å ±ï¼ˆã‚·ã‚°ãƒŠãƒ«ãªã—ï¼‰
                backtest_details = {
                    "status": "no_signal",
                    "reason": error_message or "No trading signals detected",
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                # æ—¢å­˜ã®pendingãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ï¼ˆINSERTã§ã¯ãªãUPDATEï¼‰
                conn.execute("""
                    UPDATE analyses SET
                        strategy_config_id = ?, strategy_name = ?,
                        task_status = 'completed', task_completed_at = ?,
                        total_return = ?, sharpe_ratio = ?, max_drawdown = ?, win_rate = ?, total_trades = ?,
                        status = 'no_signal', error_message = ?, generated_at = ?
                    WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ? AND task_status = 'pending'
                """, (
                    config.get('strategy_config_id'),
                    config.get('strategy_name', f"{config['strategy']}-{config['timeframe']}"),
                    datetime.now(timezone.utc).isoformat(),
                    0.0,  # ã‚·ã‚°ãƒŠãƒ«ãªã—ã®ãŸã‚0ãƒªã‚¿ãƒ¼ãƒ³
                    0.0,  # ã‚·ã‚°ãƒŠãƒ«ãªã—ã®ãŸã‚0ã‚·ãƒ£ãƒ¼ãƒ—ãƒ¬ã‚·ã‚ª
                    0.0,  # ã‚·ã‚°ãƒŠãƒ«ãªã—ã®ãŸã‚0ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
                    0.0,  # ã‚·ã‚°ãƒŠãƒ«ãªã—ã®ãŸã‚0å‹ç‡
                    0,    # ã‚·ã‚°ãƒŠãƒ«ãªã—ã®ãŸã‚0å–å¼•
                    error_message or 'No trading signals detected',
                    datetime.now().isoformat(),
                    symbol,
                    config['timeframe'],
                    config['strategy'],
                    execution_id
                ))
                
                conn.commit()
                
            self.logger.info(f"ğŸ“ ã‚·ã‚°ãƒŠãƒ«ãªã—ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ: {symbol} - {config['strategy']} ({config['timeframe']})")
            
        except Exception as e:
            self.logger.error(f"ã‚·ã‚°ãƒŠãƒ«ãªã—ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    def _execute_strategies_independently(self, configs: List[Dict], symbol: str, execution_id: str, custom_period_settings: dict = None) -> int:
        """
        æˆ¦ç•¥ã‚’ç‹¬ç«‹å®Ÿè¡Œã—ã¦ã‚¨ãƒ©ãƒ¼éš”é›¢ã‚’å®Ÿç¾
        å„æˆ¦ç•¥ã®å¤±æ•—ãŒä»–æˆ¦ç•¥ã«å½±éŸ¿ã—ãªã„ã‚ˆã†å€‹åˆ¥å‡¦ç†
        
        Args:
            configs: æˆ¦ç•¥è¨­å®šãƒªã‚¹ãƒˆ
            symbol: éŠ˜æŸ„å
            execution_id: å®Ÿè¡ŒID
            custom_period_settings: ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®š
            
        Returns:
            æˆåŠŸã—ãŸæˆ¦ç•¥æ•°
        """
        success_count = 0
        
        self.logger.info(f"ğŸ”§ æˆ¦ç•¥åˆ¥ç‹¬ç«‹å®Ÿè¡Œé–‹å§‹: {len(configs)}æˆ¦ç•¥")
        
        for i, config in enumerate(configs):
            strategy_name = f"{config['strategy']}-{config['timeframe']}"
            
            try:
                self.logger.info(f"  æˆ¦ç•¥ {i+1}/{len(configs)}: {strategy_name}")
                
                # å€‹åˆ¥æˆ¦ç•¥åˆ†æã‚’å®Ÿè¡Œ
                result = self._execute_single_strategy(
                    config, 
                    symbol, 
                    execution_id, 
                    custom_period_settings
                )
                
                if result:
                    success_count += 1
                    self.logger.info(f"  âœ… {strategy_name}: åˆ†ææˆåŠŸ")
                else:
                    self.logger.warning(f"  âš ï¸ {strategy_name}: ã‚·ã‚°ãƒŠãƒ«ãªã—")
                    # ã‚·ã‚°ãƒŠãƒ«ãªã—ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
                    self._create_no_signal_record(symbol, config, execution_id)
                    
            except Exception as e:
                error_msg = str(e)
                self.logger.error(f"  âŒ {strategy_name}: åˆ†æã‚¨ãƒ©ãƒ¼ - {error_msg[:100]}")
                
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚·ã‚°ãƒŠãƒ«ãªã—ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆï¼ˆã‚¨ãƒ©ãƒ¼æƒ…å ±ä»˜ãï¼‰
                self._create_no_signal_record(symbol, config, execution_id, error_msg[:100])
                
                # é‡è¦: ä»–æˆ¦ç•¥ã®å‡¦ç†ã‚’ç¶™ç¶š
                continue
        
        self.logger.info(f"ğŸ¯ ç‹¬ç«‹å®Ÿè¡Œçµæœ: {success_count}/{len(configs)} æˆ¦ç•¥æˆåŠŸ")
        return success_count
    
    def _execute_single_strategy(self, config: Dict, symbol: str, execution_id: str, custom_period_settings: dict = None) -> bool:
        """
        å˜ä¸€æˆ¦ç•¥ã®åˆ†æå®Ÿè¡Œ
        
        Args:
            config: æˆ¦ç•¥è¨­å®š
            symbol: éŠ˜æŸ„å
            execution_id: å®Ÿè¡ŒID
            custom_period_settings: ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®š
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        # å˜ä¸€è¨­å®šã§ãƒãƒƒãƒåˆ†æã‚’å®Ÿè¡Œ
        single_config_list = [config]
        
        processed_count = self.analysis_system.generate_batch_analysis(
            single_config_list,
            symbol=symbol,
            execution_id=execution_id,
            skip_pretask_creation=True,  # æ—¢ã«Pre-taskä½œæˆæ¸ˆã¿
            custom_period_settings=custom_period_settings
        )
        
        return processed_count > 0

    def get_execution_status(self, execution_id: str) -> Optional[Dict]:
        """å®Ÿè¡ŒçŠ¶æ³ã®å–å¾—"""
        return self.execution_db.get_execution(execution_id)
    
    def list_executions(self, limit: int = 10) -> List[Dict]:
        """å®Ÿè¡Œå±¥æ­´ã®ä¸€è¦§å–å¾—"""
        return self.execution_db.list_executions(limit=limit)
    
    async def _run_early_fail_validation(self, symbol: str):
        """Early Failæ¤œè¨¼å®Ÿè¡Œ"""
        try:
            # Early Failæ¤œè¨¼ã‚’éåŒæœŸå®Ÿè¡Œ
            import asyncio
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.early_fail_validator.validate_symbol,
                symbol
            )
            
            # validate_symbolãŒã‚³ãƒ«ãƒ¼ãƒãƒ³ã‚’è¿”ã™å ´åˆã®å¯¾å‡¦
            if asyncio.iscoroutine(result):
                result = await result
            
            if result.passed:
                self.logger.info(f"âœ… Early Failæ¤œè¨¼æˆåŠŸ: {symbol}")
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                if result.metadata:
                    self.logger.info(f"ğŸ“Š æ¤œè¨¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {result.metadata}")
            else:
                self.logger.warning(f"âŒ Early Failæ¤œè¨¼å¤±æ•—: {symbol} - {result.fail_reason}")
                if result.suggestion:
                    self.logger.info(f"ğŸ’¡ ææ¡ˆ: {result.suggestion}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Early Failæ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {symbol} - {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤±æ•—æ‰±ã„ã§çµæœã‚’è¿”ã™
            from symbol_early_fail_validator import EarlyFailResult, FailReason
            return EarlyFailResult(
                symbol=symbol,
                passed=False,
                fail_reason=FailReason.API_CONNECTION_FAILED,
                error_message=f"æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}"
            )
    
    async def _apply_filtering_framework(self, configs: List[Dict], symbol: str, execution_id: str) -> List[Dict]:
        """
        æ³¨æ„: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯éæ¨å¥¨ã§ã™ã€‚
        9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯å®Ÿéš›ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå‡¦ç†å†…ã§æ™‚ç³»åˆ—æ¯ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
        ã“ã“ã§ã¯è¨­å®šãƒ¬ãƒ™ãƒ«ã§ã®äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯è¡Œã„ã¾ã›ã‚“ã€‚
        """
        self.logger.warning("âš ï¸ _apply_filtering_framework ã¯éæ¨å¥¨ã§ã™ã€‚9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå†…ã§æ™‚ç³»åˆ—å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚")
        
        # è¨­å®šã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
        return configs
    
    async def _evaluate_config_viability(self, config: Dict, symbol: str) -> bool:
        """å€‹åˆ¥æˆ¦ç•¥è¨­å®šã®å®Ÿè¡Œå¯èƒ½æ€§è©•ä¾¡ï¼ˆè»½é‡ç‰ˆï¼‰"""
        try:
            # åŸºæœ¬çš„ãªè¨­å®šæ¤œè¨¼
            required_fields = ['symbol', 'timeframe', 'strategy']
            if not all(field in config for field in required_fields):
                return False
            
            # æˆ¦ç•¥å›ºæœ‰ã®è»½é‡ãƒã‚§ãƒƒã‚¯
            strategy = config['strategy']
            timeframe = config['timeframe']
            
            # ç°¡å˜ãªæˆ¦ç•¥ãƒ»æ™‚é–“è¶³çµ„ã¿åˆã‚ã›ãƒã‚§ãƒƒã‚¯
            if strategy == 'Conservative_ML' and timeframe in ['1m', '3m']:
                # ä¿å®ˆçš„MLæˆ¦ç•¥ã¯çŸ­æœŸé–“è¶³ã§ã¯åŠ¹æœãŒä½ã„
                return False
            
            if strategy == 'Aggressive_Traditional' and timeframe in ['1h']:
                # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–å¾“æ¥æˆ¦ç•¥ã¯é•·æœŸé–“è¶³ã§ã¯åŠ¹æœãŒä½ã„
                return False
            
            # TODO: å°†æ¥çš„ã«å®Ÿéš›ã®FilteringFrameworkã¨é€£æº
            # ç¾åœ¨ã¯åŸºæœ¬çš„ãªè«–ç†ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè£…
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ è¨­å®šè©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {config} - {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é€šã™
            return True
    
    async def _record_filtering_statistics(self, execution_id: str, total_configs: int, passed_configs: int, filtered_configs: int):
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã‚’execution_logã«è¨˜éŒ²"""
        try:
            metadata = {
                'filtering_statistics': {
                    'total_configurations': total_configs,
                    'passed_configurations': passed_configs,
                    'filtered_configurations': filtered_configs,
                    'filter_rate_percent': (filtered_configs / total_configs) * 100 if total_configs > 0 else 0,
                    'timestamp': datetime.now().isoformat()
                }
            }
            
            # execution_logã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            self.execution_db.add_execution_step(
                execution_id,
                "filtering_framework_precheck",
                ExecutionStatus.COMPLETED.value,  # .valueã‚’è¿½åŠ 
                metadata=metadata
            )
            
            self.logger.info(f"ğŸ“ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆè¨˜éŒ²å®Œäº†: {execution_id}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def _verify_analysis_results(self, symbol: str, execution_id: str) -> bool:
        """åˆ†æçµæœã®å­˜åœ¨ç¢ºèªï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæ¤œè¨¼ï¼‰"""
        try:
            import sqlite3
            from pathlib import Path
            
            analysis_db_path = Path(__file__).parent / "large_scale_analysis" / "analysis.db"
            if not analysis_db_path.exists():
                self.logger.warning(f"Analysis database not found: {analysis_db_path}")
                return False
                
            with sqlite3.connect(analysis_db_path) as conn:
                # 1. è©²å½“execution_idã®åˆ†æçµæœã‚’ç¢ºèª
                cursor = conn.execute('''
                    SELECT COUNT(*) FROM analyses 
                    WHERE symbol = ? AND execution_id = ?
                ''', (symbol, execution_id))
                
                result_count = cursor.fetchone()[0]
                
                if result_count > 0:
                    self.logger.info(f"âœ… {symbol} ã®åˆ†æçµæœç¢ºèªï¼ˆexecution_idä¸€è‡´ï¼‰: {result_count} ä»¶")
                    return True
                
                # 2. éå»10åˆ†ä»¥å†…ã®åˆ†æçµæœã‚’ç¢ºèªï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã‚‹å ´åˆï¼‰
                cursor = conn.execute('''
                    SELECT COUNT(*) FROM analyses 
                    WHERE symbol = ? 
                    AND generated_at > datetime('now', '-10 minutes')
                ''', (symbol,))
                
                recent_count = cursor.fetchone()[0]
                if recent_count > 0:
                    self.logger.info(f"âœ… {symbol} ã®æœ€è¿‘ã®åˆ†æçµæœç¢ºèª: {recent_count} ä»¶ï¼ˆéå»10åˆ†ä»¥å†…ï¼‰")
                    return True
                
                # 3. execution_idãŒNULLã®å ´åˆã‚‚ç¢ºèªï¼ˆæ—§ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
                cursor = conn.execute('''
                    SELECT COUNT(*) FROM analyses 
                    WHERE symbol = ? AND execution_id IS NULL
                    AND generated_at > datetime('now', '-5 minutes')
                ''', (symbol,))
                
                null_count = cursor.fetchone()[0]
                if null_count > 0:
                    self.logger.warning(f"âš ï¸ {symbol} ã®åˆ†æçµæœï¼ˆexecution_idãªã—ï¼‰: {null_count} ä»¶å­˜åœ¨")
                    return True
                
                # 4. è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›
                cursor = conn.execute('''
                    SELECT execution_id, generated_at FROM analyses 
                    WHERE symbol = ? 
                    ORDER BY generated_at DESC LIMIT 5
                ''', (symbol,))
                
                recent_results = cursor.fetchall()
                if recent_results:
                    self.logger.info(f"ğŸ“Š {symbol} ã®æœ€è¿‘ã®åˆ†æçµæœ:")
                    for exec_id, gen_at in recent_results:
                        self.logger.info(f"  - execution_id: {exec_id}, generated_at: {gen_at}")
                
                self.logger.error(f"âŒ {symbol} ã®åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆç¾åœ¨ã®execution_id: {execution_id}ï¼‰")
                return False
                        
        except Exception as e:
            self.logger.error(f"åˆ†æçµæœç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False


async def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«ã‚ˆã‚‹ã‚·ãƒ³ãƒœãƒ«è¿½åŠ å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Auto Symbol Training System')
    parser.add_argument('--symbol', type=str, default='HYPE', help='Symbol to add (default: HYPE)')
    parser.add_argument('--days', type=int, default=90, help='Days of historical data (default: 90)')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    
    trainer = AutoSymbolTrainer()
    
    try:
        # æŒ‡å®šã•ã‚ŒãŸéŠ˜æŸ„ã§ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        print(f"ğŸš€ Starting automatic training for symbol: {args.symbol}")
        execution_id = await trainer.add_symbol_with_training(args.symbol)
        print(f"Execution completed: {execution_id}")
        
        # å®Ÿè¡ŒçŠ¶æ³ã®ç¢ºèª
        status = trainer.get_execution_status(execution_id)
        print(f"Final status: {status['status']}")
        print(f"Steps completed: {len(status.get('steps', []))}")
        
        # å®Ÿè¡Œå±¥æ­´ã®ç¢ºèª
        executions = trainer.list_executions(limit=3)
        print(f"Recent executions: {len(executions)}")
        
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    asyncio.run(main())