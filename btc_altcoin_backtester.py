"""
BTC-ã‚¢ãƒ«ãƒˆã‚³ã‚¤ãƒ³é€£ã‚Œå®‰äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½

ã€æ¦‚è¦ã€‘
éå»ã®BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦æ¤œè¨¼ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚
å®Ÿç”¨çš„ãªæ¸…ç®—å›é¿ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
1. éå»ã®æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
2. Walk-forward ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
3. äºˆæ¸¬ç²¾åº¦ã®å¤šè§’çš„è©•ä¾¡
4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
5. æ¸…ç®—å›é¿åŠ¹æœã®æ¸¬å®š
6. çµæœå¯è¦–åŒ–

ã€ä½¿ç”¨æ–¹æ³•ã€‘
python btc_altcoin_backtester.py --symbol ETH --backtest-days 365 --optimize
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import time
import json
from dataclasses import dataclass

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import TimeSeriesSplit, ParameterGrid
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import lightgbm as lgb
import joblib

# Hyperliquid API
from hyperliquid.info import Info
from hyperliquid.utils import constants

# æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from btc_altcoin_correlation_predictor import BTCAltcoinCorrelationPredictor

warnings.filterwarnings('ignore')

@dataclass
class BTCDropEvent:
    """BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã®æƒ…å ±"""
    timestamp: datetime
    drop_pct: float
    duration_minutes: int
    volume_spike: float
    prior_trend: str  # 'up', 'down', 'sideways'
    market_hour: int
    is_weekend: bool

@dataclass
class BacktestResult:
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ"""
    symbol: str
    total_events: int
    prediction_accuracy: Dict[int, float]  # {horizon: mae}
    direction_accuracy: Dict[int, float]   # {horizon: correct_direction_pct}
    liquidation_avoidance: Dict[str, float]  # æ¸…ç®—å›é¿åŠ¹æœ
    optimal_thresholds: Dict[int, float]   # æœ€é©é–¾å€¤
    feature_importance: pd.DataFrame
    
class BTCAltcoinBacktester:
    """BTC-ã‚¢ãƒ«ãƒˆã‚³ã‚¤ãƒ³äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self, 
                 btc_drop_threshold: float = -3.0,
                 prediction_horizons: List[int] = [5, 15, 60, 240],
                 leverage_levels: List[float] = [5, 10, 20, 50]):
        """
        åˆæœŸåŒ–
        
        Args:
            btc_drop_threshold: BTCæ€¥è½æ¤œçŸ¥é–¾å€¤ï¼ˆ%ï¼‰
            prediction_horizons: äºˆæ¸¬æ™‚é–“å¹…ï¼ˆåˆ†ï¼‰
            leverage_levels: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¬ãƒãƒ¬ãƒƒã‚¸
        """
        self.btc_drop_threshold = btc_drop_threshold
        self.prediction_horizons = prediction_horizons
        self.leverage_levels = leverage_levels
        
        self.info = Info(constants.MAINNET_API_URL)
        self.predictor = BTCAltcoinCorrelationPredictor(prediction_horizons)
        
        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœä¿å­˜ç”¨
        self.btc_drop_events = []
        self.backtest_results = {}
        
    def extract_btc_drop_events(self, days: int = 365) -> List[BTCDropEvent]:
        """
        éå»ã®BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º
        
        Args:
            days: é¡ã‚‹æ—¥æ•°
            
        Returns:
            æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        print(f"éå»{days}æ—¥é–“ã®BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºä¸­...")
        
        # BTCãƒ‡ãƒ¼ã‚¿å–å¾—
        btc_data = self.predictor.fetch_historical_data('BTC', '1m', days)
        if btc_data.empty:
            print("BTCãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return []
        
        events = []
        returns = btc_data['close'].pct_change()
        
        # 15åˆ†é–“ã®ãƒ­ãƒ¼ãƒªãƒ³ã‚°ä¸‹è½ç‡ã‚’è¨ˆç®—
        rolling_returns = returns.rolling(15).sum() * 100
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤èª¿æ•´
        volatility = returns.rolling(1440).std()  # 24æ™‚é–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        median_vol = volatility.median()
        
        # æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º
        for i in range(len(rolling_returns)):
            if pd.isna(rolling_returns.iloc[i]):
                continue
                
            drop_pct = rolling_returns.iloc[i]
            current_vol = volatility.iloc[i] if not pd.isna(volatility.iloc[i]) else median_vol
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´é–¾å€¤
            adjusted_threshold = self.btc_drop_threshold * (current_vol / median_vol)
            
            if drop_pct <= adjusted_threshold:
                timestamp = btc_data.index[i]
                
                # æ€¥è½ã®è©³ç´°æƒ…å ±ã‚’è¨ˆç®—
                start_idx = max(0, i - 15)
                end_idx = min(len(btc_data), i + 60)  # 1æ™‚é–“å¾Œã¾ã§
                
                # å‡ºæ¥é«˜ã‚¹ãƒ‘ã‚¤ã‚¯
                volume_ma = btc_data['volume'].rolling(60).mean().iloc[i]
                current_volume = btc_data['volume'].iloc[i]
                volume_spike = current_volume / volume_ma if volume_ma > 0 else 1.0
                
                # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰
                if i >= 240:  # 4æ™‚é–“å‰ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                    trend_data = btc_data['close'].iloc[i-240:i]
                    trend_slope = np.polyfit(range(len(trend_data)), trend_data.values, 1)[0]
                    if trend_slope > 0.1:
                        prior_trend = 'up'
                    elif trend_slope < -0.1:
                        prior_trend = 'down'
                    else:
                        prior_trend = 'sideways'
                else:
                    prior_trend = 'unknown'
                
                # æ€¥è½ã®æŒç¶šæ™‚é–“
                duration = 15  # æœ€ä½15åˆ†
                for j in range(i + 1, min(len(returns), i + 120)):  # æœ€å¤§2æ™‚é–“
                    if returns.iloc[j] >= 0:  # ãƒ—ãƒ©ã‚¹è»¢æ›ã§çµ‚äº†
                        break
                    duration += 1
                
                event = BTCDropEvent(
                    timestamp=timestamp,
                    drop_pct=drop_pct,
                    duration_minutes=duration,
                    volume_spike=volume_spike,
                    prior_trend=prior_trend,
                    market_hour=timestamp.hour,
                    is_weekend=timestamp.weekday() >= 5
                )
                
                events.append(event)
        
        # é‡è¤‡é™¤å»ï¼ˆ30åˆ†ä»¥å†…ã®é€£ç¶šã‚¤ãƒ™ãƒ³ãƒˆï¼‰
        filtered_events = []
        for event in events:
            if not filtered_events or (event.timestamp - filtered_events[-1].timestamp).total_seconds() > 1800:
                filtered_events.append(event)
        
        print(f"æ¤œå‡ºã•ã‚ŒãŸæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆ: {len(filtered_events)}ä»¶")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆåˆ†æ
        self._analyze_drop_events(filtered_events)
        
        return filtered_events
    
    def _analyze_drop_events(self, events: List[BTCDropEvent]):
        """æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆã®çµ±è¨ˆåˆ†æ"""
        if not events:
            return
            
        drops = [e.drop_pct for e in events]
        volumes = [e.volume_spike for e in events]
        durations = [e.duration_minutes for e in events]
        
        print(f"\n=== BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆåˆ†æ ===")
        print(f"ä¸‹è½ç‡: å¹³å‡ {np.mean(drops):.2f}%, ä¸­å¤®å€¤ {np.median(drops):.2f}%")
        print(f"æœ€å¤§ä¸‹è½: {min(drops):.2f}%")
        print(f"å‡ºæ¥é«˜å€ç‡: å¹³å‡ {np.mean(volumes):.2f}x")
        print(f"æŒç¶šæ™‚é–“: å¹³å‡ {np.mean(durations):.1f}åˆ†")
        
        # æ™‚é–“å¸¯åˆ†æ
        hours = [e.market_hour for e in events]
        weekend_count = sum(1 for e in events if e.is_weekend)
        print(f"é€±æœ«ç™ºç”Ÿç‡: {weekend_count/len(events)*100:.1f}%")
        print(f"æœ€å¤šç™ºç”Ÿæ™‚é–“: {max(set(hours), key=hours.count)}æ™‚")
    
    def run_backtest(self, symbol: str, events: List[BTCDropEvent]) -> BacktestResult:
        """
        æŒ‡å®šéŠ˜æŸ„ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        
        Args:
            symbol: ã‚¢ãƒ«ãƒˆã‚³ã‚¤ãƒ³ã‚·ãƒ³ãƒœãƒ«
            events: BTCæ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ
        """
        print(f"\n{symbol}ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        # ã‚¢ãƒ«ãƒˆã‚³ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—
        alt_data = self.predictor.fetch_historical_data(symbol, '1m', 365)
        if alt_data.empty:
            print(f"{symbol}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            return None
        
        # äºˆæ¸¬ç²¾åº¦è©•ä¾¡ç”¨
        predictions_vs_actual = {horizon: {'pred': [], 'actual': []} for horizon in self.prediction_horizons}
        liquidation_stats = {lev: {'total': 0, 'avoided': 0} for lev in self.leverage_levels}
        
        # Walk-forward validation
        train_window = 30  # 30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
        
        for i, event in enumerate(events):
            if i % 10 == 0:
                print(f"  ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ä¸­: {i+1}/{len(events)}")
            
            # è¨“ç·´æœŸé–“è¨­å®š
            train_start = event.timestamp - timedelta(days=train_window)
            train_end = event.timestamp - timedelta(hours=1)  # 1æ™‚é–“å‰ã¾ã§
            
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿å–å¾—
            train_btc = self._get_data_slice(self.predictor.fetch_historical_data('BTC', '1m', train_window), 
                                           train_start, train_end)
            train_alt = self._get_data_slice(alt_data, train_start, train_end)
            
            if len(train_btc) < 1000 or len(train_alt) < 1000:
                continue
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            try:
                features = self.predictor.calculate_features(train_btc, train_alt)
                if len(features) < 500:
                    continue
                
                # å„æ™‚é–“è»¸ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»äºˆæ¸¬
                for horizon in self.prediction_horizons:
                    model, scaler = self._train_model(features, horizon)
                    if model is None:
                        continue
                    
                    # äºˆæ¸¬å®Ÿè¡Œ
                    pred_features = self.predictor.create_prediction_features(event.drop_pct)
                    X_scaled = scaler.transform([pred_features])
                    predicted_drop = model.predict(X_scaled)[0] * 100
                    
                    # å®Ÿéš›ã®ä¸‹è½å¹…å–å¾—
                    actual_drop = self._get_actual_drop(alt_data, event.timestamp, horizon)
                    if actual_drop is not None:
                        predictions_vs_actual[horizon]['pred'].append(predicted_drop)
                        predictions_vs_actual[horizon]['actual'].append(actual_drop)
                    
                    # æ¸…ç®—å›é¿åŠ¹æœè©•ä¾¡
                    for leverage in self.leverage_levels:
                        liquidation_stats[leverage]['total'] += 1
                        
                        # äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
                        predicted_loss = predicted_drop * leverage
                        liquidation_threshold = -90 / leverage
                        
                        if predicted_loss <= liquidation_threshold * 0.8:  # 80%ã§è­¦å‘Š
                            # æ—©æœŸæåˆ‡ã‚Šã‚’å®Ÿè¡Œã—ãŸã¨ä»®å®š
                            actual_loss = actual_drop * leverage
                            if actual_loss <= liquidation_threshold:
                                liquidation_stats[leverage]['avoided'] += 1
                
            except Exception as e:
                print(f"  ã‚¤ãƒ™ãƒ³ãƒˆ{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # çµæœé›†è¨ˆ
        prediction_accuracy = {}
        direction_accuracy = {}
        
        for horizon in self.prediction_horizons:
            if len(predictions_vs_actual[horizon]['pred']) > 0:
                pred = np.array(predictions_vs_actual[horizon]['pred'])
                actual = np.array(predictions_vs_actual[horizon]['actual'])
                
                # äºˆæ¸¬ç²¾åº¦
                mae = mean_absolute_error(actual, pred)
                prediction_accuracy[horizon] = mae
                
                # æ–¹å‘æ€§ç²¾åº¦
                pred_direction = pred < 0
                actual_direction = actual < 0
                direction_acc = np.mean(pred_direction == actual_direction) * 100
                direction_accuracy[horizon] = direction_acc
        
        # æ¸…ç®—å›é¿ç‡
        liquidation_avoidance = {}
        for leverage in self.leverage_levels:
            stats = liquidation_stats[leverage]
            if stats['total'] > 0:
                avoidance_rate = stats['avoided'] / stats['total'] * 100
                liquidation_avoidance[f"{leverage}x"] = avoidance_rate
        
        result = BacktestResult(
            symbol=symbol,
            total_events=len(events),
            prediction_accuracy=prediction_accuracy,
            direction_accuracy=direction_accuracy,
            liquidation_avoidance=liquidation_avoidance,
            optimal_thresholds={},  # å¾Œã§è¨ˆç®—
            feature_importance=pd.DataFrame()  # å¾Œã§è¨ˆç®—
        )
        
        return result
    
    def _get_data_slice(self, data: pd.DataFrame, start: datetime, end: datetime) -> pd.DataFrame:
        """æŒ‡å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return data[(data.index >= start) & (data.index <= end)]
    
    def _train_model(self, features: pd.DataFrame, horizon: int):
        """å˜ä¸€æ™‚é–“è»¸ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        target_col = f'alt_return_{horizon}m'
        if target_col not in features.columns:
            return None, None
        
        feature_cols = [col for col in features.columns if not col.startswith('alt_return')]
        X = features[feature_cols].dropna()
        y = features[target_col].loc[X.index].dropna()
        
        common_idx = X.index.intersection(y.index)
        if len(common_idx) < 100:
            return None, None
        
        X = X.loc[common_idx]
        y = y.loc[common_idx]
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = lgb.LGBMRegressor(
            n_estimators=50,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            verbosity=-1
        )
        model.fit(X_scaled, y)
        
        return model, scaler
    
    def _get_actual_drop(self, alt_data: pd.DataFrame, event_time: datetime, horizon: int) -> Optional[float]:
        """å®Ÿéš›ã®ä¸‹è½å¹…ã‚’å–å¾—"""
        try:
            start_idx = alt_data.index.get_indexer([event_time], method='nearest')[0]
            end_time = event_time + timedelta(minutes=horizon)
            end_idx = alt_data.index.get_indexer([end_time], method='nearest')[0]
            
            if start_idx >= 0 and end_idx >= 0 and end_idx < len(alt_data):
                start_price = alt_data['close'].iloc[start_idx]
                end_price = alt_data['close'].iloc[end_idx]
                drop_pct = ((end_price - start_price) / start_price) * 100
                return drop_pct
        except:
            pass
        return None
    
    def optimize_hyperparameters(self, symbol: str, features: pd.DataFrame) -> Dict:
        """
        ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        """
        print(f"{symbol}ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
        
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 4, 6],
            'learning_rate': [0.05, 0.1, 0.2],
            'min_child_samples': [10, 20, 50]
        }
        
        best_params = {}
        
        for horizon in self.prediction_horizons:
            target_col = f'alt_return_{horizon}m'
            if target_col not in features.columns:
                continue
            
            feature_cols = [col for col in features.columns if not col.startswith('alt_return')]
            X = features[feature_cols].dropna()
            y = features[target_col].loc[X.index].dropna()
            
            if len(X) < 500:
                continue
            
            # æ™‚ç³»åˆ—åˆ†å‰²
            tscv = TimeSeriesSplit(n_splits=3)
            best_score = float('inf')
            best_param = None
            
            for params in ParameterGrid(param_grid):
                cv_scores = []
                
                for train_idx, val_idx in tscv.split(X):
                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                    
                    scaler = StandardScaler()
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_val_scaled = scaler.transform(X_val)
                    
                    model = lgb.LGBMRegressor(**params, random_state=42, verbosity=-1)
                    model.fit(X_train_scaled, y_train)
                    
                    y_pred = model.predict(X_val_scaled)
                    mae = mean_absolute_error(y_val, y_pred)
                    cv_scores.append(mae)
                
                mean_score = np.mean(cv_scores)
                if mean_score < best_score:
                    best_score = mean_score
                    best_param = params
            
            best_params[horizon] = best_param
            print(f"  {horizon}åˆ†: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ (MAE: {best_score:.4f})")
        
        return best_params
    
    def visualize_results(self, results: Dict[str, BacktestResult]):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®å¯è¦–åŒ–"""
        print("\nãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’å¯è¦–åŒ–ä¸­...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒ
        ax1 = axes[0, 0]
        symbols = list(results.keys())
        horizons = self.prediction_horizons
        
        accuracy_data = []
        for symbol in symbols:
            for horizon in horizons:
                if horizon in results[symbol].prediction_accuracy:
                    accuracy_data.append({
                        'Symbol': symbol,
                        'Horizon': f'{horizon}m',
                        'MAE': results[symbol].prediction_accuracy[horizon]
                    })
        
        if accuracy_data:
            df_acc = pd.DataFrame(accuracy_data)
            pivot_acc = df_acc.pivot(index='Symbol', columns='Horizon', values='MAE')
            sns.heatmap(pivot_acc, annot=True, fmt='.3f', ax=ax1, cmap='RdYlBu_r')
            ax1.set_title('Prediction Accuracy (MAE)')
        
        # 2. æ–¹å‘æ€§ç²¾åº¦
        ax2 = axes[0, 1]
        direction_data = []
        for symbol in symbols:
            for horizon in horizons:
                if horizon in results[symbol].direction_accuracy:
                    direction_data.append({
                        'Symbol': symbol,
                        'Horizon': f'{horizon}m',
                        'Accuracy': results[symbol].direction_accuracy[horizon]
                    })
        
        if direction_data:
            df_dir = pd.DataFrame(direction_data)
            pivot_dir = df_dir.pivot(index='Symbol', columns='Horizon', values='Accuracy')
            sns.heatmap(pivot_dir, annot=True, fmt='.1f', ax=ax2, cmap='RdYlGn')
            ax2.set_title('Direction Accuracy (%)')
        
        # 3. æ¸…ç®—å›é¿ç‡
        ax3 = axes[1, 0]
        avoidance_data = []
        for symbol in symbols:
            for leverage, rate in results[symbol].liquidation_avoidance.items():
                avoidance_data.append({
                    'Symbol': symbol,
                    'Leverage': leverage,
                    'Avoidance_Rate': rate
                })
        
        if avoidance_data:
            df_avoid = pd.DataFrame(avoidance_data)
            pivot_avoid = df_avoid.pivot(index='Symbol', columns='Leverage', values='Avoidance_Rate')
            sns.heatmap(pivot_avoid, annot=True, fmt='.1f', ax=ax3, cmap='RdYlGn')
            ax3.set_title('Liquidation Avoidance Rate (%)')
        
        # 4. ç·åˆã‚¹ã‚³ã‚¢
        ax4 = axes[1, 1]
        score_data = []
        for symbol in symbols:
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ–¹å‘æ€§ç²¾åº¦ + æ¸…ç®—å›é¿ç‡ã®å¹³å‡ï¼‰
            dir_scores = list(results[symbol].direction_accuracy.values())
            avoid_scores = list(results[symbol].liquidation_avoidance.values())
            
            if dir_scores and avoid_scores:
                total_score = (np.mean(dir_scores) + np.mean(avoid_scores)) / 2
                score_data.append({'Symbol': symbol, 'Total_Score': total_score})
        
        if score_data:
            df_score = pd.DataFrame(score_data)
            bars = ax4.bar(df_score['Symbol'], df_score['Total_Score'])
            ax4.set_title('Overall Performance Score')
            ax4.set_ylabel('Score (%)')
            
            # ãƒãƒ¼ã«æ•°å€¤è¡¨ç¤º
            for bar, score in zip(bars, df_score['Total_Score']):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                        f'{score:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('backtest_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print("âœ… çµæœã‚’å¯è¦–åŒ–: backtest_results.png")
    
    def save_results(self, results: Dict[str, BacktestResult], filename: str = "backtest_results.json"):
        """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜"""
        serializable_results = {}
        
        for symbol, result in results.items():
            serializable_results[symbol] = {
                'symbol': result.symbol,
                'total_events': result.total_events,
                'prediction_accuracy': result.prediction_accuracy,
                'direction_accuracy': result.direction_accuracy,
                'liquidation_avoidance': result.liquidation_avoidance,
                'optimal_thresholds': result.optimal_thresholds
            }
        
        with open(filename, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"âœ… çµæœã‚’ä¿å­˜: {filename}")

def main():
    parser = argparse.ArgumentParser(description='BTC-ã‚¢ãƒ«ãƒˆã‚³ã‚¤ãƒ³äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ')
    parser.add_argument('--symbols', type=str, default='ETH,SOL,HYPE', 
                       help='ãƒ†ã‚¹ãƒˆå¯¾è±¡éŠ˜æŸ„ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--backtest-days', type=int, default=365,
                       help='ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæœŸé–“ï¼ˆæ—¥æ•°ï¼‰')
    parser.add_argument('--btc-threshold', type=float, default=-3.0,
                       help='BTCæ€¥è½æ¤œçŸ¥é–¾å€¤ï¼ˆ%ï¼‰')
    parser.add_argument('--optimize', action='store_true',
                       help='ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    symbols = [s.strip().upper() for s in args.symbols.split(',')]
    
    # ãƒãƒƒã‚¯ãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
    backtester = BTCAltcoinBacktester(btc_drop_threshold=args.btc_threshold)
    
    # æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆæŠ½å‡º
    btc_events = backtester.extract_btc_drop_events(args.backtest_days)
    
    if len(btc_events) < 10:
        print("æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆãŒå°‘ãªã™ãã¾ã™ã€‚é–¾å€¤ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # å„éŠ˜æŸ„ã§ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = {}
    
    for symbol in symbols:
        print(f"\n{'='*50}")
        print(f"{symbol} ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print(f"{'='*50}")
        
        result = backtester.run_backtest(symbol, btc_events)
        if result:
            results[symbol] = result
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print(f"\n{symbol} ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ:")
            print(f"  ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {result.total_events}")
            
            if result.prediction_accuracy:
                print(f"  äºˆæ¸¬ç²¾åº¦ (MAE):")
                for horizon, mae in result.prediction_accuracy.items():
                    print(f"    {horizon}åˆ†: {mae:.3f}%")
            
            if result.direction_accuracy:
                print(f"  æ–¹å‘æ€§ç²¾åº¦:")
                for horizon, acc in result.direction_accuracy.items():
                    print(f"    {horizon}åˆ†: {acc:.1f}%")
            
            if result.liquidation_avoidance:
                print(f"  æ¸…ç®—å›é¿ç‡:")
                for leverage, rate in result.liquidation_avoidance.items():
                    print(f"    {leverage}: {rate:.1f}%")
    
    if results:
        # çµæœå¯è¦–åŒ–
        backtester.visualize_results(results)
        
        # çµæœä¿å­˜
        backtester.save_results(results)
        
        print(f"\nğŸ‰ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†!")
        print(f"å¯¾è±¡éŠ˜æŸ„: {', '.join(symbols)}")
        print(f"æ¤œè¨¼æœŸé–“: {args.backtest_days}æ—¥é–“")
        print(f"æ€¥è½ã‚¤ãƒ™ãƒ³ãƒˆ: {len(btc_events)}ä»¶")

if __name__ == "__main__":
    main()