#!/usr/bin/env python3
"""
ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ 
execution_logså‰Šé™¤æ™‚ã«é–¢é€£ã™ã‚‹analysesãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•çš„ã«å‰Šé™¤
"""

import os
import sys
import sqlite3
import shutil
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, List, Optional, Tuple

class CascadeDeletionSystem:
    """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, execution_db_path=None, analysis_db_path=None):
        self.execution_db_path = Path(execution_db_path or "execution_logs.db")
        self.analysis_db_path = Path(analysis_db_path or "web_dashboard/large_scale_analysis/analysis.db")
        
        if not self.execution_db_path.exists():
            raise FileNotFoundError(f"execution_logs.db not found: {self.execution_db_path}")
        if not self.analysis_db_path.exists():
            raise FileNotFoundError(f"analysis.db not found: {self.analysis_db_path}")
    
    def analyze_deletion_impact(self, execution_ids: List[str]) -> Dict:
        """å‰Šé™¤ã®å½±éŸ¿ç¯„å›²ã‚’åˆ†æ"""
        print("ğŸ“Š å‰Šé™¤å½±éŸ¿ç¯„å›²åˆ†æ")
        print("-" * 40)
        
        impact_analysis = {
            'execution_logs': {
                'total_found': 0,
                'records': []
            },
            'analyses': {
                'total_affected': 0,
                'records': [],
                'by_symbol': {},
                'by_config': {}
            },
            'file_artifacts': {
                'chart_files': [],
                'compressed_files': [],
                'total_size': 0
            },
            'warnings': []
        }
        
        # execution_logs ã®ç¢ºèª
        with sqlite3.connect(self.execution_db_path) as exec_conn:
            exec_conn.row_factory = sqlite3.Row
            
            placeholders = ','.join(['?' for _ in execution_ids])
            cursor = exec_conn.execute(f"""
                SELECT execution_id, execution_type, symbol, status, timestamp_start, timestamp_end
                FROM execution_logs 
                WHERE execution_id IN ({placeholders})
            """, execution_ids)
            
            exec_records = cursor.fetchall()
            impact_analysis['execution_logs']['total_found'] = len(exec_records)
            impact_analysis['execution_logs']['records'] = [dict(row) for row in exec_records]
        
        # å­˜åœ¨ã—ãªã„execution_idã‚’è­¦å‘Š
        found_ids = {record['execution_id'] for record in impact_analysis['execution_logs']['records']}
        missing_ids = set(execution_ids) - found_ids
        if missing_ids:
            impact_analysis['warnings'].append(f"å­˜åœ¨ã—ãªã„execution_id: {missing_ids}")
        
        # analyses ã®å½±éŸ¿ç¢ºèª
        if found_ids:
            with sqlite3.connect(self.analysis_db_path) as analysis_conn:
                analysis_conn.row_factory = sqlite3.Row
                
                placeholders = ','.join(['?' for _ in found_ids])
                cursor = analysis_conn.execute(f"""
                    SELECT id, symbol, timeframe, config, chart_path, compressed_path, 
                           execution_id, generated_at
                    FROM analyses 
                    WHERE execution_id IN ({placeholders})
                """, list(found_ids))
                
                analysis_records = cursor.fetchall()
                impact_analysis['analyses']['total_affected'] = len(analysis_records)
                impact_analysis['analyses']['records'] = [dict(row) for row in analysis_records]
                
                # çµ±è¨ˆé›†è¨ˆ
                for record in analysis_records:
                    symbol = record['symbol']
                    config = record['config']
                    
                    if symbol not in impact_analysis['analyses']['by_symbol']:
                        impact_analysis['analyses']['by_symbol'][symbol] = 0
                    impact_analysis['analyses']['by_symbol'][symbol] += 1
                    
                    if config not in impact_analysis['analyses']['by_config']:
                        impact_analysis['analyses']['by_config'][config] = 0
                    impact_analysis['analyses']['by_config'][config] += 1
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã®ç¢ºèª
                    if record['chart_path']:
                        chart_path = Path(record['chart_path'])
                        if chart_path.exists():
                            impact_analysis['file_artifacts']['chart_files'].append(str(chart_path))
                            impact_analysis['file_artifacts']['total_size'] += chart_path.stat().st_size
                    
                    if record['compressed_path']:
                        compressed_path = Path(record['compressed_path'])
                        if compressed_path.exists():
                            impact_analysis['file_artifacts']['compressed_files'].append(str(compressed_path))
                            impact_analysis['file_artifacts']['total_size'] += compressed_path.stat().st_size
        
        # çµæœè¡¨ç¤º
        print(f"ğŸ“‹ å®Ÿè¡Œãƒ­ã‚°: {impact_analysis['execution_logs']['total_found']}ä»¶ãŒå‰Šé™¤å¯¾è±¡")
        for record in impact_analysis['execution_logs']['records'][:5]:
            print(f"   - {record['execution_id']}: {record['symbol']} ({record['status']})")
        if len(impact_analysis['execution_logs']['records']) > 5:
            print(f"   ... ä»– {len(impact_analysis['execution_logs']['records']) - 5}ä»¶")
        
        print(f"\nğŸ“Š åˆ†æçµæœ: {impact_analysis['analyses']['total_affected']}ä»¶ãŒå‰Šé™¤å¯¾è±¡")
        if impact_analysis['analyses']['by_symbol']:
            print("   éŠ˜æŸ„åˆ¥:")
            for symbol, count in sorted(impact_analysis['analyses']['by_symbol'].items()):
                print(f"     {symbol}: {count}ä»¶")
        
        if impact_analysis['analyses']['by_config']:
            print("   æˆ¦ç•¥åˆ¥:")
            for config, count in sorted(impact_analysis['analyses']['by_config'].items()):
                print(f"     {config}: {count}ä»¶")
        
        file_count = len(impact_analysis['file_artifacts']['chart_files']) + len(impact_analysis['file_artifacts']['compressed_files'])
        if file_count > 0:
            size_mb = impact_analysis['file_artifacts']['total_size'] / (1024 * 1024)
            print(f"\nğŸ’¾ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«: {file_count}ä»¶ ({size_mb:.1f}MB)")
        
        if impact_analysis['warnings']:
            print(f"\nâš ï¸ è­¦å‘Š:")
            for warning in impact_analysis['warnings']:
                print(f"   {warning}")
        
        return impact_analysis
    
    def backup_before_deletion(self, execution_ids: List[str]) -> Dict:
        """å‰Šé™¤å‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = Path(f"backups/cascade_deletion_{timestamp}")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backups = {}
        
        # execution_logs.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        exec_backup = backup_dir / "execution_logs_backup.db"
        shutil.copy2(self.execution_db_path, exec_backup)
        backups['execution'] = str(exec_backup)
        print(f"âœ… execution_logs.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {exec_backup}")
        
        # analysis.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        analysis_backup = backup_dir / "analysis_backup.db"
        shutil.copy2(self.analysis_db_path, analysis_backup)
        backups['analysis'] = str(analysis_backup)
        print(f"âœ… analysis.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {analysis_backup}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’è¨˜éŒ²
        backup_info = {
            'timestamp': timestamp,
            'backup_dir': str(backup_dir),
            'backups': backups,
            'target_execution_ids': execution_ids,
            'original_execution_size': self.execution_db_path.stat().st_size,
            'original_analysis_size': self.analysis_db_path.stat().st_size
        }
        
        info_file = backup_dir / "backup_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        return backup_info
    
    def execute_cascade_deletion(self, execution_ids: List[str], 
                                impact_analysis: Dict, 
                                backup_info: Dict,
                                dry_run: bool = True,
                                delete_files: bool = False) -> Dict:
        """ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚’å®Ÿè¡Œ"""
        print(f"\nğŸ—‘ï¸ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤å®Ÿè¡Œ {'(DRY RUN)' if dry_run else ''}")
        print("-" * 40)
        
        deletion_summary = {
            'execution_logs_deleted': 0,
            'analyses_deleted': 0,
            'files_deleted': 0,
            'files_size_freed': 0,
            'errors': []
        }
        
        if impact_analysis['execution_logs']['total_found'] == 0:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®execution_logsãŒã‚ã‚Šã¾ã›ã‚“")
            return deletion_summary
        
        try:
            # 1. åˆ†æçµæœå‰Šé™¤
            if impact_analysis['analyses']['total_affected'] > 0:
                if dry_run:
                    print(f"ğŸ” [DRY RUN] åˆ†æçµæœå‰Šé™¤äºˆå®š: {impact_analysis['analyses']['total_affected']}ä»¶")
                else:
                    with sqlite3.connect(self.analysis_db_path) as analysis_conn:
                        found_ids = [record['execution_id'] for record in impact_analysis['execution_logs']['records']]
                        placeholders = ','.join(['?' for _ in found_ids])
                        
                        cursor = analysis_conn.execute(f"""
                            DELETE FROM analyses WHERE execution_id IN ({placeholders})
                        """, found_ids)
                        
                        deletion_summary['analyses_deleted'] = cursor.rowcount
                        analysis_conn.commit()
                        print(f"âœ… åˆ†æçµæœå‰Šé™¤: {deletion_summary['analyses_deleted']}ä»¶")
            
            # 2. é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if delete_files and impact_analysis['file_artifacts']['total_size'] > 0:
                all_files = (impact_analysis['file_artifacts']['chart_files'] + 
                            impact_analysis['file_artifacts']['compressed_files'])
                
                if dry_run:
                    size_mb = impact_analysis['file_artifacts']['total_size'] / (1024 * 1024)
                    print(f"ğŸ” [DRY RUN] ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤äºˆå®š: {len(all_files)}ä»¶ ({size_mb:.1f}MB)")
                else:
                    deleted_files = 0
                    deleted_size = 0
                    
                    for file_path in all_files:
                        try:
                            path = Path(file_path)
                            if path.exists():
                                file_size = path.stat().st_size
                                path.unlink()
                                deleted_files += 1
                                deleted_size += file_size
                        except Exception as e:
                            deletion_summary['errors'].append(f"ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                    
                    deletion_summary['files_deleted'] = deleted_files
                    deletion_summary['files_size_freed'] = deleted_size
                    size_mb = deleted_size / (1024 * 1024)
                    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_files}ä»¶ ({size_mb:.1f}MB)")
            
            # 3. å®Ÿè¡Œãƒ­ã‚°å‰Šé™¤ï¼ˆæœ€å¾Œã«å®Ÿè¡Œï¼‰
            if dry_run:
                print(f"ğŸ” [DRY RUN] å®Ÿè¡Œãƒ­ã‚°å‰Šé™¤äºˆå®š: {impact_analysis['execution_logs']['total_found']}ä»¶")
            else:
                with sqlite3.connect(self.execution_db_path) as exec_conn:
                    found_ids = [record['execution_id'] for record in impact_analysis['execution_logs']['records']]
                    placeholders = ','.join(['?' for _ in found_ids])
                    
                    cursor = exec_conn.execute(f"""
                        DELETE FROM execution_logs WHERE execution_id IN ({placeholders})
                    """, found_ids)
                    
                    deletion_summary['execution_logs_deleted'] = cursor.rowcount
                    exec_conn.commit()
                    print(f"âœ… å®Ÿè¡Œãƒ­ã‚°å‰Šé™¤: {deletion_summary['execution_logs_deleted']}ä»¶")
                    
                    # VACUUM ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
                    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ä¸­...")
                    exec_conn.execute("VACUUM")
                    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")
        
        except Exception as e:
            deletion_summary['errors'].append(str(e))
            print(f"âŒ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        
        return deletion_summary
    
    def safe_cascade_delete(self, execution_ids: List[str], 
                          dry_run: bool = True,
                          delete_files: bool = False,
                          skip_backup: bool = False) -> bool:
        """å®‰å…¨ãªã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ï¼ˆå…¨ä½“ãƒ•ãƒ­ãƒ¼ï¼‰"""
        print("ğŸ—‘ï¸ å®‰å…¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        
        if not execution_ids:
            print("âŒ å‰Šé™¤å¯¾è±¡ã®execution_idãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: å½±éŸ¿ç¯„å›²åˆ†æ
            print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: å‰Šé™¤å½±éŸ¿ç¯„å›²åˆ†æ")
            impact_analysis = self.analyze_deletion_impact(execution_ids)
            
            if impact_analysis['execution_logs']['total_found'] == 0:
                print("âš ï¸ æŒ‡å®šã•ã‚ŒãŸexecution_idã¯å­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if not skip_backup and not dry_run:
                print("\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
                backup_info = self.backup_before_deletion(execution_ids)
            else:
                backup_info = {'backup_dir': 'ã‚¹ã‚­ãƒƒãƒ—', 'backups': {}}
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: å‰Šé™¤ç¢ºèª
            total_impact = (impact_analysis['execution_logs']['total_found'] + 
                          impact_analysis['analyses']['total_affected'])
            
            print(f"\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—3: å‰Šé™¤ç¢ºèª")
            print(f"å‰Šé™¤å¯¾è±¡: execution_logs {impact_analysis['execution_logs']['total_found']}ä»¶ + "
                  f"analyses {impact_analysis['analyses']['total_affected']}ä»¶ = è¨ˆ{total_impact}ä»¶")
            
            if not dry_run:
                print("âš ï¸ å®Ÿéš›ã®å‰Šé™¤ãŒå®Ÿè¡Œã•ã‚Œã¾ã™")
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤å®Ÿè¡Œ
            print(f"\nğŸ—‘ï¸ ã‚¹ãƒ†ãƒƒãƒ—4: ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤å®Ÿè¡Œ")
            deletion_summary = self.execute_cascade_deletion(
                execution_ids, impact_analysis, backup_info, dry_run, delete_files
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: çµæœãƒ¬ãƒãƒ¼ãƒˆ
            self._generate_deletion_report(impact_analysis, deletion_summary, backup_info, dry_run)
            
            return len(deletion_summary['errors']) == 0
            
        except Exception as e:
            print(f"âŒ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _generate_deletion_report(self, impact_analysis, deletion_summary, backup_info, dry_run):
        """å‰Šé™¤ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print(f"\nğŸ“‹ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 50)
        
        if dry_run:
            print("ğŸ” DRY RUN çµæœ:")
            print(f"  å‰Šé™¤äºˆå®š execution_logs: {impact_analysis['execution_logs']['total_found']}ä»¶")
            print(f"  å‰Šé™¤äºˆå®š analyses: {impact_analysis['analyses']['total_affected']}ä»¶")
            if impact_analysis['file_artifacts']['total_size'] > 0:
                size_mb = impact_analysis['file_artifacts']['total_size'] / (1024 * 1024)
                file_count = (len(impact_analysis['file_artifacts']['chart_files']) + 
                            len(impact_analysis['file_artifacts']['compressed_files']))
                print(f"  å‰Šé™¤äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«: {file_count}ä»¶ ({size_mb:.1f}MB)")
        else:
            print("å®Ÿè¡Œçµæœ:")
            print(f"  å‰Šé™¤æ¸ˆã¿ execution_logs: {deletion_summary['execution_logs_deleted']}ä»¶")
            print(f"  å‰Šé™¤æ¸ˆã¿ analyses: {deletion_summary['analyses_deleted']}ä»¶")
            if deletion_summary['files_deleted'] > 0:
                size_mb = deletion_summary['files_size_freed'] / (1024 * 1024)
                print(f"  å‰Šé™¤æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {deletion_summary['files_deleted']}ä»¶ ({size_mb:.1f}MB)")
            
            if backup_info['backup_dir'] != 'ã‚¹ã‚­ãƒƒãƒ—':
                print(f"\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:")
                print(f"  å ´æ‰€: {backup_info['backup_dir']}")
        
        if deletion_summary['errors']:
            print(f"\nã‚¨ãƒ©ãƒ¼:")
            for error in deletion_summary['errors']:
                print(f"  âŒ {error}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('execution_ids', nargs='+', 
                       help='å‰Šé™¤ã™ã‚‹execution_idã®ãƒªã‚¹ãƒˆ')
    parser.add_argument('--dry-run', action='store_true',
                       help='å®Ÿéš›ã®å‰Šé™¤ã‚’è¡Œã‚ãšã€å‰Šé™¤äºˆå®šã®ã¿ã‚’è¡¨ç¤º')
    parser.add_argument('--delete-files', action='store_true',
                       help='é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒ£ãƒ¼ãƒˆã€åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚‚å‰Šé™¤')
    parser.add_argument('--skip-backup', action='store_true',
                       help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--analysis-only', action='store_true',
                       help='å½±éŸ¿ç¯„å›²åˆ†æã®ã¿å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    print("ğŸ—‘ï¸ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    if args.dry_run:
        print("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰ - å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã„ã¾ã›ã‚“")
    
    try:
        cascade_system = CascadeDeletionSystem()
        
        if args.analysis_only:
            print("\nğŸ“Š å½±éŸ¿ç¯„å›²åˆ†æã®ã¿å®Ÿè¡Œ")
            cascade_system.analyze_deletion_impact(args.execution_ids)
            return
        
        success = cascade_system.safe_cascade_delete(
            execution_ids=args.execution_ids,
            dry_run=args.dry_run,
            delete_files=args.delete_files,
            skip_backup=args.skip_backup
        )
        
        if success:
            if args.dry_run:
                print("\nğŸ” DRY RUN å®Œäº† - --dry-run ãƒ•ãƒ©ã‚°ã‚’å¤–ã—ã¦å®Ÿéš›ã®å‰Šé™¤ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            else:
                print("\nâœ… ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            print("\nâŒ ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å‰Šé™¤ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            sys.exit(1)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()