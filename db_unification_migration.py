#!/usr/bin/env python3
"""
DBçµ±ä¸€ã®ãŸã‚ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®‰å…¨ã«web_dashboard/execution_logs.dbã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ«ãƒ¼ãƒˆDBã«çµ±åˆã™ã‚‹
"""

import os
import sys
import sqlite3
import shutil
from pathlib import Path
from datetime import datetime
import json

def backup_databases():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_dir = Path(f"backups/migration_{timestamp}")
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    root_db = Path("execution_logs.db")
    web_db = Path("web_dashboard/execution_logs.db")
    
    backups = {}
    
    if root_db.exists():
        backup_root = backup_dir / "execution_logs_root_backup.db"
        shutil.copy2(root_db, backup_root)
        backups['root'] = str(backup_root)
        print(f"âœ… ãƒ«ãƒ¼ãƒˆDBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_root}")
    
    if web_db.exists():
        backup_web = backup_dir / "execution_logs_web_backup.db"
        shutil.copy2(web_db, backup_web)
        backups['web'] = str(backup_web)
        print(f"âœ… WebDBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_web}")
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’è¨˜éŒ²
    backup_info = {
        'timestamp': timestamp,
        'backup_dir': str(backup_dir),
        'backups': backups,
        'original_root_exists': root_db.exists(),
        'original_web_exists': web_db.exists()
    }
    
    info_file = backup_dir / "backup_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(backup_info, f, indent=2, ensure_ascii=False)
    
    return backup_info

def analyze_databases():
    """ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦çµ±åˆè¨ˆç”»ã‚’ç«‹ã¦ã‚‹"""
    root_db = Path("execution_logs.db")
    web_db = Path("web_dashboard/execution_logs.db")
    
    analysis = {
        'root_db': {'exists': False, 'count': 0, 'sample_records': []},
        'web_db': {'exists': False, 'count': 0, 'sample_records': []},
        'conflicts': [],
        'merge_plan': {}
    }
    
    # ãƒ«ãƒ¼ãƒˆDBåˆ†æ
    if root_db.exists():
        with sqlite3.connect(root_db) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM execution_logs")
            count = cursor.fetchone()[0]
            analysis['root_db'] = {
                'exists': True,
                'count': count,
                'sample_records': []
            }
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰å–å¾—
            cursor = conn.execute("""
                SELECT execution_id, execution_type, symbol, status, timestamp_start 
                FROM execution_logs 
                ORDER BY timestamp_start DESC 
                LIMIT 5
            """)
            analysis['root_db']['sample_records'] = [
                dict(zip([col[0] for col in cursor.description], row))
                for row in cursor.fetchall()
            ]
    
    # WebDBåˆ†æ
    if web_db.exists():
        with sqlite3.connect(web_db) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM execution_logs")
            count = cursor.fetchone()[0]
            analysis['web_db'] = {
                'exists': True,
                'count': count,
                'sample_records': []
            }
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ã‚³ãƒ¼ãƒ‰å–å¾—
            cursor = conn.execute("""
                SELECT execution_id, execution_type, symbol, status, timestamp_start 
                FROM execution_logs 
                ORDER BY timestamp_start DESC 
                LIMIT 5
            """)
            analysis['web_db']['sample_records'] = [
                dict(zip([col[0] for col in cursor.description], row))
                for row in cursor.fetchall()
            ]
    
    # ç«¶åˆæ¤œå‡º
    if analysis['root_db']['exists'] and analysis['web_db']['exists']:
        with sqlite3.connect(web_db) as web_conn:
            web_conn.execute(f"ATTACH DATABASE '{root_db}' AS root_db")
            
            # é‡è¤‡ã™ã‚‹execution_idã‚’æ¤œç´¢
            cursor = web_conn.execute("""
                SELECT w.execution_id, w.status as web_status, r.status as root_status,
                       w.timestamp_start as web_time, r.timestamp_start as root_time
                FROM execution_logs w
                JOIN root_db.execution_logs r ON w.execution_id = r.execution_id
            """)
            
            conflicts = []
            for row in cursor.fetchall():
                conflicts.append({
                    'execution_id': row[0],
                    'web_status': row[1],
                    'root_status': row[2],
                    'web_timestamp': row[3],
                    'root_timestamp': row[4]
                })
            
            analysis['conflicts'] = conflicts
    
    return analysis

def migrate_data(dry_run=True):
    """ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«çµ±åˆ"""
    root_db = Path("execution_logs.db")
    web_db = Path("web_dashboard/execution_logs.db")
    
    if not web_db.exists():
        print("âŒ Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰DBãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return False
    
    if not root_db.exists():
        print("â„¹ï¸ ãƒ«ãƒ¼ãƒˆDBãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€WebDBã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™")
        if not dry_run:
            shutil.copy2(web_db, root_db)
            print(f"âœ… {web_db} ã‚’ {root_db} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
        else:
            print(f"ğŸ” [DRY RUN] {web_db} ã‚’ {root_db} ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹äºˆå®š")
        return True
    
    # ä¸¡æ–¹ã®DBãŒå­˜åœ¨ã™ã‚‹å ´åˆã®çµ±åˆ
    print("ğŸ”„ ä¸¡æ–¹ã®DBãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã™...")
    
    if not dry_run:
        with sqlite3.connect(web_db) as web_conn:
            web_conn.execute(f"ATTACH DATABASE '{root_db}' AS root_db")
            
            # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã¯ç„¡è¦–ï¼‰
            cursor = web_conn.execute("""
                INSERT OR IGNORE INTO root_db.execution_logs 
                SELECT * FROM execution_logs 
                WHERE execution_id NOT IN (
                    SELECT execution_id FROM root_db.execution_logs
                )
            """)
            
            inserted_count = cursor.rowcount
            print(f"âœ… {inserted_count}ä»¶ã®æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’çµ±åˆã—ã¾ã—ãŸ")
            
            # çµ±åˆå¾Œã®ç¢ºèª
            cursor = web_conn.execute("SELECT COUNT(*) FROM root_db.execution_logs")
            total_count = cursor.fetchone()[0]
            print(f"ğŸ“Š çµ±åˆå¾Œã®ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_count}")
            
            return True
    else:
        # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼šçµ±åˆã•ã‚Œã‚‹äºˆå®šã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
        with sqlite3.connect(web_db) as web_conn:
            web_conn.execute(f"ATTACH DATABASE '{root_db}' AS root_db")
            
            cursor = web_conn.execute("""
                SELECT execution_id, symbol, status, timestamp_start
                FROM execution_logs 
                WHERE execution_id NOT IN (
                    SELECT execution_id FROM root_db.execution_logs
                )
                ORDER BY timestamp_start DESC
            """)
            
            new_records = cursor.fetchall()
            print(f"ğŸ” [DRY RUN] {len(new_records)}ä»¶ã®æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒçµ±åˆã•ã‚Œã‚‹äºˆå®š:")
            for record in new_records[:10]:  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
                print(f"  - {record[0]}: {record[1]} ({record[2]}) at {record[3]}")
            
            if len(new_records) > 10:
                print(f"  ... ä»– {len(new_records) - 10}ä»¶")
            
            return True

def update_web_dashboard_config(dry_run=True):
    """Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒãƒ«ãƒ¼ãƒˆDBã‚’å‚ç…§ã™ã‚‹ã‚ˆã†è¨­å®šã‚’æ›´æ–°"""
    app_py_path = Path("web_dashboard/app.py")
    
    if not app_py_path.exists():
        print(f"âŒ {app_py_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return False
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(app_py_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä¿®æ­£å¯¾è±¡ã‚’æ¤œç´¢
    target_line = "exec_db_path = 'execution_logs.db'"
    replacement_line = "exec_db_path = '../execution_logs.db'  # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®DBã‚’å‚ç…§"
    
    if target_line in content:
        if not dry_run:
            new_content = content.replace(target_line, replacement_line)
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            backup_path = app_py_path.with_suffix('.py.backup')
            shutil.copy2(app_py_path, backup_path)
            print(f"ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            with open(app_py_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"âœ… {app_py_path} ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            print(f"   å¤‰æ›´: {target_line}")
            print(f"   â†’   {replacement_line}")
        else:
            print(f"ğŸ” [DRY RUN] {app_py_path} ã‚’æ›´æ–°ã™ã‚‹äºˆå®š:")
            print(f"  å¤‰æ›´: {target_line}")
            print(f"  â†’   {replacement_line}")
        return True
    else:
        print(f"âš ï¸ {app_py_path} ã«å¯¾è±¡ã®è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print(f"   æ¤œç´¢å¯¾è±¡: {target_line}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DBçµ±ä¸€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³')
    parser.add_argument('--dry-run', action='store_true', 
                       help='å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãšã€å¤‰æ›´äºˆå®šã®ã¿ã‚’è¡¨ç¤º')
    parser.add_argument('--backup-only', action='store_true',
                       help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã¿å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    print("ğŸ”§ DBçµ±ä¸€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    
    if args.dry_run:
        print("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰ - å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    print("\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
    backup_info = backup_databases()
    
    if args.backup_only:
        print("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã¿å®Œäº†ã—ã¾ã—ãŸ")
        return
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ
    print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ")
    analysis = analyze_databases()
    
    print(f"ãƒ«ãƒ¼ãƒˆDB: {analysis['root_db']['count']}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
    print(f"WebDB: {analysis['web_db']['count']}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    if analysis['conflicts']:
        print(f"âš ï¸ ç«¶åˆãƒ¬ã‚³ãƒ¼ãƒ‰: {len(analysis['conflicts'])}ä»¶")
        for conflict in analysis['conflicts'][:5]:
            print(f"  - {conflict['execution_id']}: Web({conflict['web_status']}) vs Root({conflict['root_status']})")
    else:
        print("âœ… ç«¶åˆãªã—")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    success = migrate_data(dry_run=args.dry_run)
    
    if not success:
        print("âŒ ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šæ›´æ–°
    print("\nâš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—4: Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šæ›´æ–°")
    config_success = update_web_dashboard_config(dry_run=args.dry_run)
    
    if not config_success:
        print("âŒ è¨­å®šæ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    print("\n" + "=" * 50)
    if args.dry_run:
        print("ğŸ” DRY RUN å®Œäº† - --dry-run ãƒ•ãƒ©ã‚°ã‚’å¤–ã—ã¦å®Ÿéš›ã®å¤‰æ›´ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    else:
        print("âœ… DBçµ±ä¸€ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
        print(f"ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€: {backup_info['backup_dir']}")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’å†èµ·å‹•")
        print("2. éŠ˜æŸ„è¿½åŠ ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ")
        print("3. å…¨ä½“çš„ãªå‹•ä½œç¢ºèª")

if __name__ == "__main__":
    main()