"""
é«˜ç²¾åº¦MLäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

ç¾åœ¨57%ã®ç²¾åº¦ã‚’70%ä»¥ä¸Šã«æ”¹å–„ã™ã‚‹æ–°ã—ã„äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã€‚
ä¸»è¦æ”¹å–„ç‚¹:
1. é‡è¦ãªåŸºæœ¬ç‰¹å¾´é‡ã®å¾©æ´»
2. æ”¹å–„ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
3. æ™‚ç³»åˆ—çš„ç‰¹å¾´é‡ã®è¿½åŠ 
4. ã‚·ãƒ³ãƒ—ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
"""

import pandas as pd
import numpy as np
import warnings
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb

try:
    import xgboost as xgb
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ XGBoostãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚LightGBMã¨RandomForestã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

import joblib
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from interfaces import IBreakoutPredictor, SupportResistanceLevel, BreakoutPrediction

warnings.filterwarnings('ignore')

class EnhancedMLPredictor(IBreakoutPredictor):
    """
    é«˜ç²¾åº¦MLäºˆæ¸¬å™¨
    
    æ”¹å–„ç‚¹:
    - åŸºæœ¬ç‰¹å¾´é‡ã®å¾©æ´»ï¼ˆclose, high, lowç­‰ï¼‰
    - æ”¹å–„ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - æ™‚ç³»åˆ—çš„ç‰¹å¾´é‡
    - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
    """
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_columns = []
        self.accuracy_metrics = {}
        
        # æ”¹å–„ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.model_params = {
            'xgb': {
                'n_estimators': 300,
                'max_depth': 8,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'random_state': 42
            },
            'lgb': {
                'n_estimators': 400,
                'max_depth': 10,
                'learning_rate': 0.03,
                'num_leaves': 64,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'objective': 'binary',
                'metric': 'auc',
                'random_state': 42,
                'verbosity': -1
            },
            'rf': {
                'n_estimators': 200,
                'max_depth': 12,
                'min_samples_split': 5,
                'min_samples_leaf': 2,
                'max_features': 'sqrt',
                'random_state': 42
            }
        }
    
    def create_enhanced_features(self, data: pd.DataFrame, levels: List[SupportResistanceLevel]) -> pd.DataFrame:
        """
        æ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        
        é‡è¦ãªæ”¹å–„ç‚¹:
        1. åŸºæœ¬ç‰¹å¾´é‡ã®å¾©æ´»
        2. æ™‚ç³»åˆ—çš„ç‰¹å¾´é‡ã®è¿½åŠ 
        3. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã®è¿½åŠ 
        """
        
        features = data.copy()
        
        # === 1. åŸºæœ¬ç‰¹å¾´é‡ã®å¾©æ´»ï¼ˆéåº¦ã«å‰Šé™¤ã•ã‚Œã¦ã„ãŸé‡è¦ç‰¹å¾´é‡ï¼‰ ===
        if 'close' not in features.columns:
            print("âš ï¸ åŸºæœ¬ä¾¡æ ¼æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return pd.DataFrame()
        
        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        features['price_return'] = features['close'].pct_change()
        features['log_return'] = np.log(features['close'] / features['close'].shift(1))
        
        # ç§»å‹•å¹³å‡ï¼ˆå¾©æ´»ï¼‰
        features['sma_5'] = features['close'].rolling(5).mean()
        features['sma_10'] = features['close'].rolling(10).mean()
        features['sma_20'] = features['close'].rolling(20).mean()
        features['ema_5'] = features['close'].ewm(span=5).mean()
        features['ema_10'] = features['close'].ewm(span=10).mean()
        features['ema_20'] = features['close'].ewm(span=20).mean()
        
        # ç§»å‹•å¹³å‡ä¹–é›¢ç‡ï¼ˆå¾©æ´»ï¼‰
        features['sma_5_deviation'] = (features['close'] - features['sma_5']) / features['sma_5']
        features['sma_20_deviation'] = (features['close'] - features['sma_20']) / features['sma_20']
        features['ema_20_deviation'] = (features['close'] - features['ema_20']) / features['ema_20']
        
        # === 2. æŠ€è¡“çš„æŒ‡æ¨™ã®æ”¹å–„ç‰ˆ ===
        # RSIï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        features['rsi_14'] = self._calculate_rsi(features['close'], 14)
        features['rsi_7'] = self._calculate_rsi(features['close'], 7)
        features['rsi_21'] = self._calculate_rsi(features['close'], 21)
        
        # MACDï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        macd_data = self._calculate_macd(features['close'])
        features['macd'] = macd_data['macd']
        features['macd_signal'] = macd_data['signal']
        features['macd_histogram'] = macd_data['histogram']
        features['macd_divergence'] = features['macd'] - features['macd_signal']
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆå¾©æ´»ï¼‰
        bb_data = self._calculate_bollinger_bands(features['close'])
        features['bb_upper'] = bb_data['upper']
        features['bb_middle'] = bb_data['middle']
        features['bb_lower'] = bb_data['lower']
        features['bb_position'] = (features['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
        features['bb_squeeze'] = (features['bb_upper'] - features['bb_lower']) / features['bb_middle']
        
        # === 3. æ™‚ç³»åˆ—çš„ç‰¹å¾´é‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰ ===
        # ä¾¡æ ¼ã®åŠ é€Ÿåº¦ï¼ˆ2æ¬¡å¾®åˆ†ï¼‰
        features['price_acceleration'] = features['close'].diff().diff()
        features['volume_acceleration'] = features['volume'].diff().diff()
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        features['trend_strength'] = self._calculate_trend_strength(features['close'])
        
        # å‡ºæ¥é«˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        features['volume_sma_ratio'] = features['volume'] / features['volume'].rolling(20).mean()
        features['volume_trend'] = features['volume'].rolling(5).apply(lambda x: np.polyfit(range(5), x, 1)[0] if len(x) == 5 else 0)
        
        # === 4. ãƒ¬ãƒ™ãƒ«ç‰¹ç•°çš„ç‰¹å¾´é‡ ===
        if levels:
            level_features = self._add_level_features(features, levels)
            # ãƒ¬ãƒ™ãƒ«ç‰¹å¾´é‡ã®è¿½åŠ ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            if level_features is None:
                return None  # ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚¹ã‚­ãƒƒãƒ—
            features = level_features
        else:
            # ãƒ¬ãƒ™ãƒ«ãŒç©ºã®å ´åˆã¯ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            print("âš ï¸ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“ - ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return None  # ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚¹ã‚­ãƒƒãƒ—
        
        # === 5. åŸºæœ¬çš„ãªä¾¡æ ¼ãƒ¬ãƒ³ã‚¸ç‰¹å¾´é‡ï¼ˆå¿…è¦ãªç‰¹å¾´é‡ã‚’å…ˆã«ä½œæˆï¼‰ ===
        features['high_low_ratio'] = (features['high'] - features['low']) / features['close']
        features['close_location'] = (features['close'] - features['low']) / (features['high'] - features['low'])
        
        # === 6. ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰ ===
        features['rsi_volume_interaction'] = features['rsi_14'] * features['volume_sma_ratio']
        features['macd_volatility_interaction'] = features['macd_histogram'] * features['high_low_ratio']
        features['price_volume_interaction'] = features['price_return'] * features['volume_trend']
        
        # === 7. çµ±è¨ˆçš„ç‰¹å¾´é‡ ===
        # ä¾¡æ ¼ã®çµ±è¨ˆï¼ˆçŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸï¼‰
        for window in [5, 10, 20]:
            features[f'price_std_{window}'] = features['close'].rolling(window).std()
            features[f'price_skew_{window}'] = features['close'].rolling(window).skew()
            features[f'volume_std_{window}'] = features['volume'].rolling(window).std()
        
        # === 8. ãƒ©ã‚°ç‰¹å¾´é‡ï¼ˆæ™‚ç³»åˆ—æƒ…å ±ï¼‰ ===
        for lag in [1, 2, 3, 5]:
            features[f'close_lag_{lag}'] = features['close'].shift(lag)
            features[f'volume_lag_{lag}'] = features['volume'].shift(lag)
            features[f'rsi_lag_{lag}'] = features['rsi_14'].shift(lag)
        
        # NaNå€¤ã®å‡¦ç†
        features = features.fillna(method='ffill').fillna(method='bfill')
        
        print(f"âœ… æ‹¡å¼µç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(features.columns)}å€‹ã®ç‰¹å¾´é‡")
        return features
    
    def _add_level_features(self, features: pd.DataFrame, levels: List[SupportResistanceLevel]) -> pd.DataFrame:
        """ãƒ¬ãƒ™ãƒ«ç‰¹ç•°çš„ç‰¹å¾´é‡ã‚’è¿½åŠ """
        
        current_price = features['close'].iloc[-1]
        
        # æœ€ã‚‚è¿‘ã„ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹
        supports = [l for l in levels if l.level_type == 'support' and l.price < current_price]
        resistances = [l for l in levels if l.level_type == 'resistance' and l.price > current_price]
        
        if supports:
            nearest_support = min(supports, key=lambda x: abs(x.price - current_price))
            features['support_distance'] = (current_price - nearest_support.price) / current_price
            features['support_strength'] = nearest_support.strength
            features['support_touches'] = nearest_support.touch_count
        else:
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã—ã¦ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—ã‚’ä¿ƒã™
            return None  # ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚¹ã‚­ãƒƒãƒ—
        
        if resistances:
            nearest_resistance = min(resistances, key=lambda x: abs(x.price - current_price))
            features['resistance_distance'] = (nearest_resistance.price - current_price) / current_price
            features['resistance_strength'] = nearest_resistance.strength
            features['resistance_touches'] = nearest_resistance.touch_count
        else:
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Noneã‚’è¿”ã—ã¦ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—ã‚’ä¿ƒã™
            return None  # ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚¹ã‚­ãƒƒãƒ—
        
        # ãƒ¬ãƒ™ãƒ«é–“ãƒã‚¸ã‚·ãƒ§ãƒ³
        if supports and resistances:
            total_range = resistances[0].price - supports[0].price
            current_position = (current_price - supports[0].price) / total_range
            features['level_position'] = current_position
        else:
            # ã‚µãƒãƒ¼ãƒˆã¾ãŸã¯ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ¤œå‡ºã§ããªã„å ´åˆã¯ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            return None  # ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚¹ã‚­ãƒƒãƒ—
        
        return features
    
    def train_model(self, data: pd.DataFrame, levels: List[SupportResistanceLevel]) -> bool:
        """
        æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨“ç·´
        """
        try:
            print("ğŸ‹ï¸ é«˜ç²¾åº¦MLäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
            
            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features = self.create_enhanced_features(data, levels)
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯è¨“ç·´å¤±æ•—
            if features is None:
                print("âŒ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€MLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return False
            
            if len(features) < 200:
                print("âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ200ä»¶æœªæº€ï¼‰")
                return False
            
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«ã®ä½œæˆ
            X, y = self._create_training_data(features, levels)
            
            if len(X) < 30:
                print("âš ï¸ ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ30ä»¶æœªæº€ï¼‰")
                return False
            elif len(X) < 100:
                print(f"âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã‚ã§ã™ï¼ˆ{len(X)}ä»¶ï¼‰ã€‚ç²¾åº¦ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X)}ä»¶, ãƒã‚¸ãƒ†ã‚£ãƒ–æ¯”ç‡: {y.mean():.2f}")
            
            # ç‰¹å¾´é‡ã®æ¨™æº–åŒ–
            X_scaled = self.scaler.fit_transform(X)
            self.feature_columns = X.columns.tolist()
            
            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            self.models = {}
            cv_scores = {}
            
            # XGBoost
            if HAS_XGBOOST:
                print("  ğŸš€ XGBoostè¨“ç·´ä¸­...")
                self.models['xgb'] = xgb.XGBClassifier(**self.model_params['xgb'])
                self.models['xgb'].fit(X_scaled, y)
                cv_scores['xgb'] = self._cross_validate(self.models['xgb'], X_scaled, y)
            
            # LightGBM
            print("  âš¡ LightGBMè¨“ç·´ä¸­...")
            self.models['lgb'] = lgb.LGBMClassifier(**self.model_params['lgb'])
            self.models['lgb'].fit(X_scaled, y)
            cv_scores['lgb'] = self._cross_validate(self.models['lgb'], X_scaled, y)
            
            # RandomForest
            print("  ğŸŒ² RandomForestè¨“ç·´ä¸­...")
            self.models['rf'] = RandomForestClassifier(**self.model_params['rf'])
            self.models['rf'].fit(X_scaled, y)
            cv_scores['rf'] = self._cross_validate(self.models['rf'], X_scaled, y)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã®è¨ˆç®—
            self.ensemble_weights = self._calculate_ensemble_weights(cv_scores)
            
            # ç²¾åº¦è©•ä¾¡
            ensemble_score = np.average(list(cv_scores.values()), weights=list(self.ensemble_weights.values()))
            
            self.accuracy_metrics = {
                'ensemble_auc': ensemble_score,
                'individual_scores': cv_scores,
                'ensemble_weights': self.ensemble_weights
            }
            
            self.is_trained = True
            print(f"âœ… è¨“ç·´å®Œäº†! ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«AUC: {ensemble_score:.3f}")
            
            # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢è¡¨ç¤º
            for model_name, score in cv_scores.items():
                weight = self.ensemble_weights[model_name]
                print(f"  {model_name}: AUC={score:.3f}, é‡ã¿={weight:.2f}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def predict_breakout(self, current_data: pd.DataFrame, level: SupportResistanceLevel) -> BreakoutPrediction:
        """
        ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã«ã‚ˆã‚‹ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆç¢ºç‡äºˆæ¸¬
        """
        try:
            if not self.is_trained:
                print("âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return self._create_default_prediction(level)
            
            # ç‰¹å¾´é‡ä½œæˆ
            features = self.create_enhanced_features(current_data, [level])
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            if features is None:
                print("âš ï¸ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ã®å®Ÿãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ - ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None  # ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            
            if features.empty or len(features) < 10:
                print("âš ï¸ ååˆ†ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ - ã‚·ã‚°ãƒŠãƒ«æ¤œçŸ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None  # ã‚·ã‚°ãƒŠãƒ«ã‚¹ã‚­ãƒƒãƒ—
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
            X = features[self.feature_columns].iloc[-1:].fillna(0)
            X_scaled = self.scaler.transform(X)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            predictions = {}
            for model_name, model in self.models.items():
                if hasattr(model, 'predict_proba'):
                    pred = model.predict_proba(X_scaled)[0, 1]
                else:
                    pred = model.predict(X_scaled)[0]
                predictions[model_name] = pred
            
            # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
            breakout_prob = np.average(
                list(predictions.values()), 
                weights=[self.ensemble_weights[name] for name in predictions.keys()]
            )
            
            bounce_prob = 1.0 - breakout_prob
            
            # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆäºˆæ¸¬ã®ä¸€è‡´åº¦ã«åŸºã¥ãï¼‰
            prediction_std = np.std(list(predictions.values()))
            confidence = max(0.1, 1.0 - prediction_std * 2)
            
            # ä¾¡æ ¼ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—
            current_price = current_data['close'].iloc[-1]
            if level.level_type == 'resistance':
                target_price = level.price * 1.015  # 1.5%ä¸Š
            else:
                target_price = level.price * 0.985  # 1.5%ä¸‹
            
            return BreakoutPrediction(
                level=level,
                breakout_probability=float(breakout_prob),
                bounce_probability=float(bounce_prob),
                prediction_confidence=float(confidence),
                predicted_price_target=float(target_price),
                time_horizon_minutes=30,  # 30åˆ†äºˆæ¸¬
                model_name=f"EnhancedEnsemble_AUC{self.accuracy_metrics.get('ensemble_auc', 0.5):.2f}"
            )
            
        except Exception as e:
            print(f"âŒ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_default_prediction(level)
    
    def _create_training_data(self, features: pd.DataFrame, levels: List[SupportResistanceLevel]) -> Tuple[pd.DataFrame, pd.Series]:
        """
        æ”¹å–„ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ä½œæˆãƒ­ã‚¸ãƒƒã‚¯
        
        æ”¹å–„ç‚¹:
        - è·é›¢é–¾å€¤ã‚’2%ã‹ã‚‰1%ã«å¤‰æ›´
        - é€£ç¶šçš„ç›¸äº’ä½œç”¨ã®é™¤å»
        - ã‚ˆã‚Šå³å¯†ãªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆåˆ¤å®š
        """
        
        interactions = []
        
        for level in levels:
            level_price = level.price
            
            # ä¾¡æ ¼ãŒãƒ¬ãƒ™ãƒ«ã«æ¥è¿‘ã—ãŸæ™‚ç‚¹ã‚’æ¤œå‡ºï¼ˆè·é›¢é–¾å€¤1%ï¼‰
            distance_threshold = 0.01  # 2%ã‹ã‚‰1%ã«æ”¹å–„
            
            if level.level_type == 'resistance':
                near_level = features['close'] >= level_price * (1 - distance_threshold)
            else:  # support
                near_level = features['close'] <= level_price * (1 + distance_threshold)
            
            near_indices = features[near_level].index
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ•°å€¤å‹ã§ãªã„å ´åˆã¯æ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
            if len(near_indices) > 0 and not isinstance(near_indices[0], (int, float)):
                # å…ƒã®DataFrameã®ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                near_indices = features.reset_index().index[near_level.values]
            
            if len(near_indices) < 2:
                continue
            
            # é€£ç¶šã™ã‚‹æ¥è§¦ã‚’é™¤å»ï¼ˆæ”¹å–„ç‚¹ï¼‰
            filtered_indices = self._remove_consecutive_touches(near_indices)
            
            for idx in filtered_indices:
                if idx + 10 >= len(features):  # æœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                    continue
                
                # 10æœŸé–“å¾Œã®ä¾¡æ ¼å¤‰åŒ–ã‚’ç¢ºèª
                current_price = features.loc[idx, 'close']
                future_price = features.iloc[idx + 10]['close']
                price_change = (future_price - current_price) / current_price
                
                # ã‚ˆã‚Šå³å¯†ãªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆåˆ¤å®šï¼ˆæ”¹å–„ç‚¹ï¼‰
                if level.level_type == 'resistance':
                    # ä¸ŠæŠœã‘ã®å ´åˆï¼š1.5%ä»¥ä¸Šã®ä¸Šæ˜‡ã‚’ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã¨ã™ã‚‹
                    breakout = price_change > 0.015
                else:  # support
                    # ä¸‹æŠœã‘ã®å ´åˆï¼š1.5%ä»¥ä¸Šã®ä¸‹è½ã‚’ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã¨ã™ã‚‹
                    breakout = price_change < -0.015
                
                interactions.append({
                    'index': idx,
                    'breakout': breakout,
                    'price_change': price_change,
                    'level_type': level.level_type,
                    'level_strength': level.strength
                })
        
        if not interactions:
            print("âš ï¸ ãƒ¬ãƒ™ãƒ«ç›¸äº’ä½œç”¨ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame(), pd.Series()
        
        # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’çµåˆ
        interaction_df = pd.DataFrame(interactions)
        indices = interaction_df['index'].values
        
        X = features.iloc[indices][self._get_feature_columns(features)]
        y = interaction_df['breakout'].astype(int)
        
        print(f"ğŸ“Š ä½œæˆã•ã‚ŒãŸãƒ©ãƒ™ãƒ«: ç·æ•°{len(y)}, ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ{y.sum()}ä»¶ ({y.mean():.2%})")
        
        return X, y
    
    def _remove_consecutive_touches(self, indices: pd.Index) -> List[int]:
        """é€£ç¶šã™ã‚‹æ¥è§¦ã‚’é™¤å»"""
        if len(indices) <= 1:
            return list(indices)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ•´æ•°ã«å¤‰æ›
        indices_list = indices.tolist()
        filtered = [indices_list[0]]
        
        for i in range(1, len(indices_list)):
            # å‹å®‰å…¨ãªæ¯”è¼ƒ
            current_idx = indices_list[i]
            last_idx = filtered[-1]
            
            # ä¸¡æ–¹ã¨ã‚‚æ•°å€¤å‹ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
            if isinstance(current_idx, (int, float)) and isinstance(last_idx, (int, float)):
                if current_idx - last_idx > 5:  # 5æœŸé–“ä»¥ä¸Šé›¢ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿æ¡ç”¨
                    filtered.append(current_idx)
            else:
                # æ•°å€¤ã§ãªã„å ´åˆã¯ãã®ã¾ã¾è¿½åŠ ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
                filtered.append(current_idx)
        
        return filtered
    
    def _get_feature_columns(self, features: pd.DataFrame) -> List[str]:
        """äºˆæ¸¬ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’å–å¾—"""
        exclude_cols = ['timestamp', 'trades']
        return [col for col in features.columns if col not in exclude_cols]
    
    def _cross_validate(self, model, X, y, n_splits=5):
        """æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        tscv = TimeSeriesSplit(n_splits=n_splits)
        scores = []
        
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            model.fit(X_train, y_train)
            y_pred = model.predict_proba(X_val)[:, 1]
            score = roc_auc_score(y_val, y_pred)
            scores.append(score)
        
        return np.mean(scores)
    
    def _calculate_ensemble_weights(self, cv_scores: Dict[str, float]) -> Dict[str, float]:
        """CVçµæœã«åŸºã¥ãå‹•çš„é‡ã¿è¨ˆç®—"""
        # æ€§èƒ½ã«åŸºã¥ãé‡ã¿ä»˜ã‘
        total_score = sum(cv_scores.values())
        weights = {name: score / total_score for name, score in cv_scores.items()}
        
        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = sum(weights.values())
        weights = {name: weight / total_weight for name, weight in weights.items()}
        
        return weights
    
    def _create_default_prediction(self, level: SupportResistanceLevel) -> BreakoutPrediction:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ã‚’ä½œæˆ"""
        return BreakoutPrediction(
            level=level,
            breakout_probability=0.5,
            bounce_probability=0.5,
            prediction_confidence=0.3,
            predicted_price_target=None,
            time_horizon_minutes=30,
            model_name="EnhancedEnsemble_Default"
        )
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def _calculate_macd(self, prices: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Dict:
        """MACDè¨ˆç®—"""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        macd_signal = macd.ewm(span=signal).mean()
        macd_histogram = macd - macd_signal
        
        return {
            'macd': macd,
            'signal': macd_signal,
            'histogram': macd_histogram
        }
    
    def _calculate_bollinger_bands(self, prices: pd.Series, window: int = 20, std_dev: float = 2) -> Dict:
        """ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰è¨ˆç®—"""
        sma = prices.rolling(window=window).mean()
        std = prices.rolling(window=window).std()
        upper = sma + (std * std_dev)
        lower = sma - (std * std_dev)
        
        return {
            'upper': upper,
            'middle': sma,
            'lower': lower
        }
    
    def _calculate_trend_strength(self, prices: pd.Series, window: int = 20) -> pd.Series:
        """ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—"""
        returns = prices.pct_change()
        trend_strength = returns.rolling(window=window).apply(
            lambda x: np.sum(np.sign(x) == np.sign(x).iloc[-1]) / len(x)
        )
        return trend_strength
    
    def get_model_accuracy(self) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã‚’å–å¾—"""
        return self.accuracy_metrics
    
    def save_model(self, filepath: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        try:
            model_data = {
                'models': self.models,
                'scaler': self.scaler,
                'feature_columns': self.feature_columns,
                'ensemble_weights': self.ensemble_weights,
                'accuracy_metrics': self.accuracy_metrics,
                'is_trained': self.is_trained
            }
            joblib.dump(model_data, filepath)
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {filepath}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            model_data = joblib.load(filepath)
            self.models = model_data['models']
            self.scaler = model_data['scaler']
            self.feature_columns = model_data['feature_columns']
            self.ensemble_weights = model_data['ensemble_weights']
            self.accuracy_metrics = model_data['accuracy_metrics']
            self.is_trained = model_data['is_trained']
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    print("ğŸ§ª Enhanced ML Predictor ãƒ†ã‚¹ãƒˆ")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    sample_data = pd.DataFrame({
        'timestamp': pd.date_range('2024-01-01', periods=1000, freq='H'),
        'close': np.random.randn(1000).cumsum() + 100,
        'high': np.random.randn(1000).cumsum() + 105,
        'low': np.random.randn(1000).cumsum() + 95,
        'volume': np.random.uniform(1000, 10000, 1000)
    })
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ™ãƒ«
    from interfaces import SupportResistanceLevel
    sample_level = SupportResistanceLevel(
        price=100.0,
        strength=0.8,
        touch_count=3,
        level_type='resistance',
        first_touch=datetime.now(),
        last_touch=datetime.now(),
        volume_at_level=5000.0,
        distance_from_current=0.02
    )
    
    predictor = EnhancedMLPredictor()
    
    # ç‰¹å¾´é‡ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    features = predictor.create_enhanced_features(sample_data, [sample_level])
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆãƒ†ã‚¹ãƒˆ: {len(features.columns)}å€‹ã®ç‰¹å¾´é‡")
    
    print("ğŸ‰ Enhanced ML Predictorå®Ÿè£…å®Œäº†!")