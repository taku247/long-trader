#!/usr/bin/env python3
"""
å­¤ç«‹åˆ†æçµæœã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–“ã®å‚ç…§æ•´åˆæ€§ã‚’ä¿®å¾©ã—ã€ä¸è¦ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å‰Šé™¤
"""

import os
import sys
import sqlite3
import shutil
from pathlib import Path
from datetime import datetime, timedelta
import json
import argparse

class OrphanedDataCleanup:
    """å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, execution_db_path=None, analysis_db_path=None):
        self.execution_db_path = Path(execution_db_path or "execution_logs.db")
        self.analysis_db_path = Path(analysis_db_path or "web_dashboard/large_scale_analysis/analysis.db")
        
        if not self.execution_db_path.exists():
            raise FileNotFoundError(f"execution_logs.db not found: {self.execution_db_path}")
        if not self.analysis_db_path.exists():
            raise FileNotFoundError(f"analysis.db not found: {self.analysis_db_path}")
    
    def backup_databases(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = Path(f"backups/orphaned_cleanup_{timestamp}")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backups = {}
        
        # analysis.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        analysis_backup = backup_dir / "analysis_backup.db"
        shutil.copy2(self.analysis_db_path, analysis_backup)
        backups['analysis'] = str(analysis_backup)
        print(f"âœ… analysis.db ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {analysis_backup}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã‚’è¨˜éŒ²
        backup_info = {
            'timestamp': timestamp,
            'backup_dir': str(backup_dir),
            'backups': backups,
            'original_analysis_size': self.analysis_db_path.stat().st_size if self.analysis_db_path.exists() else 0
        }
        
        info_file = backup_dir / "backup_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        return backup_info
    
    def analyze_orphaned_data(self):
        """å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ"""
        print("ğŸ“Š å­¤ç«‹ãƒ‡ãƒ¼ã‚¿åˆ†æ")
        print("-" * 40)
        
        analysis = {
            'total_analyses': 0,
            'null_execution_id': 0,
            'empty_execution_id': 0,
            'invalid_execution_id': 0,
            'valid_execution_id': 0,
            'orphaned_records': [],
            'old_records': [],
            'valid_execution_ids': set(),
            'size_info': {
                'analysis_db_size': 0,
                'potential_savings': 0
            }
        }
        
        # analysis.db ã®ã‚µã‚¤ã‚ºæƒ…å ±
        analysis['size_info']['analysis_db_size'] = self.analysis_db_path.stat().st_size
        
        # æœ‰åŠ¹ãªexecution_idã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        with sqlite3.connect(self.execution_db_path) as exec_conn:
            cursor = exec_conn.execute("SELECT execution_id FROM execution_logs")
            analysis['valid_execution_ids'] = {row[0] for row in cursor.fetchall()}
        
        # analyses ã®è©³ç´°åˆ†æ
        with sqlite3.connect(self.analysis_db_path) as analysis_conn:
            # ç·æ•°
            cursor = analysis_conn.execute("SELECT COUNT(*) FROM analyses")
            analysis['total_analyses'] = cursor.fetchone()[0]
            
            if analysis['total_analyses'] == 0:
                print("ğŸ“ˆ åˆ†æçµæœ: 0ä»¶ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãªã—ï¼‰")
                return analysis
            
            # NULL execution_id
            cursor = analysis_conn.execute("SELECT COUNT(*) FROM analyses WHERE execution_id IS NULL")
            analysis['null_execution_id'] = cursor.fetchone()[0]
            
            # ç©ºæ–‡å­— execution_id
            cursor = analysis_conn.execute("SELECT COUNT(*) FROM analyses WHERE execution_id = ''")
            analysis['empty_execution_id'] = cursor.fetchone()[0]
            
            # ç„¡åŠ¹ execution_id (å­˜åœ¨ã—ãªã„ã‚‚ã®)
            cursor = analysis_conn.execute("""
                SELECT id, symbol, timeframe, config, execution_id, generated_at
                FROM analyses 
                WHERE execution_id IS NOT NULL AND execution_id != ''
            """)
            
            all_records = cursor.fetchall()
            for record in all_records:
                execution_id = record[4]
                if execution_id not in analysis['valid_execution_ids']:
                    analysis['orphaned_records'].append({
                        'id': record[0],
                        'symbol': record[1],
                        'timeframe': record[2],
                        'config': record[3],
                        'execution_id': execution_id,
                        'generated_at': record[5]
                    })
                    analysis['invalid_execution_id'] += 1
                else:
                    analysis['valid_execution_id'] += 1
            
            # å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆ30æ—¥ä»¥ä¸Šå‰ã§execution_idãŒNULLï¼‰ã®æ¤œå‡º
            cutoff_date = (datetime.now() - timedelta(days=30)).isoformat()
            cursor = analysis_conn.execute("""
                SELECT id, symbol, timeframe, config, generated_at
                FROM analyses 
                WHERE execution_id IS NULL 
                AND generated_at < ?
                ORDER BY generated_at
            """, (cutoff_date,))
            
            analysis['old_records'] = [
                {
                    'id': row[0],
                    'symbol': row[1],
                    'timeframe': row[2],
                    'config': row[3],
                    'generated_at': row[4]
                }
                for row in cursor.fetchall()
            ]
        
        # æ½œåœ¨çš„ãªå®¹é‡ç¯€ç´„è¨ˆç®—
        total_orphaned = analysis['null_execution_id'] + analysis['empty_execution_id'] + analysis['invalid_execution_id']
        if analysis['total_analyses'] > 0:
            savings_ratio = total_orphaned / analysis['total_analyses']
            analysis['size_info']['potential_savings'] = int(analysis['size_info']['analysis_db_size'] * savings_ratio)
        
        # çµæœè¡¨ç¤º
        print(f"ğŸ“ˆ ç·åˆ†æçµæœ: {analysis['total_analyses']}ä»¶")
        print(f"   - æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {analysis['valid_execution_id']}ä»¶")
        print(f"   - NULL execution_id: {analysis['null_execution_id']}ä»¶")
        print(f"   - ç©ºæ–‡å­— execution_id: {analysis['empty_execution_id']}ä»¶")
        print(f"   - ç„¡åŠ¹ execution_id: {analysis['invalid_execution_id']}ä»¶")
        print(f"   - å¤ã„å­¤ç«‹ãƒ‡ãƒ¼ã‚¿(30æ—¥ä»¥ä¸Š): {len(analysis['old_records'])}ä»¶")
        
        print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {analysis['size_info']['analysis_db_size']:,} bytes")
        if analysis['size_info']['potential_savings'] > 0:
            print(f"ğŸ’¾ æ½œåœ¨çš„ç¯€ç´„å®¹é‡: {analysis['size_info']['potential_savings']:,} bytes")
        
        if analysis['orphaned_records']:
            print(f"\nâš ï¸ ç„¡åŠ¹execution_idã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
            for record in analysis['orphaned_records'][:5]:
                print(f"   ID:{record['id']} {record['symbol']}-{record['timeframe']}-{record['config']} -> {record['execution_id']}")
            if len(analysis['orphaned_records']) > 5:
                print(f"   ... ä»– {len(analysis['orphaned_records']) - 5}ä»¶")
        
        if analysis['old_records']:
            print(f"\nğŸ“… å¤ã„å­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
            for record in analysis['old_records'][:5]:
                print(f"   ID:{record['id']} {record['symbol']}-{record['timeframe']}-{record['config']} at {record['generated_at']}")
            if len(analysis['old_records']) > 5:
                print(f"   ... ä»– {len(analysis['old_records']) - 5}ä»¶")
        
        return analysis
    
    def create_cleanup_plan(self, analysis_result):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨ˆç”»ã‚’ä½œæˆ"""
        print(f"\nğŸ—‘ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨ˆç”»")
        print("-" * 40)
        
        plan = {
            'null_execution_id': analysis_result['null_execution_id'],
            'empty_execution_id': analysis_result['empty_execution_id'],
            'invalid_execution_id': analysis_result['invalid_execution_id'],
            'old_records': len(analysis_result['old_records']),
            'total_to_delete': 0,
            'deletion_strategy': []
        }
        
        plan['total_to_delete'] = (
            plan['null_execution_id'] + 
            plan['empty_execution_id'] + 
            plan['invalid_execution_id']
        )
        
        if plan['null_execution_id'] > 0:
            plan['deletion_strategy'].append(f"1. NULL execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {plan['null_execution_id']}ä»¶")
        
        if plan['empty_execution_id'] > 0:
            plan['deletion_strategy'].append(f"2. ç©ºæ–‡å­— execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {plan['empty_execution_id']}ä»¶")
        
        if plan['invalid_execution_id'] > 0:
            plan['deletion_strategy'].append(f"3. ç„¡åŠ¹ execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {plan['invalid_execution_id']}ä»¶")
        
        if plan['total_to_delete'] > 0:
            print("å‰Šé™¤äºˆå®š:")
            for strategy in plan['deletion_strategy']:
                print(f"   {strategy}")
            print(f"\nğŸ“Š ç·å‰Šé™¤äºˆå®š: {plan['total_to_delete']}ä»¶")
        else:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        return plan
    
    def execute_cleanup(self, analysis_result, dry_run=True):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        print(f"\nğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ {'(DRY RUN)' if dry_run else ''}")
        print("-" * 40)
        
        cleanup_summary = {
            'deleted_null': 0,
            'deleted_empty': 0,
            'deleted_invalid': 0,
            'total_deleted': 0,
            'errors': []
        }
        
        if analysis_result['total_analyses'] == 0:
            print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return cleanup_summary
        
        try:
            with sqlite3.connect(self.analysis_db_path) as conn:
                if not dry_run:
                    conn.execute("BEGIN TRANSACTION")
                
                # 1. NULL execution_id ã®å‰Šé™¤
                if analysis_result['null_execution_id'] > 0:
                    if dry_run:
                        print(f"ğŸ” [DRY RUN] NULL execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤äºˆå®š: {analysis_result['null_execution_id']}ä»¶")
                    else:
                        cursor = conn.execute("DELETE FROM analyses WHERE execution_id IS NULL")
                        cleanup_summary['deleted_null'] = cursor.rowcount
                        print(f"âœ… NULL execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {cleanup_summary['deleted_null']}ä»¶")
                
                # 2. ç©ºæ–‡å­— execution_id ã®å‰Šé™¤
                if analysis_result['empty_execution_id'] > 0:
                    if dry_run:
                        print(f"ğŸ” [DRY RUN] ç©ºæ–‡å­— execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤äºˆå®š: {analysis_result['empty_execution_id']}ä»¶")
                    else:
                        cursor = conn.execute("DELETE FROM analyses WHERE execution_id = ''")
                        cleanup_summary['deleted_empty'] = cursor.rowcount
                        print(f"âœ… ç©ºæ–‡å­— execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {cleanup_summary['deleted_empty']}ä»¶")
                
                # 3. ç„¡åŠ¹ execution_id ã®å‰Šé™¤
                if analysis_result['invalid_execution_id'] > 0:
                    invalid_ids = [record['execution_id'] for record in analysis_result['orphaned_records']]
                    if dry_run:
                        print(f"ğŸ” [DRY RUN] ç„¡åŠ¹ execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤äºˆå®š: {len(invalid_ids)}ä»¶")
                        for i, invalid_id in enumerate(invalid_ids[:5]):
                            print(f"   {i+1}. {invalid_id}")
                        if len(invalid_ids) > 5:
                            print(f"   ... ä»– {len(invalid_ids) - 5}ä»¶")
                    else:
                        for invalid_id in invalid_ids:
                            cursor = conn.execute("DELETE FROM analyses WHERE execution_id = ?", (invalid_id,))
                            cleanup_summary['deleted_invalid'] += cursor.rowcount
                        print(f"âœ… ç„¡åŠ¹ execution_id ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {cleanup_summary['deleted_invalid']}ä»¶")
                
                if not dry_run:
                    conn.execute("COMMIT")
                    
                    # VACUUM ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºã‚’æœ€é©åŒ–
                    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ä¸­...")
                    conn.execute("VACUUM")
                    print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")
                
                cleanup_summary['total_deleted'] = (
                    cleanup_summary['deleted_null'] + 
                    cleanup_summary['deleted_empty'] + 
                    cleanup_summary['deleted_invalid']
                )
                
        except Exception as e:
            cleanup_summary['errors'].append(str(e))
            print(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            if not dry_run:
                try:
                    conn.execute("ROLLBACK")
                    print("ğŸ”„ ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
                except:
                    pass
        
        return cleanup_summary
    
    def generate_cleanup_report(self, analysis_result, cleanup_summary, backup_info):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print(f"\nğŸ“‹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 50)
        
        # å®Ÿè¡Œå‰ã®çŠ¶æ³
        print("å®Ÿè¡Œå‰ã®çŠ¶æ³:")
        print(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {analysis_result['total_analyses']}")
        print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {analysis_result['size_info']['analysis_db_size']:,} bytes")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ
        print("\nã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ:")
        if cleanup_summary['total_deleted'] > 0:
            print(f"  å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {cleanup_summary['total_deleted']}")
            if cleanup_summary['deleted_null'] > 0:
                print(f"    - NULL execution_id: {cleanup_summary['deleted_null']}ä»¶")
            if cleanup_summary['deleted_empty'] > 0:
                print(f"    - ç©ºæ–‡å­— execution_id: {cleanup_summary['deleted_empty']}ä»¶")
            if cleanup_summary['deleted_invalid'] > 0:
                print(f"    - ç„¡åŠ¹ execution_id: {cleanup_summary['deleted_invalid']}ä»¶")
        else:
            print("  å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: 0")
        
        # å®Ÿè¡Œå¾Œã®çŠ¶æ³
        if cleanup_summary['total_deleted'] > 0:
            remaining_records = analysis_result['total_analyses'] - cleanup_summary['total_deleted']
            print(f"\nå®Ÿè¡Œå¾Œã®çŠ¶æ³:")
            print(f"  æ®‹å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {remaining_records}")
            
            # ã‚µã‚¤ã‚ºå‰Šæ¸›åŠ¹æœ
            if self.analysis_db_path.exists():
                new_size = self.analysis_db_path.stat().st_size
                size_reduction = analysis_result['size_info']['analysis_db_size'] - new_size
                if size_reduction > 0:
                    print(f"  æ–°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {new_size:,} bytes")
                    print(f"  å‰Šæ¸›ã‚µã‚¤ã‚º: {size_reduction:,} bytes ({size_reduction/analysis_result['size_info']['analysis_db_size']*100:.1f}%)")
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        if cleanup_summary['errors']:
            print(f"\nâš ï¸ ã‚¨ãƒ©ãƒ¼:")
            for error in cleanup_summary['errors']:
                print(f"    {error}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
        print(f"\nğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:")
        print(f"  å ´æ‰€: {backup_info['backup_dir']}")
        print(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«: {backup_info['backups']['analysis']}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'analysis_before': analysis_result,
            'cleanup_summary': cleanup_summary,
            'backup_info': backup_info
        }
        
        report_file = Path(backup_info['backup_dir']) / "cleanup_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"  ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='å­¤ç«‹åˆ†æçµæœã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--dry-run', action='store_true',
                       help='å®Ÿéš›ã®å‰Šé™¤ã‚’è¡Œã‚ãšã€å‰Šé™¤äºˆå®šã®ã¿ã‚’è¡¨ç¤º')
    parser.add_argument('--skip-backup', action='store_true',
                       help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--analysis-only', action='store_true',
                       help='åˆ†æã®ã¿å®Ÿè¡Œï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯è¡Œã‚ãªã„ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸ§¹ å­¤ç«‹åˆ†æçµæœã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    if args.dry_run:
        print("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰ - å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã„ã¾ã›ã‚“")
    
    try:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        cleanup = OrphanedDataCleanup()
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        if not args.skip_backup and not args.analysis_only:
            print("\nğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
            backup_info = cleanup.backup_databases()
        else:
            backup_info = {'backup_dir': 'ã‚¹ã‚­ãƒƒãƒ—', 'backups': {}}
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å­¤ç«‹ãƒ‡ãƒ¼ã‚¿åˆ†æ
        print("\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: å­¤ç«‹ãƒ‡ãƒ¼ã‚¿åˆ†æ")
        analysis_result = cleanup.analyze_orphaned_data()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨ˆç”»
        print("\nğŸ—‘ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨ˆç”»ä½œæˆ")
        cleanup_plan = cleanup.create_cleanup_plan(analysis_result)
        
        if args.analysis_only:
            print("\nâœ… åˆ†æã®ã¿å®Œäº†ã—ã¾ã—ãŸ")
            return
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        print("\nğŸ§¹ ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
        cleanup_summary = cleanup.execute_cleanup(analysis_result, dry_run=args.dry_run)
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if not args.dry_run:
            print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
            cleanup.generate_cleanup_report(analysis_result, cleanup_summary, backup_info)
        
        print("\n" + "=" * 60)
        if args.dry_run:
            print("ğŸ” DRY RUN å®Œäº† - --dry-run ãƒ•ãƒ©ã‚°ã‚’å¤–ã—ã¦å®Ÿéš›ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        elif cleanup_summary['total_deleted'] > 0:
            print("âœ… å­¤ç«‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†ï¼")
            print(f"ğŸ“Š {cleanup_summary['total_deleted']}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            if not args.skip_backup:
                print(f"ğŸ“ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€: {backup_info['backup_dir']}")
        else:
            print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()