"""
å¤§è¦æ¨¡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
æ•°åƒãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªåˆ†æãƒ»å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""
import pandas as pd
import numpy as np
import os
import json
import sqlite3
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import cpu_count
import shutil
import gzip
import pickle
from datetime import datetime, timedelta, timezone
import logging
from pathlib import Path

# ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from engines.price_consistency_validator import PriceConsistencyValidator, UnifiedPriceData

# é€²æ—ãƒ­ã‚¬ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from progress_logger import SymbolProgressLogger

# ã‚¨ãƒ©ãƒ¼ä¾‹å¤–ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from engines.leverage_decision_engine import InsufficientConfigurationError

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScalableAnalysisSystem:
    def __init__(self, base_dir="large_scale_analysis"):
        import os
        import inspect
        
        # ğŸ”§ å¼·åˆ¶çš„ã«rootãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®DBã‚’ä½¿ç”¨ï¼ˆå®Œå…¨DBçµ±ä¸€å¼·åŒ–ç‰ˆï¼‰
        script_dir = Path(__file__).parent.absolute()  # scalable_analysis_system.pyã®çµ¶å¯¾ãƒ‘ã‚¹
        
        # å¸¸ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®æ­£è¦DBã‚’ä½¿ç”¨ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ç¦æ­¢ï¼‰
        if os.path.isabs(base_dir):
            # çµ¶å¯¾ãƒ‘ã‚¹æŒ‡å®šã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            self.base_dir = Path(base_dir)
        else:
            # ç›¸å¯¾ãƒ‘ã‚¹æŒ‡å®šã¯å¿…ãšã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåŸºæº–ã«çµ±ä¸€
            self.base_dir = script_dir / base_dir
        
        # web_dashboardãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®DBä½œæˆã‚’å®Œå…¨ã«é˜²æ­¢
        if 'web_dashboard' in str(self.base_dir):
            logger.warning(f"âš ï¸ web_dashboardå†…DBä½œæˆã‚’é˜»æ­¢: {self.base_dir}")
            self.base_dir = script_dir / base_dir
            
        self.base_dir.mkdir(exist_ok=True)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        self.db_path = self.base_dir / "analysis.db"
        self.charts_dir = self.base_dir / "charts"
        self.data_dir = self.base_dir / "data"
        self.compressed_dir = self.base_dir / "compressed"
        
        # DBä½¿ç”¨ãƒ­ã‚°æ©Ÿèƒ½è¿½åŠ 
        caller_frame = inspect.currentframe().f_back
        caller_file = caller_frame.f_code.co_filename if caller_frame else "unknown"
        caller_function = caller_frame.f_code.co_name if caller_frame else "unknown"
        
        logger.info(f"ğŸ” ScalableAnalysisSystemåˆæœŸåŒ–:")
        logger.info(f"  ğŸ“ base_dir: {self.base_dir.absolute()}")
        logger.info(f"  ğŸ—ƒï¸ DB path: {self.db_path.absolute()}")
        logger.info(f"  ğŸ“ å‘¼ã³å‡ºã—å…ƒ: {os.path.basename(caller_file)}:{caller_function}")
        logger.info(f"  ğŸ• ç¾åœ¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        
        for dir_path in [self.charts_dir, self.data_dir, self.compressed_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self.price_validator = PriceConsistencyValidator(
            warning_threshold_pct=1.0,
            error_threshold_pct=5.0,
            critical_threshold_pct=10.0
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.init_database()
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.current_execution_id = None
    
    def init_database(self):
        """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        logger.info(f"ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–: {self.db_path.absolute()}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            existing_tables = [row[0] for row in cursor.fetchall()]
            logger.info(f"  ğŸ“‹ æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: {existing_tables}")
            
            # analysesãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã‚«ãƒ©ãƒ æ§‹é€ ã‚’ç¢ºèª
            if 'analyses' in existing_tables:
                cursor.execute("PRAGMA table_info(analyses);")
                columns = [row[1] for row in cursor.fetchall()]
                logger.info(f"  ğŸ“Š analysesãƒ†ãƒ¼ãƒ–ãƒ«ã‚«ãƒ©ãƒ : {columns}")
                
                # execution_idã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
                if 'execution_id' in columns:
                    logger.info("  âœ… execution_idã‚«ãƒ©ãƒ : å­˜åœ¨")
                else:
                    logger.warning("  âš ï¸ execution_idã‚«ãƒ©ãƒ : ä¸åœ¨")
                    # execution_idã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
                    cursor.execute('ALTER TABLE analyses ADD COLUMN execution_id TEXT')
                    logger.info("  âœ… execution_idã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                
                if 'task_status' in columns:
                    logger.info("  âœ… task_statusã‚«ãƒ©ãƒ : å­˜åœ¨")
                else:
                    logger.warning("  âš ï¸ task_statusã‚«ãƒ©ãƒ : ä¸åœ¨")
                    # ä¸è¶³ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_status TEXT DEFAULT "pending"')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_started_at TIMESTAMP')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_completed_at TIMESTAMP')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN error_message TEXT')
                    logger.info("  âœ… task_statusã‚«ãƒ©ãƒ ç­‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            
            # åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analyses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    config TEXT NOT NULL,
                    generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    total_trades INTEGER,
                    win_rate REAL,
                    total_return REAL,
                    sharpe_ratio REAL,
                    max_drawdown REAL,
                    avg_leverage REAL,
                    chart_path TEXT,
                    compressed_path TEXT,
                    status TEXT DEFAULT 'pending',
                    execution_id TEXT,
                    task_status TEXT DEFAULT 'pending',
                    task_started_at TIMESTAMP,
                    task_completed_at TIMESTAMP,
                    error_message TEXT
                )
            ''')
            
            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS backtest_summary (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    metric_name TEXT,
                    metric_value REAL,
                    FOREIGN KEY (analysis_id) REFERENCES analyses (id)
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol_timeframe ON analyses (symbol, timeframe)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_config ON analyses (config)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_sharpe ON analyses (sharpe_ratio)')
            
            conn.commit()
    
    def _should_cancel_execution(self, execution_id: str = None) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª"""
        if not execution_id and hasattr(self, 'current_execution_id'):
            execution_id = self.current_execution_id
        
        if not execution_id:
            return False
            
        try:
            from execution_log_database import ExecutionLogDatabase
            db = ExecutionLogDatabase()
            with sqlite3.connect(db.db_path) as conn:
                cursor = conn.execute(
                    'SELECT status FROM execution_logs WHERE execution_id = ?',
                    (execution_id,)
                )
                row = cursor.fetchone()
                return row and row[0] == 'CANCELLED'
        except Exception as e:
            logger.warning(f"Failed to check cancellation status: {e}")
            return False
    
    def generate_batch_analysis(self, batch_configs, max_workers=None, symbol=None, execution_id=None, skip_pretask_creation=False):
        """
        ãƒãƒƒãƒã§å¤§é‡ã®åˆ†æã‚’ä¸¦åˆ—ç”Ÿæˆ
        
        Args:
            batch_configs: [{'symbol': 'BTC', 'timeframe': '1h', 'config': 'ML'}, ...]
            max_workers: ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CPUæ•°ï¼‰
            symbol: éŠ˜æŸ„åï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
            execution_id: å®Ÿè¡ŒIDï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
            skip_pretask_creation: Pre-taskä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹
        """
        if max_workers is None:
            max_workers = min(cpu_count(), 4)  # Rate Limitå¯¾ç­–ã§æœ€å¤§4ä¸¦åˆ—
        
        # å®Ÿè¡ŒIDã‚’è¨­å®š
        self.current_execution_id = execution_id
        
        # ğŸ”¥ é‡è¦: Pre-taskä½œæˆï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¿½è·¡ã®ãŸã‚ï¼‰
        if execution_id and not skip_pretask_creation:
            self._create_pre_tasks(batch_configs, execution_id)
        
        # é€²æ—ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
        progress_logger = None
        if symbol and execution_id:
            progress_logger = SymbolProgressLogger(symbol, execution_id, len(batch_configs))
            progress_logger.log_phase_start("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ", f"{len(batch_configs)}ãƒ‘ã‚¿ãƒ¼ãƒ³, {max_workers}ä¸¦åˆ—")
        else:
            logger.info(f"ãƒãƒƒãƒåˆ†æé–‹å§‹: {len(batch_configs)}ãƒ‘ã‚¿ãƒ¼ãƒ³, {max_workers}ä¸¦åˆ—")
        
        self.max_workers = max_workers  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿å­˜
        # self.progress_logger = progress_logger  # ğŸ› PickleåŒ–ã‚¨ãƒ©ãƒ¼ä¿®æ­£: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¸ã®ä¿å­˜ã‚’ç„¡åŠ¹åŒ–
        
        # ãƒãƒƒãƒã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunk_size = max(1, len(batch_configs) // max_workers)
        chunks = [batch_configs[i:i + chunk_size] for i in range(0, len(batch_configs), chunk_size)]
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i, chunk in enumerate(chunks):
                # execution_idã‚’æ˜ç¤ºçš„ã«æ¸¡ã™
                future = executor.submit(self._process_chunk, chunk, i, self.current_execution_id)
                futures.append(future)
            
            # çµæœåé›†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            total_processed = 0
            for i, future in enumerate(futures):
                try:
                    # å„ãƒãƒ£ãƒ³ã‚¯ã«30åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                    processed_count = future.result(timeout=1800)  # 30 minutes
                    total_processed += processed_count
                    
                    if progress_logger:
                        # é€²æ—ãƒ­ã‚°ã¯å€‹åˆ¥æˆ¦ç•¥å®Œäº†æ™‚ã«å‡ºåŠ›ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç°¡æ½”ã«
                        pass
                    else:
                        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(futures)} å®Œäº†: {processed_count}ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†")
                except Exception as e:
                    logger.error(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    if progress_logger:
                        progress_logger.log_error(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", "ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ç ´æã•ã›ãªã„
                    if "BrokenProcessPool" in str(e):
                        logger.error("ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ç ´ææ¤œå‡º - æ®‹ã‚Šã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        break
        
        if progress_logger:
            progress_logger.log_phase_complete("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
            # æˆåŠŸåˆ¤å®š: åˆ†æãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆï¼ˆã‚·ã‚°ãƒŠãƒ«ãªã—ã§ã‚‚æˆåŠŸï¼‰
            analysis_attempted = len(batch_configs) > 0
            progress_logger.log_final_summary(analysis_attempted)
        else:
            logger.info(f"ãƒãƒƒãƒåˆ†æå®Œäº†: {total_processed}ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†å®Œäº†")
        
        return total_processed
    
    def _create_pre_tasks(self, batch_configs, execution_id):
        """Pre-taskä½œæˆï¼ˆåˆ†æå®Ÿè¡Œå‰ã«pendingãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆï¼‰"""
        logger.info(f"ğŸ¯ Pre-taskä½œæˆé–‹å§‹: {len(batch_configs)}ã‚¿ã‚¹ã‚¯, execution_id={execution_id}")
        logger.info(f"  ğŸ—ƒï¸ ä½œæˆå…ˆDB: {self.db_path.absolute()}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            created_count = 0
            for config in batch_configs:
                symbol = config['symbol']
                timeframe = config['timeframe']
                
                # configè¾æ›¸ã‹ã‚‰é©åˆ‡ãªã‚­ãƒ¼ã‚’å–å¾—ï¼ˆ_process_chunkã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                if 'strategy' in config:
                    config_name = config['strategy']
                elif 'config' in config:
                    config_name = config['config']
                else:
                    config_name = 'Default'
                
                try:
                    # æ—¢å­˜ã®pendingã‚¿ã‚¹ã‚¯ç¢ºèªï¼ˆå®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯é‡è¤‡ä½œæˆã‚’é˜²ãï¼‰
                    cursor.execute('''
                        SELECT COUNT(*) FROM analyses 
                        WHERE symbol=? AND timeframe=? AND config=? AND execution_id=? 
                        AND task_status IN ('pending', 'running')
                    ''', (symbol, timeframe, config_name, execution_id))
                    
                    if cursor.fetchone()[0] == 0:
                        # Pendingã‚¿ã‚¹ã‚¯ä½œæˆ
                        cursor.execute('''
                            INSERT INTO analyses 
                            (symbol, timeframe, config, task_status, execution_id, status)
                            VALUES (?, ?, ?, 'pending', ?, 'running')
                        ''', (symbol, timeframe, config_name, execution_id))
                        created_count += 1
                
                except Exception as e:
                    logger.error(f"Pre-taskä½œæˆã‚¨ãƒ©ãƒ¼ {symbol} {timeframe} {config_name}: {e}")
            
            conn.commit()
            logger.info(f"âœ… Pre-taskä½œæˆå®Œäº†: {created_count}ã‚¿ã‚¹ã‚¯ä½œæˆ")
            
        return created_count
    
    def _process_chunk(self, configs_chunk, chunk_id, execution_id=None):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ã§å®Ÿè¡Œï¼‰"""
        import time
        import random
        import os
        
        # execution_idã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ç”¨ï¼‰
        if execution_id:
            os.environ['CURRENT_EXECUTION_ID'] = execution_id
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: execution_id {execution_id} ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š")
        
        # ãƒ—ãƒ­ã‚»ã‚¹é–“ã®ç«¶åˆã‚’é˜²ããŸã‚ã€ã‚ãšã‹ãªé…å»¶ã‚’è¿½åŠ 
        # TODO: ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã¯å“è³ªå•é¡Œã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ (2024-06-18)
        # time.sleep(random.uniform(0.1, 0.5))
        # ãƒãƒ£ãƒ³ã‚¯IDãƒ™ãƒ¼ã‚¹ã®æ±ºå®šçš„é…å»¶ã«å¤‰æ›´
        time.sleep(0.1 + (chunk_id % 5) * 0.1)  # 0.1-0.5ç§’ã®æ±ºå®šçš„é…å»¶
        
        processed = 0
        for config in configs_chunk:
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ç¢ºèªï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®å„è¨­å®šã§ç¢ºèªï¼‰
            # ProcessPoolExecutorå†…ã§ã¯ execution_id ã‚’å¼•æ•°ã‹ã‚‰å–å¾—
            check_execution_id = execution_id or getattr(self, 'current_execution_id', None)
            if check_execution_id:
                if self._should_cancel_execution(check_execution_id):
                    logger.info(f"Cancellation detected for {check_execution_id}, stopping chunk {chunk_id}")
                    break
            
            try:
                # è¨­å®šã®å‹ãƒã‚§ãƒƒã‚¯
                if not isinstance(config, dict):
                    logger.error(f"Config is not a dict: {type(config)} - {config}")
                    continue
                
                # configè¾æ›¸ã‹ã‚‰é©åˆ‡ãªã‚­ãƒ¼ã‚’å–å¾—
                if 'strategy' in config:
                    strategy = config['strategy']
                elif 'config' in config:
                    strategy = config['config']
                else:
                    strategy = 'Default'
                
                # æˆ¦ç•¥ã‚­ãƒ¼æ¤œè¨¼å¼·åŒ–
                if not strategy or strategy == 'Default':
                    logger.warning(f"Invalid or missing strategy in config: {config}")
                    continue
                
                # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                if 'symbol' not in config or 'timeframe' not in config:
                    logger.error(f"Missing required keys in config: {config}")
                    continue
                
                result, metrics = self._generate_single_analysis(
                    config['symbol'], 
                    config['timeframe'], 
                    strategy
                )
                if result:
                    processed += 1
                    
                    # é€²æ—ãƒ­ã‚¬ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€æˆ¦ç•¥å®Œäº†ã‚’ãƒ­ã‚°
                    # ğŸ› PickleåŒ–ã‚¨ãƒ©ãƒ¼ä¿®æ­£: ProcessPoolExecutorç’°å¢ƒã§ã¯é€²æ—ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
                    # if hasattr(self, 'progress_logger') and self.progress_logger:
                    #     try:
                    #         self.progress_logger.log_strategy_complete(
                    #             config['timeframe'], 
                    #             strategy,
                    #             metrics or {}
                    #         )
                    #     except Exception as log_error:
                    #         logger.warning(f"Progress logging error: {log_error}")
                    
                    if processed % 10 == 0:
                        logger.info(f"Chunk {chunk_id}: {processed}/{len(configs_chunk)} å®Œäº†")
            except Exception as e:
                logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼ {config}: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        return processed
    
    def _update_task_status(self, symbol, timeframe, config, status, error_message=None):
        """task_statusã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°"""
        execution_id = os.environ.get('CURRENT_EXECUTION_ID')
        logger.info(f"ğŸ”„ task_statusæ›´æ–°: {symbol} {timeframe} {config} â†’ {status}")
        logger.info(f"  ğŸ—ƒï¸ æ›´æ–°å…ˆDB: {self.db_path.absolute()}")
        logger.info(f"  ğŸ”‘ execution_id: {execution_id}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            try:
                if status == 'running':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, task_started_at = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, datetime.now(timezone.utc).isoformat(), symbol, timeframe, config, execution_id))
                elif status == 'failed':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, error_message = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, error_message, symbol, timeframe, config, execution_id))
                elif status == 'completed':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, task_completed_at = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, datetime.now(timezone.utc).isoformat(), symbol, timeframe, config, execution_id))
                
                updated_rows = cursor.rowcount
                conn.commit()
                logger.info(f"âœ… task_statusæ›´æ–°æˆåŠŸ: {updated_rows}è¡Œæ›´æ–°")
                
            except Exception as e:
                logger.error(f"âŒ task_statusæ›´æ–°ã‚¨ãƒ©ãƒ¼: {symbol} {timeframe} {config}")
                logger.error(f"  ğŸ—ƒï¸ DB path: {self.db_path.absolute()}")
                logger.error(f"  ğŸ“ ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
                raise
    
    def _generate_single_analysis(self, symbol, timeframe, config):
        """å˜ä¸€ã®åˆ†æã‚’ç”Ÿæˆï¼ˆãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆä½¿ç”¨ç‰ˆ + task_statusæ›´æ–°ï¼‰"""
        analysis_id = f"{symbol}_{timeframe}_{config}"
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
        if self._analysis_exists(analysis_id):
            return False, None
        
        # task_statusã‚’'running'ã«æ›´æ–°
        try:
            self._update_task_status(symbol, timeframe, config, 'running')
        except Exception as e:
            logger.warning(f"Failed to update task_status to running: {e}")
        
        # ãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸåˆ†æã‚’è©¦è¡Œ
        try:
            trades_data = self._generate_real_analysis(symbol, timeframe, config)
        except Exception as e:
            logger.error(f"Real analysis failed for {symbol} {timeframe} {config}: {e}")
            logger.error(f"Analysis terminated - no fallback to sample data")
            
            # task_statusã‚’'failed'ã«æ›´æ–°
            try:
                self._update_task_status(symbol, timeframe, config, 'failed', str(e))
            except Exception as update_error:
                logger.warning(f"Failed to update task_status to failed: {update_error}")
            
            return False, None
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self._calculate_metrics(trades_data)
        
        # ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ä¿å­˜
        compressed_path = self._save_compressed_data(analysis_id, trades_data)
        
        # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
        chart_path = None
        if self._should_generate_chart(metrics):
            chart_path = self._generate_lightweight_chart(analysis_id, trades_data, metrics)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆexecution_idä»˜ã + task_statusæ›´æ–°å«ã‚€ï¼‰
        execution_id = os.environ.get('CURRENT_EXECUTION_ID')
        self._save_to_database(symbol, timeframe, config, metrics, chart_path, compressed_path, execution_id)
        
        return True, metrics
    
    def _get_exchange_from_config(self, config) -> str:
        """è¨­å®šã‹ã‚‰å–å¼•æ‰€ã‚’å–å¾—"""
        import json
        import os
        
        # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        try:
            if os.path.exists('exchange_config.json'):
                with open('exchange_config.json', 'r') as f:
                    exchange_config = json.load(f)
                    return exchange_config.get('default_exchange', 'hyperliquid').lower()
        except Exception as e:
            logger.warning(f"Failed to load exchange config: {e}")
        
        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿
        env_exchange = os.getenv('EXCHANGE_TYPE', '').lower()
        if env_exchange in ['hyperliquid', 'gateio']:
            return env_exchange
        
        # 3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Hyperliquid
        return 'hyperliquid'
    
    def _load_timeframe_config(self, timeframe):
        """æ™‚é–“è¶³è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            config_path = 'config/timeframe_conditions.json'
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    return config_data.get('timeframe_configs', {}).get(timeframe, {})
        except Exception as e:
            logger.warning(f"æ™‚é–“è¶³è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    def _generate_real_analysis(self, symbol, timeframe, config, evaluation_period_days=90):
        """æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æ - å¸‚å ´æ¡ä»¶ã‚’æº€ãŸã—ãŸå ´åˆã®ã¿ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ"""
        try:
            # æœ¬æ ¼çš„ãªæˆ¦ç•¥åˆ†æã®ãŸã‚ã€å®Ÿéš›ã®APIãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            from engines.high_leverage_bot_orchestrator import HighLeverageBotOrchestrator
            
            # å–å¼•æ‰€è¨­å®šã‚’å–å¾—
            exchange = self._get_exchange_from_config(config)
            
            print(f"ğŸ¯ å®Ÿãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æˆ¦ç•¥åˆ†æã‚’é–‹å§‹: {symbol} {timeframe} {config} ({exchange})")
            print("   â³ ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨MLåˆ†æã®ãŸã‚ã€å‡¦ç†ã«æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
            
            # ä¿®æ­£: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤å•é¡Œè§£æ±ºã®ãŸã‚ã€æ¯å›æ–°ã—ã„ãƒœãƒƒãƒˆã‚’ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼‰
            # ç†ç”±: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å†åˆ©ç”¨ã«ã‚ˆã‚Šã€å…¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã§åŒã˜ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãŒä½¿ç”¨ã•ã‚Œã‚‹å•é¡Œã‚’è§£æ±º
            bot = HighLeverageBotOrchestrator(use_default_plugins=True, exchange=exchange)
            print(f"ğŸ”„ {symbol} æ–°è¦ãƒœãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (ä¾¡æ ¼å¤šæ§˜æ€§ç¢ºä¿ã®ãŸã‚)")
            
            # è¤‡æ•°å›åˆ†æã‚’å®Ÿè¡Œã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Œå…¨ãƒ­ã‚°æŠ‘åˆ¶ï¼‰
            trades = []
            import sys
            import os
            import contextlib
            import time
            
            # é€²æ—è¡¨ç¤ºç”¨
            print(f"ğŸ”„ {symbol} {timeframe} {config}: æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æé–‹å§‹")
            
            # å®Œå…¨ã«ãƒ­ã‚°ã‚’æŠ‘åˆ¶ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            @contextlib.contextmanager
            def suppress_all_output():
                with open(os.devnull, 'w') as devnull:
                    old_stdout = sys.stdout
                    old_stderr = sys.stderr
                    try:
                        sys.stdout = devnull
                        sys.stderr = devnull
                        yield
                    finally:
                        sys.stdout = old_stdout
                        sys.stderr = old_stderr
            
            # æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®ã‚·ã‚°ãƒŠãƒ«ç”ŸæˆæœŸé–“è¨­å®š
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(days=evaluation_period_days)
            
            # æ™‚é–“è¶³è¨­å®šã‹ã‚‰è©•ä¾¡é–“éš”ã‚’å–å¾—
            tf_config = self._load_timeframe_config(timeframe)
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©•ä¾¡é–“éš”ã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆ†å˜ä½ï¼‰
            evaluation_interval_minutes = tf_config.get('evaluation_interval_minutes')
            
            if evaluation_interval_minutes:
                evaluation_interval = timedelta(minutes=evaluation_interval_minutes)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–“éš”
                default_intervals = {
                    '1m': timedelta(minutes=5),   '3m': timedelta(minutes=15),
                    '5m': timedelta(minutes=30),  '15m': timedelta(hours=1),
                    '30m': timedelta(hours=2),    '1h': timedelta(hours=4),
                    '4h': timedelta(hours=12),    '1d': timedelta(days=1)
                }
                evaluation_interval = default_intervals.get(timeframe, timedelta(hours=4))
            
            # æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®åˆ†æå®Ÿè¡Œ
            current_time = start_time
            total_evaluations = 0
            signals_generated = 0
            
            # æ™‚é–“è¶³è¨­å®šã‹ã‚‰æœ€å¤§è©•ä¾¡å›æ•°ã‚’å–å¾—
            max_evaluations = tf_config.get('max_evaluations', 100)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
            
            print(f"ğŸ” æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æ: {start_time.strftime('%Y-%m-%d')} ã‹ã‚‰ {end_time.strftime('%Y-%m-%d')}")
            print(f"ğŸ“Š è©•ä¾¡é–“éš”: {evaluation_interval} ({timeframe}è¶³æœ€é©åŒ–)")
            print(f"ğŸ›¡ï¸ æœ€å¤§è©•ä¾¡å›æ•°: {max_evaluations}å›")
            
            max_signals = max_evaluations // 2  # è©•ä¾¡å›æ•°ã®åŠåˆ†ã¾ã§ï¼ˆä¾‹ï¼š20è©•ä¾¡ã§æœ€å¤§10ã‚·ã‚°ãƒŠãƒ«ï¼‰
            
            while (current_time <= end_time and 
                   total_evaluations < max_evaluations and 
                   signals_generated < max_signals):
                total_evaluations += 1
                try:
                    # å‡ºåŠ›æŠ‘åˆ¶ã§å¸‚å ´æ¡ä»¶ã®è©•ä¾¡ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ•ãƒ©ã‚°ä»˜ãï¼‰
                    with suppress_all_output():
                        result = bot.analyze_symbol(symbol, timeframe, config, is_backtest=True, target_timestamp=current_time)
                    
                    if not result or 'current_price' not in result:
                        current_time += evaluation_interval
                        continue
                    
                    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã®è©•ä¾¡
                    should_enter = self._evaluate_entry_conditions(result, timeframe)
                    
                    if not should_enter:
                        # æ¡ä»¶ã‚’æº€ãŸã•ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
                        if symbol == 'OP' and total_evaluations <= 5:  # æœ€åˆã®5å›ã®ã¿ãƒ­ã‚°
                            logger.error(f"ğŸš¨ OPæ¡ä»¶ä¸æº€è¶³ #{total_evaluations}: leverage={result.get('leverage')}, confidence={result.get('confidence')}, RR={result.get('risk_reward_ratio')}")
                        current_time += evaluation_interval
                        continue
                    
                    signals_generated += 1
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆæ¡ä»¶æº€è¶³æ™‚ï¼‰
                    if signals_generated % 5 == 0:
                        progress_pct = ((current_time - start_time).total_seconds() / 
                                      (end_time - start_time).total_seconds()) * 100
                        print(f"ğŸ¯ {symbol} {timeframe}: ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ {signals_generated}ä»¶ (é€²æ—: {progress_pct:.1f}%)")
                    
                    # ãƒ¬ãƒãƒ¬ãƒƒã‚¸ã¨TP/SLä¾¡æ ¼ã‚’è¨ˆç®—
                    leverage = result.get('leverage', 5.0)
                    confidence = result.get('confidence', 70.0) / 100.0
                    risk_reward = result.get('risk_reward_ratio', 2.0)
                    current_price = result.get('current_price')
                    if current_price is None:
                        logger.error(f"No current_price in analysis result for {symbol}")
                        raise Exception(f"Missing current_price in analysis result for {symbol}")
                    
                    # TP/SLè¨ˆç®—æ©Ÿèƒ½ã‚’ä½¿ç”¨
                    from engines.stop_loss_take_profit_calculators import (
                        DefaultSLTPCalculator, ConservativeSLTPCalculator, AggressiveSLTPCalculator,
                        TraditionalSLTPCalculator, MLSLTPCalculator
                    )
                    from interfaces.data_types import MarketContext
                    
                    # æ¡ä»¶æº€è¶³æ™‚ã®å®Ÿéš›ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                    trade_time = current_time
                    
                    # æˆ¦ç•¥ã«å¿œã˜ãŸTP/SLè¨ˆç®—å™¨ã‚’é¸æŠ
                    if 'Conservative' in config:
                        sltp_calculator = ConservativeSLTPCalculator()
                    elif 'Aggressive_Traditional' in config:
                        sltp_calculator = TraditionalSLTPCalculator()
                    elif 'Aggressive' in config:
                        sltp_calculator = AggressiveSLTPCalculator()
                    elif 'Full_ML' in config:
                        sltp_calculator = MLSLTPCalculator()
                    else:
                        sltp_calculator = DefaultSLTPCalculator()
                    
                    # æ¨¡æ“¬çš„ãªå¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    market_context = MarketContext(
                        current_price=current_price,
                        volume_24h=1000000.0,
                        volatility=0.03,
                        trend_direction='BULLISH',
                        market_phase='MARKUP',
                        timestamp=trade_time
                    )
                    
                    # å®Ÿéš›ã®æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºï¼ˆæŸ”è»Ÿãªã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ç‰ˆï¼‰
                    try:
                        from engines.support_resistance_adapter import FlexibleSupportResistanceDetector
                        
                        # OHLCVãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸçš„ã«å–å¾—ï¼ˆãƒœãƒƒãƒˆå†…ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                        try:
                            # ãƒœãƒƒãƒˆãŒæ—¢ã«å–å¾—ã—ãŸOHLCVãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                                ohlcv_data = bot._cached_data
                            else:
                                # ãƒœãƒƒãƒˆã®fetch_market_dataãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                                ohlcv_data = bot._fetch_market_data(symbol, timeframe)
                            
                            if ohlcv_data.empty:
                                raise Exception("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                                
                        except Exception as ohlcv_error:
                            # OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            raise Exception(f"OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(ohlcv_error)}")
                        
                        if len(ohlcv_data) < 50:
                            raise Exception(f"æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚{len(ohlcv_data)}æœ¬ï¼ˆæœ€ä½50æœ¬å¿…è¦ï¼‰")
                        
                        # æŸ”è»Ÿãªæ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
                        detector = FlexibleSupportResistanceDetector(
                            min_touches=2, 
                            tolerance_pct=0.01,
                            use_ml_enhancement=True
                        )
                        
                        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¡¨ç¤º
                        provider_info = detector.get_provider_info()
                        print(f"       æ¤œå‡ºãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider_info['base_provider']}")
                        print(f"       MLå¼·åŒ–: {provider_info['ml_provider']}")
                        
                        # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šã‚’æ¤œå‡º
                        support_levels, resistance_levels = detector.detect_levels(ohlcv_data, current_price)
                        
                        # ä¸Šä½ãƒ¬ãƒ™ãƒ«ã®ã¿é¸æŠï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
                        max_levels = 3
                        support_levels = support_levels[:max_levels]
                        resistance_levels = resistance_levels[:max_levels]
                        
                        if not support_levels and not resistance_levels:
                            raise Exception(f"æœ‰åŠ¹ãªæ”¯æŒç·šãƒ»æŠµæŠ—ç·šãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        
                        print(f"   âœ… æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºæˆåŠŸ: æ”¯æŒç·š{len(support_levels)}å€‹, æŠµæŠ—ç·š{len(resistance_levels)}å€‹")
                        
                        # MLäºˆæ¸¬ã‚¹ã‚³ã‚¢æƒ…å ±ã‚‚è¡¨ç¤º
                        if provider_info['ml_provider'] != "Disabled":
                            if support_levels:
                                avg_ml_score = np.mean([getattr(s, 'ml_bounce_probability', 0) for s in support_levels])
                                print(f"       æ”¯æŒç·šMLåç™ºäºˆæ¸¬: å¹³å‡{avg_ml_score:.2f}")
                            if resistance_levels:
                                avg_ml_score = np.mean([getattr(r, 'ml_bounce_probability', 0) for r in resistance_levels])
                                print(f"       æŠµæŠ—ç·šMLåç™ºäºˆæ¸¬: å¹³å‡{avg_ml_score:.2f}")
                        else:
                            print(f"       MLäºˆæ¸¬: ç„¡åŠ¹åŒ–")
                        
                        # TP/SLä¾¡æ ¼ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
                        sltp_levels = sltp_calculator.calculate_levels(
                            current_price=current_price,
                            leverage=leverage,
                            support_levels=support_levels,
                            resistance_levels=resistance_levels,
                            market_context=market_context
                        )
                        
                    except Exception as e:
                        # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šåˆ†æã‚’åœæ­¢
                        error_msg = f"æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºãƒ»åˆ†æã«å¤±æ•—: {str(e)}"
                        print(f"âŒ {symbol} {timeframe} {config}: {error_msg}")
                        logger.error(f"Support/resistance analysis error for {symbol}: {error_msg}")
                        raise Exception(f"æˆ¦ç•¥åˆ†æå¤±æ•— - {error_msg}")
                    
                    # ğŸ”§ é‡è¦ãªä¿®æ­£: å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ãƒˆãƒ¬ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ã‚’å–å¾—
                    # ç†ç”±: current_priceãŒå›ºå®šå€¤ã®ãŸã‚ã€å®Ÿéš›ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    entry_price = self._get_real_market_price(bot, symbol, timeframe, trade_time)
                    
                    # SL/TPä¾¡æ ¼ã‚’ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã§å†è¨ˆç®—
                    sltp_levels = sltp_calculator.calculate_levels(
                        current_price=entry_price,  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—
                        leverage=leverage,
                        support_levels=support_levels,
                        resistance_levels=resistance_levels,
                        market_context=market_context
                    )
                    
                    # å®Ÿéš›ã®TP/SLä¾¡æ ¼ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ï¼‰
                    tp_price = sltp_levels.take_profit_price
                    sl_price = sltp_levels.stop_loss_price
                    
                    # ğŸ”§ ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼è«–ç†ãƒã‚§ãƒƒã‚¯
                    if sl_price >= entry_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: æåˆ‡ã‚Šä¾¡æ ¼({sl_price:.4f})ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼({entry_price:.4f})ä»¥ä¸Š")
                        continue
                    if tp_price <= entry_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: åˆ©ç¢ºä¾¡æ ¼({tp_price:.4f})ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼({entry_price:.4f})ä»¥ä¸‹")
                        continue
                    if sl_price >= tp_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: æåˆ‡ã‚Šä¾¡æ ¼({sl_price:.4f})ãŒåˆ©ç¢ºä¾¡æ ¼({tp_price:.4f})ä»¥ä¸Š")
                        continue
                    
                    # æˆ¦ç•¥åˆ†æã§ã¯ã€current_priceã‚‚entry_priceã¨åŒã˜ã«ã—ã¦æ•´åˆæ€§ã‚’ä¿ã¤
                    # ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®open vs closeã«ã‚ˆã‚‹ä¾¡æ ¼å·®ã‚’é˜²ã
                    current_price = entry_price
                    
                    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
                    price_consistency_result = self.price_validator.validate_price_consistency(
                        analysis_price=current_price,
                        entry_price=entry_price,
                        symbol=symbol,
                        context=f"{timeframe}_{config}_trade_{len(trades)+1}"
                    )
                    
                    if not price_consistency_result.is_consistent:
                        logger.warning(f"ä¾¡æ ¼æ•´åˆæ€§å•é¡Œæ¤œå‡º: {symbol} {timeframe} - {price_consistency_result.message}")
                        for recommendation in price_consistency_result.recommendations:
                            logger.warning(f"æ¨å¥¨å¯¾å¿œ: {recommendation}")
                        
                        # é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆã®å ´åˆã¯å–å¼•ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if price_consistency_result.inconsistency_level.value == 'critical':
                            logger.error(f"é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆã®ãŸã‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: {symbol} at {trade_time}")
                            continue
                    
                    # çµ±ä¸€ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                    unified_price_data = self.price_validator.create_unified_price_data(
                        analysis_price=current_price,
                        entry_price=entry_price,
                        symbol=symbol,
                        timeframe=timeframe,
                        market_timestamp=trade_time,
                        data_source=exchange
                    )
                    
                    # TP/SLåˆ°é”ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ã‚ºåˆ¤å®šï¼ˆå®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                    exit_time, exit_price, is_success = self._find_tp_sl_exit(
                        bot, symbol, timeframe, trade_time, entry_price, tp_price, sl_price
                    )
                    
                    # åˆ°é”åˆ¤å®šãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if exit_time is None:
                        # å–¶æ¥­æ™‚é–“å†…ï¼ˆå¹³æ—¥ã®9:00-21:00 JST = 0:00-12:00 UTCï¼‰ã«èª¿æ•´
                        if trade_time.weekday() >= 5:  # åœŸæ—¥ã¯æœˆæ›œã«ç§»å‹•
                            trade_time += timedelta(days=(7 - trade_time.weekday()))
                        # æ™‚é–“èª¿æ•´ï¼ˆ9:00-21:00 JST = 0:00-12:00 UTCï¼‰
                        hour = trade_time.hour
                        if hour < 0:  # JST 9:00 = UTC 0:00
                            trade_time = trade_time.replace(hour=0)
                        elif hour > 12:  # JST 21:00 = UTC 12:00
                            trade_time = trade_time.replace(hour=12)
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ™‚é–“è¶³ã«å¿œã˜ãŸæœŸé–“å¾Œã«å»ºå€¤æ±ºæ¸ˆ
                        exit_minutes = self._get_fallback_exit_minutes(timeframe)
                        exit_time = trade_time + timedelta(minutes=exit_minutes)
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¤å®šä¸èƒ½ã®ãŸã‚å»ºå€¤æ±ºæ¸ˆï¼ˆãƒ—ãƒ©ãƒã‚¤0ï¼‰
                        is_success = None  # åˆ¤å®šä¸èƒ½ã‚’ç¤ºã™
                        exit_price = entry_price  # å»ºå€¤æ±ºæ¸ˆ
                    
                    # PnLè¨ˆç®—
                    pnl_pct = (exit_price - entry_price) / entry_price
                    leveraged_pnl = pnl_pct * leverage
                    
                    # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®ç·åˆæ¤œè¨¼
                    backtest_validation = self.price_validator.validate_backtest_result(
                        entry_price=entry_price,
                        stop_loss_price=sl_price,
                        take_profit_price=tp_price,
                        exit_price=exit_price,
                        duration_minutes=int((exit_time - trade_time).total_seconds() / 60),
                        symbol=symbol
                    )
                    
                    if not backtest_validation['is_valid']:
                        logger.warning(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœç•°å¸¸æ¤œçŸ¥: {symbol} {timeframe}")
                        logger.warning(f"å•é¡Œ: {', '.join(backtest_validation['issues'])}")
                        
                        # é‡å¤§ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯éŠ˜æŸ„è¿½åŠ è‡ªä½“ã‚’åœæ­¢
                        if backtest_validation['severity_level'] == 'critical':
                            logger.error(f"é‡å¤§ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç•°å¸¸ã®ãŸã‚éŠ˜æŸ„è¿½åŠ ã‚’åœæ­¢: {symbol} at {trade_time}")
                            logger.error(f"è©³ç´°: {', '.join(backtest_validation['issues'])}")
                            raise Exception(f"é‡å¤§ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç•°å¸¸æ¤œçŸ¥: {', '.join(backtest_validation['issues'])}")
                    
                    # æ—¥æœ¬æ™‚é–“ï¼ˆUTC+9ï¼‰ã§è¡¨ç¤º
                    jst_entry_time = trade_time + timedelta(hours=9)
                    jst_exit_time = exit_time + timedelta(hours=9)
                    
                    trades.append({
                        'entry_time': jst_entry_time.strftime('%Y-%m-%d %H:%M:%S JST'),
                        'exit_time': jst_exit_time.strftime('%Y-%m-%d %H:%M:%S JST'),
                        'entry_price': entry_price,
                        'exit_price': exit_price,
                        'take_profit_price': tp_price,
                        'stop_loss_price': sl_price,
                        'leverage': leverage,
                        'pnl_pct': leveraged_pnl,
                        'confidence': confidence,
                        'is_success': is_success,
                        'trade_type': 'breakeven' if is_success is None else ('profit' if is_success else 'loss'),
                        'strategy': config,
                        # ä¾¡æ ¼æ•´åˆæ€§æƒ…å ±ã®è¿½åŠ 
                        'price_consistency_score': unified_price_data.consistency_score,
                        'price_validation_level': price_consistency_result.inconsistency_level.value,
                        'backtest_validation_severity': backtest_validation['severity_level'],
                        'analysis_price': current_price  # ãƒ‡ãƒãƒƒã‚°ç”¨
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ (è©•ä¾¡{total_evaluations}): {str(e)[:100]}")
                    logger.warning(f"Analysis failed for {symbol} at {current_time}: {e}")
                
                # æ¬¡ã®è©•ä¾¡æ™‚ç‚¹ã«é€²ã‚€
                current_time += evaluation_interval
            
            # è©•ä¾¡å›æ•°åˆ¶é™ã«é”ã—ãŸå ´åˆã®è­¦å‘Š
            if total_evaluations >= max_evaluations:
                print(f"âš ï¸ {symbol} {timeframe} {config}: æœ€å¤§è©•ä¾¡å›æ•°({max_evaluations})ã«é”ã—ã¾ã—ãŸ")
            
            if not trades:
                print(f"â„¹ï¸ {symbol} {timeframe} {config}: è©•ä¾¡æœŸé–“ä¸­ã«æ¡ä»¶ã‚’æº€ãŸã™ã‚·ã‚°ãƒŠãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []  # ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ï¼‰
            
            evaluation_rate = (signals_generated / total_evaluations * 100) if total_evaluations > 0 else 0
            print(f"âœ… {symbol} {timeframe} {config}: æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æå®Œäº†")
            print(f"   ğŸ“Š ç·è©•ä¾¡æ•°: {total_evaluations}, ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ: {signals_generated}ä»¶ ({evaluation_rate:.1f}%)")
            
            # ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            if trades:
                validation_summary = self.price_validator.get_validation_summary(hours=24)
                if validation_summary['total_validations'] > 0:
                    print(f"   ğŸ” ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {validation_summary['consistency_rate']:.1f}% æ•´åˆæ€§")
                    print(f"   ğŸ“ˆ å¹³å‡ä¾¡æ ¼å·®ç•°: {validation_summary['avg_difference_pct']:.2f}%")
                    if validation_summary['level_counts'].get('critical', 0) > 0:
                        print(f"   âš ï¸ é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆ: {validation_summary['level_counts']['critical']}ä»¶")
            
            return trades
            
        except Exception as e:
            logger.error(f"Condition-based analysis failed: {e}")
            raise
    
    def _evaluate_entry_conditions(self, analysis_result, timeframe):
        """
        ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’è©•ä¾¡ã—ã¦ã€ã‚·ã‚°ãƒŠãƒ«ç”ŸæˆãŒé©åˆ‡ã‹ã‚’åˆ¤å®š
        
        Args:
            analysis_result: ãƒã‚¤ãƒ¬ãƒãƒœãƒƒãƒˆã‹ã‚‰ã®åˆ†æçµæœ
            timeframe: æ™‚é–“è¶³
            
        Returns:
            bool: ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã©ã†ã‹
        """
        
        # åŸºæœ¬çš„ãªæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        leverage = analysis_result.get('leverage', 0)
        confidence = analysis_result.get('confidence', 0) / 100.0  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‹ã‚‰æ¯”ç‡ã«å¤‰æ›
        risk_reward = analysis_result.get('risk_reward_ratio', 0)
        current_price = analysis_result.get('current_price', 0)
        
        # çµ±åˆè¨­å®šã‹ã‚‰æ¡ä»¶ã‚’å–å¾—ï¼ˆæˆ¦ç•¥ã‚‚è€ƒæ…®ï¼‰
        try:
            from config.unified_config_manager import UnifiedConfigManager
            config_manager = UnifiedConfigManager()
            
            # åˆ†æä¸­ã®æˆ¦ç•¥ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Balancedï¼‰
            strategy = analysis_result.get('strategy', 'Balanced')
            
            # çµ±åˆè¨­å®šã‹ã‚‰æ¡ä»¶ã‚’å–å¾—
            conditions = config_manager.get_entry_conditions(timeframe, strategy)
            
        except Exception as e:
            # è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯éŠ˜æŸ„è¿½åŠ ã‚’åœæ­¢
            error_msg = f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è¨­å®šãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}"
            print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {error_msg}")
            raise InsufficientConfigurationError(
                message=error_msg,
                error_type="entry_conditions_config_failed",
                missing_config="unified_entry_conditions"
            )
        
        # æ¡ä»¶è©•ä¾¡
        conditions_met = []
        
        # 1. ãƒ¬ãƒãƒ¬ãƒƒã‚¸æ¡ä»¶
        leverage_ok = leverage >= conditions['min_leverage']
        conditions_met.append(('leverage', leverage_ok, f"{leverage:.1f}x >= {conditions['min_leverage']}x"))
        
        # 2. ä¿¡é ¼åº¦æ¡ä»¶
        confidence_ok = confidence >= conditions['min_confidence']
        conditions_met.append(('confidence', confidence_ok, f"{confidence:.1%} >= {conditions['min_confidence']:.1%}"))
        
        # 3. ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¡ä»¶
        risk_reward_ok = risk_reward >= conditions['min_risk_reward']
        conditions_met.append(('risk_reward', risk_reward_ok, f"{risk_reward:.1f} >= {conditions['min_risk_reward']}"))
        
        # 4. ä¾¡æ ¼ã®æœ‰åŠ¹æ€§
        price_ok = current_price > 0
        conditions_met.append(('price', price_ok, f"price={current_price}"))
        
        # å…¨ã¦ã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        all_conditions_met = all(condition[1] for condition in conditions_met)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: OPã®æ¡ä»¶è©•ä¾¡è©³ç´°
        if 'OP' in str(analysis_result.get('symbol', '')):
            logger.error(f"ğŸ” OPæ¡ä»¶è©•ä¾¡è©³ç´°:")
            logger.error(f"   ãƒ¬ãƒãƒ¬ãƒƒã‚¸: {leverage} (å¿…è¦: {conditions['min_leverage']}) â†’ {leverage_ok}")
            logger.error(f"   ä¿¡é ¼åº¦: {confidence:.1%} (å¿…è¦: {conditions['min_confidence']:.1%}) â†’ {confidence_ok}")
            logger.error(f"   RRæ¯”: {risk_reward} (å¿…è¦: {conditions['min_risk_reward']}) â†’ {risk_reward_ok}")
            logger.error(f"   çµæœ: {all_conditions_met}")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°ï¼ˆæ¡ä»¶æº€è¶³æ™‚ã®ã¿è©³ç´°è¡¨ç¤ºï¼‰
        if all_conditions_met:
            print(f"   âœ… ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶æº€è¶³: L={leverage:.1f}x, C={confidence:.1%}, RR={risk_reward:.1f}")
        
        return all_conditions_met
    
    def _analysis_exists(self, analysis_id):
        """åˆ†æãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # ãƒãƒƒã‚·ãƒ¥IDã®å ´åˆã¯DBã‹ã‚‰ç›´æ¥æ¤œç´¢
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ã¾ãšãƒãƒƒã‚·ãƒ¥IDã§æ¤œç´¢ã‚’è©¦è¡Œ
            if len(analysis_id) == 32:  # MD5ãƒãƒƒã‚·ãƒ¥ã®å ´åˆ
                cursor.execute(
                    'SELECT COUNT(*) FROM analyses WHERE symbol || "_" || timeframe || "_" || config = ?',
                    (analysis_id,)
                )
            else:
                # å¾“æ¥ã®å½¢å¼ã®å ´åˆ
                try:
                    symbol, timeframe, config = analysis_id.split('_', 2)
                    cursor.execute(
                        'SELECT COUNT(*) FROM analyses WHERE symbol=? AND timeframe=? AND config=?',
                        (symbol, timeframe, config)
                    )
                except ValueError:
                    return False
            return cursor.fetchone()[0] > 0
    
    # TODO: ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆã¯å“è³ªå•é¡Œã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ (2024-06-18)
    # ç¾åœ¨ã¯ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ãŒã€å°†æ¥çš„ã«å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ–¹æ³•ã«ç½®ãæ›ãˆã‚‹å¿…è¦ã‚ã‚Š
    # def _generate_sample_trades(self, symbol, timeframe, config, num_trades=100):
    #     """ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    #     np.random.seed(hash(f"{symbol}_{timeframe}_{config}") % 2**32)
    #     
    #     # åŸºæœ¬æ€§èƒ½è¨­å®š
    #     base_performance = {
    #         'Conservative_ML': {'sharpe': 1.2, 'win_rate': 0.65},
    #         'Aggressive_Traditional': {'sharpe': 1.8, 'win_rate': 0.55},
    #         'Full_ML': {'sharpe': 2.1, 'win_rate': 0.62}
    #     }.get(config, {'sharpe': 1.5, 'win_rate': 0.58})
    #     
    #     trades = []
    #     cumulative_return = 0
    #     
    #     for i in range(num_trades):
    #         # ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆ
    #         is_win = np.random.random() < base_performance['win_rate']
    #         
    #         if is_win:
    #             pnl_pct = np.random.exponential(0.03)
    #         else:
    #             pnl_pct = -np.random.exponential(0.015)
    #         
    #         leverage = np.random.uniform(2.0, 8.0)
    #         leveraged_pnl = pnl_pct * leverage
    #         cumulative_return += leveraged_pnl
    #         
    #         trades.append({
    #             'trade_id': i,
    #             'pnl_pct': leveraged_pnl,
    #             'leverage': leverage,
    #             'is_win': is_win,
    #             'cumulative_return': cumulative_return
    #         })
    #     
    #     return pd.DataFrame(trades)
    
    # æš«å®šå®Ÿè£…: ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
    def _generate_sample_trades(self, symbol, timeframe, config, num_trades=100):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰"""
        raise NotImplementedError(
            "ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆã¯å“è³ªå•é¡Œã®ãŸã‚ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            "å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )
    
    def _calculate_metrics(self, trades_data):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # ãƒªã‚¹ãƒˆã®å ´åˆã¯DataFrameã«å¤‰æ›
        if isinstance(trades_data, list):
            if not trades_data:
                return {
                    'total_trades': 0,
                    'total_return': 0,
                    'win_rate': 0,
                    'sharpe_ratio': 0,
                    'max_drawdown': 0,
                    'avg_leverage': 0
                }
            
            # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã¨is_winã‚’è¨ˆç®—
            cumulative_return = 0
            for trade in trades_data:
                cumulative_return += trade['pnl_pct']
                trade['cumulative_return'] = cumulative_return
                
                # is_winè¨ˆç®—ã®æ”¹è‰¯ï¼ˆå»ºå€¤æ±ºæ¸ˆå¯¾å¿œï¼‰
                is_success = trade.get('is_success')
                if is_success is None:
                    # å»ºå€¤æ±ºæ¸ˆï¼ˆåˆ¤å®šä¸èƒ½ï¼‰ã®å ´åˆã¯å‹æ•—åˆ¤å®šå¯¾è±¡å¤–
                    trade['is_win'] = None
                else:
                    # é€šå¸¸ã®å‹æ•—åˆ¤å®š
                    trade['is_win'] = is_success if is_success is not None else (trade['pnl_pct'] > 0)
            
            trades_df = pd.DataFrame(trades_data)
        else:
            trades_df = trades_data
        
        if len(trades_df) == 0:
            return {
                'total_trades': 0,
                'total_return': 0,
                'win_rate': 0,
                'sharpe_ratio': 0,
                'max_drawdown': 0,
                'avg_leverage': 0
            }
        
        total_return = trades_df['cumulative_return'].iloc[-1] if len(trades_df) > 0 else 0
        
        # å‹ç‡è¨ˆç®—ã®æ”¹è‰¯ï¼ˆå»ºå€¤æ±ºæ¸ˆã‚’é™¤å¤–ï¼‰
        if len(trades_df) > 0:
            valid_trades = trades_df['is_win'].notna()  # Noneä»¥å¤–ï¼ˆå‹æ•—åˆ¤å®šå¯èƒ½ï¼‰
            win_rate = trades_df[valid_trades]['is_win'].mean() if valid_trades.sum() > 0 else 0
        else:
            win_rate = 0
        
        # Sharpeæ¯”ã®ç°¡æ˜“è¨ˆç®—
        returns = trades_df['pnl_pct'].values
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
        
        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
        cum_returns = trades_df['cumulative_return'].values
        peak = np.maximum.accumulate(cum_returns)
        drawdown = (cum_returns - peak) / peak
        max_drawdown = np.min(drawdown) if len(drawdown) > 0 else 0
        
        # ä¾¡æ ¼æ•´åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        price_consistency_metrics = {}
        if 'price_consistency_score' in trades_df.columns:
            price_consistency_metrics = {
                'avg_price_consistency': trades_df['price_consistency_score'].mean(),
                'critical_price_issues': len(trades_df[trades_df['price_validation_level'] == 'critical']),
                'critical_backtest_issues': len(trades_df[trades_df['backtest_validation_severity'] == 'critical'])
            }
        else:
            price_consistency_metrics = {
                'avg_price_consistency': 1.0,
                'critical_price_issues': 0,
                'critical_backtest_issues': 0
            }
        
        # å»ºå€¤æ±ºæ¸ˆçµ±è¨ˆã®è¨ˆç®—
        breakeven_stats = {}
        if len(trades_df) > 0:
            breakeven_count = trades_df['is_win'].isna().sum()
            total_decisive_trades = len(trades_df) - breakeven_count
            breakeven_stats = {
                'breakeven_trades': breakeven_count,
                'decisive_trades': total_decisive_trades,
                'breakeven_rate': (breakeven_count / len(trades_df)) if len(trades_df) > 0 else 0
            }
        else:
            breakeven_stats = {
                'breakeven_trades': 0,
                'decisive_trades': 0,
                'breakeven_rate': 0
            }
        
        base_metrics = {
            'total_trades': len(trades_df),
            'total_return': total_return,
            'win_rate': win_rate,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'avg_leverage': trades_df['leverage'].mean() if len(trades_df) > 0 else 0
        }
        
        # å»ºå€¤æ±ºæ¸ˆçµ±è¨ˆã‚’è¿½åŠ 
        base_metrics.update(breakeven_stats)
        
        # ä¾¡æ ¼æ•´åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµåˆ
        base_metrics.update(price_consistency_metrics)
        return base_metrics
    
    def _save_compressed_data(self, analysis_id, trades_df):
        """ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ã—ã¦ä¿å­˜"""
        compressed_path = self.compressed_dir / f"{analysis_id}.pkl.gz"
        
        with gzip.open(compressed_path, 'wb') as f:
            pickle.dump(trades_df, f)
        
        return str(compressed_path)
    
    def _should_generate_chart(self, metrics):
        """ãƒãƒ£ãƒ¼ãƒˆç”ŸæˆãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®šï¼ˆä¸Šä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ã¿ï¼‰"""
        return metrics['sharpe_ratio'] > 1.5  # Sharpe > 1.5ã®ã¿ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ
    
    def _generate_lightweight_chart(self, analysis_id, trades_df, metrics):
        """è»½é‡ç‰ˆãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ"""
        # å®Ÿè£…ã¯çœç•¥ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
        chart_path = self.charts_dir / f"{analysis_id}_chart.html"
        
        # ç°¡æ˜“HTMLä½œæˆ
        html_content = f"""
        <html>
        <head><title>{analysis_id} Analysis</title></head>
        <body>
        <h1>{analysis_id}</h1>
        <p>Sharpe Ratio: {metrics['sharpe_ratio']:.2f}</p>
        <p>Win Rate: {metrics['win_rate']:.1%}</p>
        <p>Total Return: {metrics['total_return']:.1%}</p>
        </body>
        </html>
        """
        
        with open(chart_path, 'w') as f:
            f.write(html_content)
        
        return str(chart_path)
    
    def _save_to_database(self, symbol, timeframe, config, metrics, chart_path, compressed_path, execution_id=None):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆexecution_idå¯¾å¿œ + task_statusæ›´æ–°ï¼‰"""
        logger.info(f"ğŸ’¾ DBä¿å­˜é–‹å§‹: {symbol} {timeframe} {config}")
        logger.info(f"  ğŸ—ƒï¸ ä¿å­˜å…ˆDB: {self.db_path.absolute()}")
        logger.info(f"  ğŸ”‘ execution_id: {execution_id or os.environ.get('CURRENT_EXECUTION_ID', 'None')}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # execution_idã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯å¼•æ•°ã‹ã‚‰å–å¾—
            if not execution_id:
                execution_id = os.environ.get('CURRENT_EXECUTION_ID')
            
            try:
                # ğŸ”¥ é‡è¦: INSERTã‹ã‚‰UPDATEã«å¤‰æ›´ï¼ˆpre-taskãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°ï¼‰
                cursor.execute('''
                    UPDATE analyses SET
                        total_trades=?, win_rate=?, total_return=?, 
                        sharpe_ratio=?, max_drawdown=?, avg_leverage=?, 
                        chart_path=?, compressed_path=?, 
                        status='completed', task_status='completed', task_completed_at=?
                    WHERE symbol=? AND timeframe=? AND config=? AND execution_id=?
                ''', (
                    metrics['total_trades'], metrics['win_rate'], metrics['total_return'],
                    metrics['sharpe_ratio'], metrics['max_drawdown'], metrics['avg_leverage'],
                    chart_path, compressed_path, 
                    datetime.now(timezone.utc).isoformat(),
                    symbol, timeframe, config, execution_id
                ))
                
                updated_rows = cursor.rowcount
                if updated_rows == 0:
                    # Pre-taskãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å¾“æ¥é€šã‚ŠINSERT
                    logger.warning(f"âš ï¸ Pre-taskãƒ¬ã‚³ãƒ¼ãƒ‰ãªã— - INSERTå®Ÿè¡Œ: {symbol} {timeframe} {config}")
                    cursor.execute('''
                        INSERT INTO analyses 
                        (symbol, timeframe, config, total_trades, win_rate, total_return, 
                         sharpe_ratio, max_drawdown, avg_leverage, chart_path, compressed_path, status, 
                         task_status, task_completed_at, execution_id)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'completed', 'completed', ?, ?)
                    ''', (
                        symbol, timeframe, config,
                        metrics['total_trades'], metrics['win_rate'], metrics['total_return'],
                        metrics['sharpe_ratio'], metrics['max_drawdown'], metrics['avg_leverage'],
                        chart_path, compressed_path, 
                        datetime.now(timezone.utc).isoformat(),
                        execution_id
                    ))
                
                conn.commit()
                logger.info(f"âœ… DBä¿å­˜æˆåŠŸ: {symbol} {timeframe} {config} ({'UPDATE' if updated_rows > 0 else 'INSERT'})")
                
            except Exception as e:
                logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {symbol} {timeframe} {config}")
                logger.error(f"  ğŸ—ƒï¸ DB path: {self.db_path.absolute()}")
                logger.error(f"  ğŸ“ ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
                raise
    
    def query_analyses(self, filters=None, order_by='sharpe_ratio', limit=100):
        """åˆ†æçµæœã‚’ã‚¯ã‚¨ãƒª"""
        with sqlite3.connect(self.db_path) as conn:
            query = "SELECT * FROM analyses WHERE status='completed'"
            params = []
            
            if filters:
                if 'symbol' in filters:
                    if isinstance(filters['symbol'], list):
                        query += " AND symbol IN ({})".format(','.join(['?' for _ in filters['symbol']]))
                        params.extend(filters['symbol'])
                    else:
                        query += " AND symbol = ?"
                        params.append(filters['symbol'])
                
                if 'timeframe' in filters:
                    if isinstance(filters['timeframe'], list):
                        query += " AND timeframe IN ({})".format(','.join(['?' for _ in filters['timeframe']]))
                        params.extend(filters['timeframe'])
                    else:
                        query += " AND timeframe = ?"
                        params.append(filters['timeframe'])
                
                if 'config' in filters:
                    if isinstance(filters['config'], list):
                        query += " AND config IN ({})".format(','.join(['?' for _ in filters['config']]))
                        params.extend(filters['config'])
                    else:
                        query += " AND config = ?"
                        params.append(filters['config'])
                
                if 'min_sharpe' in filters:
                    query += " AND sharpe_ratio >= ?"
                    params.append(filters['min_sharpe'])
            
            query += f" ORDER BY {order_by} DESC LIMIT ?"
            params.append(limit)
            
            cursor = conn.cursor()
            cursor.execute(query, params)
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            
            # Create a list of dictionaries instead of pandas DataFrame
            result = []
            for row in rows:
                result.append(dict(zip(columns, row)))
            return result
    
    def get_statistics(self):
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) as total, status FROM analyses GROUP BY status")
            status_counts = cursor.fetchall()
            
            # æ€§èƒ½çµ±è¨ˆ
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_analyses,
                    AVG(sharpe_ratio) as avg_sharpe,
                    MAX(sharpe_ratio) as max_sharpe,
                    AVG(total_return) as avg_return,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    COUNT(DISTINCT config) as unique_configs
                FROM analyses WHERE status='completed'
            """)
            perf_stats = cursor.fetchone()
            
            return {
                'status_counts': dict(status_counts),
                'performance': {
                    'total_analyses': perf_stats[0],
                    'avg_sharpe': perf_stats[1],
                    'max_sharpe': perf_stats[2],
                    'avg_return': perf_stats[3],
                    'unique_symbols': perf_stats[4],
                    'unique_configs': perf_stats[5]
                }
            }
    
    def load_compressed_trades(self, symbol, timeframe, config):
        """åœ§ç¸®ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        analysis_id = f"{symbol}_{timeframe}_{config}"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‘ã‚¹ã‚’å–å¾—
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT compressed_path FROM analyses WHERE symbol=? AND timeframe=? AND config=?",
                (symbol, timeframe, config)
            )
            result = cursor.fetchone()
            
            if not result or not result[0]:
                logger.warning(f"åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {analysis_id}")
                return None
            
            compressed_path = result[0]
        
        # åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        try:
            with gzip.open(compressed_path, 'rb') as f:
                trades_df = pickle.load(f)
            logger.info(f"ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {analysis_id} ({len(trades_df)}ãƒˆãƒ¬ãƒ¼ãƒ‰)")
            return trades_df
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {analysis_id}: {e}")
            return None
    
    def load_multiple_trades(self, criteria=None):
        """è¤‡æ•°ã®åœ§ç¸®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬èª­ã¿è¾¼ã¿"""
        # ã‚¯ã‚¨ãƒªæ¡ä»¶ã«åŸºã¥ã„ã¦åˆ†æã‚’å–å¾—
        analyses_df = self.query_analyses(filters=criteria)
        
        all_trades = {}
        for _, analysis in analyses_df.iterrows():
            trades_df = self.load_compressed_trades(
                analysis['symbol'], 
                analysis['timeframe'], 
                analysis['config']
            )
            if trades_df is not None:
                key = f"{analysis['symbol']}_{analysis['timeframe']}_{analysis['config']}"
                all_trades[key] = trades_df
        
        logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(all_trades)}ä»¶ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿")
        return all_trades
    
    def export_to_csv(self, symbol, timeframe, config, output_path=None):
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        trades_df = self.load_compressed_trades(symbol, timeframe, config)
        
        if trades_df is None:
            return False
        
        if output_path is None:
            output_path = f"{symbol}_{timeframe}_{config}_trades_export.csv"
        
        trades_df.to_csv(output_path, index=False)
        logger.info(f"CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}")
        return True
    
    def get_analysis_details(self, symbol, timeframe, config):
        """åˆ†æã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ + ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰åŸºæœ¬æƒ…å ±å–å¾—
        with sqlite3.connect(self.db_path) as conn:
            query = """
                SELECT * FROM analyses 
                WHERE symbol=? AND timeframe=? AND config=?
            """
            analysis_info = pd.read_sql_query(query, conn, params=[symbol, timeframe, config])
        
        if analysis_info.empty:
            return None
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
        trades_df = self.load_compressed_trades(symbol, timeframe, config)
        
        return {
            'info': analysis_info.iloc[0].to_dict(),
            'trades': trades_df
        }
    
    def cleanup_low_performers(self, min_sharpe=0.5):
        """ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # å‰Šé™¤å¯¾è±¡ã‚’å–å¾—
            cursor.execute(
                "SELECT chart_path, compressed_path FROM analyses WHERE sharpe_ratio < ?",
                (min_sharpe,)
            )
            to_delete = cursor.fetchall()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            deleted_files = 0
            for chart_path, data_path in to_delete:
                for file_path in [chart_path, data_path]:
                    if file_path and os.path.exists(file_path):
                        os.remove(file_path)
                        deleted_files += 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚‚å‰Šé™¤
            cursor.execute("DELETE FROM analyses WHERE sharpe_ratio < ?", (min_sharpe,))
            deleted_records = cursor.rowcount
            conn.commit()
            
            logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {deleted_records}ãƒ¬ã‚³ãƒ¼ãƒ‰, {deleted_files}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤")
            return deleted_records, deleted_files
    
    def _get_real_market_price(self, bot, symbol, timeframe, trade_time):
        """
        å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŒ‡å®šæ™‚åˆ»ã®ä¾¡æ ¼ã‚’å–å¾—
        ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»ãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚ç¾å®Ÿçš„ï¼‰
        
        Args:
            bot: HighLeverageBotOrchestrator ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«  
            timeframe: æ™‚é–“è¶³
            trade_time: ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»
        
        Returns:
            float: å®Ÿéš›ã®å¸‚å ´ä¾¡æ ¼ï¼ˆè©²å½“ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ï¼‰
        """
        try:
            # ãƒœãƒƒãƒˆã‹ã‚‰å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                market_data = bot._cached_data
            else:
                market_data = bot._fetch_market_data(symbol, timeframe)
            
            if market_data.empty:
                raise Exception(f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™: {symbol}")
            
            # ãƒ‡ãƒ¼ã‚¿ã® timestamp ã‚«ãƒ©ãƒ ã‚’ç¢ºèªãƒ»ä½œæˆ
            if 'timestamp' not in market_data.columns:
                if market_data.index.name == 'timestamp' or pd.api.types.is_datetime64_any_dtype(market_data.index):
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
                    market_data = market_data.reset_index()
                    if 'index' in market_data.columns:
                        market_data = market_data.rename(columns={'index': 'timestamp'})
                else:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ãªã„å ´åˆã¯ä½œæˆ
                    market_data['timestamp'] = pd.to_datetime(market_data.index, utc=True)
            
            # timestampã‚«ãƒ©ãƒ ã‚’datetimeå‹ã«ç¢ºå®Ÿã«å¤‰æ›
            market_data['timestamp'] = pd.to_datetime(market_data['timestamp'], utc=True)
            
            # trade_timeã‚’UTCã«å¤‰æ›
            if trade_time.tzinfo is None:
                target_time = trade_time
            else:
                target_time = trade_time.astimezone(timezone.utc)
            
            # trade_timeãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã‚’è¨ˆç®—
            candle_start_time = self._get_candle_start_time(target_time, timeframe)
            
            # è©²å½“ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã‚’ç‰¹å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è€ƒæ…®ï¼‰
            # market_dataã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’UTCã«çµ±ä¸€
            if market_data['timestamp'].dt.tz is None:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_localize('UTC')
            else:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_convert('UTC')
            
            # candle_start_timeã‚‚UTCã«çµ±ä¸€
            if candle_start_time.tzinfo is None:
                candle_start_time = candle_start_time.replace(tzinfo=timezone.utc)
            else:
                candle_start_time = candle_start_time.astimezone(timezone.utc)
            
            # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ•°åˆ†ã®èª¤å·®ã‚’è¨±å®¹ï¼‰
            time_tolerance = timedelta(minutes=1)
            target_candles = market_data[
                abs(market_data['timestamp'] - candle_start_time) <= time_tolerance
            ]
            
            if target_candles.empty:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
                available_times = market_data['timestamp'].head(10).tolist()
                raise Exception(
                    f"è©²å½“ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symbol} {timeframe} "
                    f"trade_time={target_time}, candle_start={candle_start_time}. "
                    f"åˆ©ç”¨å¯èƒ½ãªæœ€åˆã®10ä»¶: {available_times}. "
                    f"å®Ÿéš›ã®å€¤ã®ã¿ä½¿ç”¨ã®ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚"
                )
            
            # æœ€ã‚‚è¿‘ã„ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã‚’é¸æŠ
            target_candle = target_candles.iloc[0]
            
            # ãã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ã‚’è¿”ã™ï¼ˆæœ€ã‚‚ç¾å®Ÿçš„ãªã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ï¼‰
            open_price = float(target_candle['open'])
            
            return open_price
            
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ä½¿ç”¨ã›ãšã€ã‚¨ãƒ©ãƒ¼ã§æˆ¦ç•¥åˆ†æã‚’çµ‚äº†
            raise Exception(f"å®Ÿéš›ã®å¸‚å ´ä¾¡æ ¼å–å¾—ã«å¤±æ•—: {symbol} - {str(e)}")
    
    def _get_candle_start_time(self, trade_time, timeframe):
        """
        ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»ãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã‚’è¨ˆç®—
        
        Args:
            trade_time: ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»
            timeframe: æ™‚é–“è¶³ï¼ˆä¾‹: '15m', '1h', '4h'ï¼‰
        
        Returns:
            datetime: ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»
        """
        # æ™‚é–“è¶³ã‚’åˆ†å˜ä½ã«å¤‰æ›
        timeframe_minutes = {
            '1m': 1, '3m': 3, '5m': 5, '15m': 15, 
            '30m': 30, '1h': 60, '4h': 240, '1d': 1440
        }
        
        if timeframe not in timeframe_minutes:
            raise Exception(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„æ™‚é–“è¶³: {timeframe}")
        
        minutes_interval = timeframe_minutes[timeframe]
        
        # trade_timeã‚’è©²å½“ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã«ä¸¸ã‚ã‚‹
        if timeframe == '1d':
            # æ—¥è¶³ã®å ´åˆã¯00:00:00ã«ä¸¸ã‚ã‚‹
            candle_start = trade_time.replace(hour=0, minute=0, second=0, microsecond=0)
        else:
            # åˆ†è¶³ãƒ»æ™‚é–“è¶³ã®å ´åˆ
            total_minutes = trade_time.hour * 60 + trade_time.minute
            candle_minutes = (total_minutes // minutes_interval) * minutes_interval
            
            candle_hour = candle_minutes // 60
            candle_minute = candle_minutes % 60
            
            candle_start = trade_time.replace(
                hour=candle_hour, 
                minute=candle_minute, 
                second=0, 
                microsecond=0
            )
        
        return candle_start
    
    def _find_tp_sl_exit(self, bot, symbol, timeframe, entry_time, entry_price, tp_price, sl_price):
        """
        TP/SLåˆ°é”ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ã‚ºåˆ¤å®š
        
        Args:
            bot: HighLeverageBotOrchestrator ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«
            timeframe: æ™‚é–“è¶³
            entry_time: ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ™‚åˆ»
            entry_price: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼
            tp_price: åˆ©ç¢ºä¾¡æ ¼
            sl_price: æåˆ‡ã‚Šä¾¡æ ¼
            
        Returns:
            tuple: (exit_time, exit_price, is_success)
        """
        try:
            # ãƒœãƒƒãƒˆã‹ã‚‰å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                market_data = bot._cached_data
            else:
                market_data = bot._fetch_market_data(symbol, timeframe)
            
            if market_data.empty:
                return None, None, None
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã®æº–å‚™
            if 'timestamp' not in market_data.columns:
                if market_data.index.name == 'timestamp' or pd.api.types.is_datetime64_any_dtype(market_data.index):
                    market_data = market_data.reset_index()
                    if 'index' in market_data.columns:
                        market_data = market_data.rename(columns={'index': 'timestamp'})
                else:
                    market_data['timestamp'] = pd.to_datetime(market_data.index, utc=True)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’çµ±ä¸€
            market_data['timestamp'] = pd.to_datetime(market_data['timestamp'], utc=True)
            if market_data['timestamp'].dt.tz is None:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_localize('UTC')
            else:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_convert('UTC')
            
            # entry_timeã‚’UTCã«å¤‰æ›
            if entry_time.tzinfo is None:
                entry_time_utc = entry_time.replace(tzinfo=timezone.utc)
            else:
                entry_time_utc = entry_time.astimezone(timezone.utc)
            
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            after_entry_data = market_data[market_data['timestamp'] > entry_time_utc].copy()
            
            if after_entry_data.empty:
                return None, None, None
            
            # å„ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã§TP/SLåˆ°é”ã‚’ãƒã‚§ãƒƒã‚¯
            for _, candle in after_entry_data.iterrows():
                # ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’æƒ³å®š
                candle_high = float(candle['high'])
                candle_low = float(candle['low'])
                
                # åˆ©ç¢ºãƒ©ã‚¤ãƒ³åˆ°é”ãƒã‚§ãƒƒã‚¯
                if candle_high >= tp_price:
                    exit_time = candle['timestamp']
                    exit_price = tp_price
                    is_success = True
                    return exit_time, exit_price, is_success
                
                # æåˆ‡ã‚Šãƒ©ã‚¤ãƒ³åˆ°é”ãƒã‚§ãƒƒã‚¯
                if candle_low <= sl_price:
                    exit_time = candle['timestamp']
                    exit_price = sl_price
                    is_success = False
                    return exit_time, exit_price, is_success
            
            # è©•ä¾¡æœŸé–“å†…ã«åˆ°é”ã—ãªã‹ã£ãŸå ´åˆ
            return None, None, None
            
        except Exception as e:
            logger.warning(f"TP/SLåˆ°é”åˆ¤å®šã‚¨ãƒ©ãƒ¼: {symbol} - {e}")
            return None, None, None
    
    def _get_fallback_exit_minutes(self, timeframe):
        """æ™‚é–“è¶³ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€€å‡ºæ™‚é–“ã‚’å–å¾—"""
        fallback_minutes = {
            '1m': 15,    # 15åˆ†å¾Œ
            '3m': 30,    # 30åˆ†å¾Œ
            '5m': 45,    # 45åˆ†å¾Œ
            '15m': 60,   # 1æ™‚é–“å¾Œ
            '30m': 90,   # 1.5æ™‚é–“å¾Œ
            '1h': 120    # 2æ™‚é–“å¾Œ
        }
        return fallback_minutes.get(timeframe, 60)

def generate_large_scale_configs(symbols_count=20, timeframes=4, configs=10):
    """å¤§è¦æ¨¡è¨­å®šã‚’ç”Ÿæˆ"""
    symbols = [f"TOKEN{i:03d}" for i in range(1, symbols_count + 1)]
    timeframes_list = ['1m', '3m', '5m', '15m', '30m', '1h'][:timeframes]
    configs_list = [f"Config_{i:02d}" for i in range(1, configs + 1)]
    
    batch_configs = []
    for symbol in symbols:
        for timeframe in timeframes_list:
            for config in configs_list:
                batch_configs.append({
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'config': config
                })
    
    return batch_configs

def main():
    """å¤§è¦æ¨¡åˆ†æã®ãƒ‡ãƒ¢"""
    system = ScalableAnalysisSystem()
    
    # 1000ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨­å®šç”Ÿæˆ
    configs = generate_large_scale_configs(symbols_count=10, timeframes=4, configs=25)
    print(f"ç”Ÿæˆã•ã‚ŒãŸè¨­å®šæ•°: {len(configs)}")
    
    # ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ
    processed = system.generate_batch_analysis(configs, max_workers=4)
    print(f"å‡¦ç†å®Œäº†: {processed}ãƒ‘ã‚¿ãƒ¼ãƒ³")
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = system.get_statistics()
    print("\nçµ±è¨ˆ:")
    print(f"  ç·åˆ†ææ•°: {stats['performance']['total_analyses']}")
    print(f"  å¹³å‡Sharpe: {stats['performance']['avg_sharpe']:.2f}")
    print(f"  æœ€é«˜Sharpe: {stats['performance']['max_sharpe']:.2f}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯éŠ˜æŸ„æ•°: {stats['performance']['unique_symbols']}")
    
    # ä¸Šä½çµæœè¡¨ç¤º
    top_results = system.query_analyses(
        filters={'min_sharpe': 1.5},
        order_by='sharpe_ratio',
        limit=10
    )
    print(f"\nä¸Šä½10çµæœ:")
    print(top_results[['symbol', 'timeframe', 'config', 'sharpe_ratio', 'total_return']].to_string())

# ScalableAnalysisSystem ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
def add_get_timeframe_config_method():
    """ScalableAnalysisSystemã«è¨­å®šå–å¾—ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ """
    import types
    
    def get_timeframe_config(self, timeframe: str):
        """æ™‚é–“è¶³è¨­å®šã‚’å–å¾—ï¼ˆå¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
        try:
            from config.timeframe_config_manager import TimeframeConfigManager
            config_manager = TimeframeConfigManager()
            return config_manager.get_timeframe_config(timeframe)
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            default_configs = {
                '1m': {'max_evaluations': 100},
                '3m': {'max_evaluations': 80},
                '5m': {'max_evaluations': 120},
                '15m': {'max_evaluations': 100},
                '30m': {'max_evaluations': 80},
                '1h': {'max_evaluations': 100}
            }
            base_config = {
                'data_days': 90,
                'evaluation_interval_minutes': 240,
                'max_evaluations': 50,
                'min_leverage': 3.0,
                'min_confidence': 0.5,
                'min_risk_reward': 2.0
            }
            base_config.update(default_configs.get(timeframe, {}))
            return base_config
    
    # ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ 
    ScalableAnalysisSystem.get_timeframe_config = get_timeframe_config

# ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ ã‚’å®Ÿè¡Œ
add_get_timeframe_config_method()

if __name__ == "__main__":
    main()