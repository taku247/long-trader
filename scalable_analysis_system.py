"""
å¤§è¦æ¨¡ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
æ•°åƒãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªåˆ†æãƒ»å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
"""
import pandas as pd
import numpy as np
import os
import json
import sqlite3
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import cpu_count
import shutil
import gzip
import pickle
from datetime import datetime, timedelta, timezone
import logging
from pathlib import Path
import asyncio
import aiohttp
import time
from typing import List
import requests

# ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from engines.price_consistency_validator import PriceConsistencyValidator, UnifiedPriceData

# é€²æ—ãƒ­ã‚¬ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from progress_logger import SymbolProgressLogger

# ã‚¨ãƒ©ãƒ¼ä¾‹å¤–ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from engines.leverage_decision_engine import InsufficientConfigurationError

# Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
# ç†ç”±: æ€§èƒ½å•é¡Œ - "è»½é‡äº‹å‰ãƒã‚§ãƒƒã‚¯"ã¨è¬³ã„ãªãŒã‚‰é‡ã„è¨ˆç®—ã‚’å®Ÿè¡Œ
# è©³ç´°: README.mdå‚ç…§

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScalableAnalysisSystem:
    def __init__(self, base_dir="large_scale_analysis"):
        import os
        import inspect
        
        # ğŸ”§ å¼·åˆ¶çš„ã«rootãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®DBã‚’ä½¿ç”¨ï¼ˆå®Œå…¨DBçµ±ä¸€å¼·åŒ–ç‰ˆï¼‰
        script_dir = Path(__file__).parent.absolute()  # scalable_analysis_system.pyã®çµ¶å¯¾ãƒ‘ã‚¹
        
        # å¸¸ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®æ­£è¦DBã‚’ä½¿ç”¨ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ç¦æ­¢ï¼‰
        if os.path.isabs(base_dir):
            # çµ¶å¯¾ãƒ‘ã‚¹æŒ‡å®šã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            self.base_dir = Path(base_dir)
        else:
            # ç›¸å¯¾ãƒ‘ã‚¹æŒ‡å®šã¯å¿…ãšã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåŸºæº–ã«çµ±ä¸€
            self.base_dir = script_dir / base_dir
        
        # web_dashboardãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®DBä½œæˆã‚’å®Œå…¨ã«é˜²æ­¢
        if 'web_dashboard' in str(self.base_dir):
            logger.warning(f"âš ï¸ web_dashboardå†…DBä½œæˆã‚’é˜»æ­¢: {self.base_dir}")
            self.base_dir = script_dir / base_dir
            
        self.base_dir.mkdir(exist_ok=True)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        self.db_path = self.base_dir / "analysis.db"
        self.charts_dir = self.base_dir / "charts"
        self.data_dir = self.base_dir / "data"
        self.compressed_dir = self.base_dir / "compressed"
        
        # Note: åˆæœŸåŒ–ãƒ­ã‚°ã‚’å‰Šé™¤ï¼ˆå†—é•·å‡ºåŠ›é˜²æ­¢ï¼‰
        
        for dir_path in [self.charts_dir, self.data_dir, self.compressed_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self.price_validator = PriceConsistencyValidator(
            warning_threshold_pct=1.0,
            error_threshold_pct=5.0,
            critical_threshold_pct=10.0
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.init_database()
        
        # å®Ÿè¡Œåˆ¶å¾¡
        self.current_execution_id = None
        
        # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å‰Šé™¤æ¸ˆã¿ (æ€§èƒ½å•é¡Œã«ã‚ˆã‚Šæ’¤å»ƒ)
        # è©³ç´°: README.mdå‚ç…§
    
    def init_database(self):
        """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            # analysesãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã‚«ãƒ©ãƒ æ§‹é€ ã‚’ç¢ºèª
            if 'analyses' in existing_tables:
                cursor.execute("PRAGMA table_info(analyses);")
                columns = [row[1] for row in cursor.fetchall()]
                
                # execution_idã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
                if 'execution_id' not in columns:
                    # execution_idã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
                    cursor.execute('ALTER TABLE analyses ADD COLUMN execution_id TEXT')
                    logger.info("execution_idã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                
                if 'task_status' not in columns:
                    # ä¸è¶³ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_status TEXT DEFAULT "pending"')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_started_at TIMESTAMP')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN task_completed_at TIMESTAMP')
                    cursor.execute('ALTER TABLE analyses ADD COLUMN error_message TEXT')
                    logger.info("task_statusã‚«ãƒ©ãƒ ç­‰ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            
            # åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analyses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    config TEXT NOT NULL,
                    generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    total_trades INTEGER,
                    win_rate REAL,
                    total_return REAL,
                    sharpe_ratio REAL,
                    max_drawdown REAL,
                    avg_leverage REAL,
                    chart_path TEXT,
                    compressed_path TEXT,
                    status TEXT DEFAULT 'pending',
                    execution_id TEXT,
                    task_status TEXT DEFAULT 'pending',
                    task_started_at TIMESTAMP,
                    task_completed_at TIMESTAMP,
                    error_message TEXT
                )
            ''')
            
            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS backtest_summary (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    metric_name TEXT,
                    metric_value REAL,
                    FOREIGN KEY (analysis_id) REFERENCES analyses (id)
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol_timeframe ON analyses (symbol, timeframe)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_config ON analyses (config)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_sharpe ON analyses (sharpe_ratio)')
            
            conn.commit()
    
    def _should_cancel_execution(self, execution_id: str = None) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª"""
        if not execution_id and hasattr(self, 'current_execution_id'):
            execution_id = self.current_execution_id
        
        if not execution_id:
            return False
            
        try:
            from execution_log_database import ExecutionLogDatabase
            db = ExecutionLogDatabase()
            with sqlite3.connect(db.db_path) as conn:
                cursor = conn.execute(
                    'SELECT status FROM execution_logs WHERE execution_id = ?',
                    (execution_id,)
                )
                row = cursor.fetchone()
                return row and row[0] == 'CANCELLED'
        except Exception as e:
            logger.warning(f"Failed to check cancellation status: {e}")
            return False
    
    def generate_batch_analysis(self, batch_configs, max_workers=None, symbol=None, execution_id=None, skip_pretask_creation=False, custom_period_settings=None):
        """
        ãƒãƒƒãƒã§å¤§é‡ã®åˆ†æã‚’ä¸¦åˆ—ç”Ÿæˆ
        
        Args:
            batch_configs: [{'symbol': 'BTC', 'timeframe': '1h', 'config': 'ML'}, ...]
            max_workers: ä¸¦åˆ—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CPUæ•°ï¼‰
            symbol: éŠ˜æŸ„åï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
            execution_id: å®Ÿè¡ŒIDï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
            skip_pretask_creation: Pre-taskä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹
            custom_period_settings: ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®š
        """
        # ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šã®åˆæœŸåŒ–ï¼ˆå®‰å…¨æ€§ã®ãŸã‚æœ€åˆã«å®Ÿè¡Œï¼‰
        custom_period_settings = custom_period_settings or {}
        
        if max_workers is None:
            max_workers = min(cpu_count(), 4)  # Rate Limitå¯¾ç­–ã§æœ€å¤§4ä¸¦åˆ—
        
        # å®Ÿè¡ŒIDã‚’è¨­å®š
        self.current_execution_id = execution_id
        
        # ğŸ”¥ é‡è¦: Pre-taskä½œæˆï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¿½è·¡ã®ãŸã‚ï¼‰
        if execution_id and not skip_pretask_creation:
            self._create_pre_tasks(batch_configs, execution_id)
        
        # é€²æ—ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–
        progress_logger = None
        if symbol and execution_id:
            progress_logger = SymbolProgressLogger(symbol, execution_id, len(batch_configs))
            progress_logger.log_phase_start("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ", f"{len(batch_configs)}ãƒ‘ã‚¿ãƒ¼ãƒ³, {max_workers}ä¸¦åˆ—")
        else:
            logger.info(f"ãƒãƒƒãƒåˆ†æé–‹å§‹: {len(batch_configs)}ãƒ‘ã‚¿ãƒ¼ãƒ³, {max_workers}ä¸¦åˆ—")
        
        self.max_workers = max_workers  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿å­˜
        # self.progress_logger = progress_logger  # ğŸ› PickleåŒ–ã‚¨ãƒ©ãƒ¼ä¿®æ­£: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¸ã®ä¿å­˜ã‚’ç„¡åŠ¹åŒ–
        
        # ãƒãƒƒãƒã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunk_size = max(1, len(batch_configs) // max_workers)
        chunks = [batch_configs[i:i + chunk_size] for i in range(0, len(batch_configs), chunk_size)]
        
        # æœŸé–“è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ç”¨ï¼‰
        import os
        import json
        if custom_period_settings and custom_period_settings.get('mode'):
            os.environ['CUSTOM_PERIOD_SETTINGS'] = json.dumps(custom_period_settings)
            logger.info(f"ğŸ“… æœŸé–“è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š: {custom_period_settings}")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚‚å­ãƒ—ãƒ­ã‚»ã‚¹ã«ä¼é”
        debug_mode = os.environ.get('SUPPORT_RESISTANCE_DEBUG', 'false')
        if debug_mode.lower() == 'true':
            logger.info(f"ğŸ” Support/Resistance ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
            # å­ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚‚ç¢ºå®Ÿã«è¨­å®šã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
            os.environ['SUPPORT_RESISTANCE_DEBUG'] = 'true'
        
        # execution_idã‚‚å­ãƒ—ãƒ­ã‚»ã‚¹ã«ä¼é”ï¼ˆprogress_trackerç”¨ï¼‰
        if execution_id:
            os.environ['CURRENT_EXECUTION_ID'] = execution_id
            logger.info(f"ğŸ“ å®Ÿè¡ŒIDã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š: {execution_id}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹progress_trackerã®åˆæœŸåŒ–
            try:
                from file_based_progress_tracker import file_progress_tracker
                file_progress_tracker.start_analysis(symbol, execution_id)
                logger.info(f"ğŸ“ FileBasedProgressTrackeråˆæœŸåŒ–å®Œäº†: {execution_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ FileBasedProgressTrackeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i, chunk in enumerate(chunks):
                # execution_idã‚’æ˜ç¤ºçš„ã«æ¸¡ã™
                future = executor.submit(self._process_chunk, chunk, i, self.current_execution_id)
                futures.append(future)
            
            # çµæœåé›†ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            total_processed = 0
            for i, future in enumerate(futures):
                try:
                    # å„ãƒãƒ£ãƒ³ã‚¯ã«30åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                    processed_count = future.result(timeout=1800)  # 30 minutes
                    total_processed += processed_count
                    
                    if progress_logger:
                        # é€²æ—ãƒ­ã‚°ã¯å€‹åˆ¥æˆ¦ç•¥å®Œäº†æ™‚ã«å‡ºåŠ›ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç°¡æ½”ã«
                        pass
                    else:
                        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(futures)} å®Œäº†: {processed_count}ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†")
                except Exception as e:
                    logger.error(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    if progress_logger:
                        progress_logger.log_error(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", "ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ç ´æã•ã›ãªã„
                    if "BrokenProcessPool" in str(e):
                        logger.error("ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ç ´ææ¤œå‡º - æ®‹ã‚Šã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        break
        
        if progress_logger:
            progress_logger.log_phase_complete("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
            # æˆåŠŸåˆ¤å®š: åˆ†æãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆï¼ˆã‚·ã‚°ãƒŠãƒ«ãªã—ã§ã‚‚æˆåŠŸï¼‰
            analysis_attempted = len(batch_configs) > 0
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹progress_trackeræ›´æ–° - æ–°ã—ã„FileBasedProgressTrackerã‚’ä½¿ç”¨
            if execution_id:
                try:
                    from file_based_progress_tracker import file_progress_tracker
                    file_progress_tracker.update_stage(execution_id, "support_resistance")
                    logger.info(f"ğŸ“ æ–°progress_trackeræ®µéšæ›´æ–°: backtest_completed â†’ support_resistance")
                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹progress_trackeræ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            progress_logger.log_final_summary(analysis_attempted)
        else:
            logger.info(f"ãƒãƒƒãƒåˆ†æå®Œäº†: {total_processed}ãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†å®Œäº†")
        
        return total_processed
    
    def _create_pre_tasks(self, batch_configs, execution_id):
        """Pre-taskä½œæˆï¼ˆåˆ†æå®Ÿè¡Œå‰ã«pendingãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆï¼‰"""
        logger.info(f"ğŸ¯ Pre-taskä½œæˆé–‹å§‹: {len(batch_configs)}ã‚¿ã‚¹ã‚¯, execution_id={execution_id}")
        logger.info(f"  ğŸ—ƒï¸ ä½œæˆå…ˆDB: {self.db_path.absolute()}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            created_count = 0
            for config in batch_configs:
                symbol = config['symbol']
                timeframe = config['timeframe']
                
                # configè¾æ›¸ã‹ã‚‰é©åˆ‡ãªã‚­ãƒ¼ã‚’å–å¾—ï¼ˆ_process_chunkã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                if 'strategy' in config:
                    config_name = config['strategy']
                elif 'config' in config:
                    config_name = config['config']
                else:
                    config_name = 'Default'
                
                try:
                    # æ—¢å­˜ã®pendingã‚¿ã‚¹ã‚¯ç¢ºèªï¼ˆå®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯é‡è¤‡ä½œæˆã‚’é˜²ãï¼‰
                    cursor.execute('''
                        SELECT COUNT(*) FROM analyses 
                        WHERE symbol=? AND timeframe=? AND config=? AND execution_id=? 
                        AND task_status IN ('pending', 'running')
                    ''', (symbol, timeframe, config_name, execution_id))
                    
                    if cursor.fetchone()[0] == 0:
                        # Pendingã‚¿ã‚¹ã‚¯ä½œæˆ
                        cursor.execute('''
                            INSERT INTO analyses 
                            (symbol, timeframe, config, task_status, execution_id, status)
                            VALUES (?, ?, ?, 'pending', ?, 'running')
                        ''', (symbol, timeframe, config_name, execution_id))
                        created_count += 1
                
                except Exception as e:
                    logger.error(f"Pre-taskä½œæˆã‚¨ãƒ©ãƒ¼ {symbol} {timeframe} {config_name}: {e}")
            
            conn.commit()
            logger.info(f"âœ… Pre-taskä½œæˆå®Œäº†: {created_count}ã‚¿ã‚¹ã‚¯ä½œæˆ")
            
        return created_count
    
    def _setup_child_process_logging(self):
        """å­ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
        import logging
        import os
        
        # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        logger = logging.getLogger()
        logger.handlers = []
        
        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
        logger.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        formatter = logging.Formatter('%(asctime)s - %(process)d - %(levelname)s - %(message)s')
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # server.logãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        server_log_path = os.path.join(os.path.dirname(__file__), 'web_dashboard', 'server.log')
        if os.path.exists(os.path.dirname(server_log_path)):
            try:
                file_handler = logging.FileHandler(server_log_path, mode='a')
                file_handler.setLevel(logging.INFO)
                file_handler.setFormatter(formatter)
                logger.addHandler(file_handler)
            except Exception as e:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¿½åŠ ã«å¤±æ•—ã—ãŸå ´åˆã¯ç„¡è¦–
                pass
        
        # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ­ã‚¬ãƒ¼ã‚‚å†è¨­å®šï¼ˆProcessPoolExecutorç’°å¢ƒå¼·åŒ–ï¼‰
        for module_name in ['__main__', 'scalable_analysis_system', 'engines.support_resistance_detector', 
                           'engines.support_resistance_adapter', 'engines.high_leverage_bot_orchestrator',
                           'engines.analysis_result']:
            module_logger = logging.getLogger(module_name)
            module_logger.setLevel(logging.INFO)
            
            # å­ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            has_console_handler = any(isinstance(h, logging.StreamHandler) for h in module_logger.handlers)
            if not has_console_handler:
                console_handler_child = logging.StreamHandler()
                console_handler_child.setLevel(logging.INFO)
                console_handler_child.setFormatter(formatter)
                module_logger.addHandler(console_handler_child)
    
    def _process_chunk(self, configs_chunk, chunk_id, execution_id=None):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ã§å®Ÿè¡Œï¼‰"""
        import time
        import random
        import os
        
        # å­ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’è¿½åŠ 
        self._setup_child_process_logging()
        
        # execution_idã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ç”¨ï¼‰
        if execution_id:
            os.environ['CURRENT_EXECUTION_ID'] = execution_id
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: execution_id {execution_id} ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š")
        else:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
            env_execution_id = os.environ.get('CURRENT_EXECUTION_ID')
            if env_execution_id:
                execution_id = env_execution_id
                logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰execution_id {execution_id} ã‚’å–å¾—")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®ç¢ºèªï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹å†…ï¼‰
        debug_mode = os.environ.get('SUPPORT_RESISTANCE_DEBUG', 'false').lower() == 'true'
        if debug_mode:
            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_id}: Support/Resistance ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ (PID: {os.getpid()})")
        
        # ãƒ—ãƒ­ã‚»ã‚¹é–“ã®ç«¶åˆã‚’é˜²ããŸã‚ã€ã‚ãšã‹ãªé…å»¶ã‚’è¿½åŠ 
        # TODO: ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã¯å“è³ªå•é¡Œã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ (2024-06-18)
        # time.sleep(random.uniform(0.1, 0.5))
        # ãƒãƒ£ãƒ³ã‚¯IDãƒ™ãƒ¼ã‚¹ã®æ±ºå®šçš„é…å»¶ã«å¤‰æ›´
        time.sleep(0.1 + (chunk_id % 5) * 0.1)  # 0.1-0.5ç§’ã®æ±ºå®šçš„é…å»¶
        
        processed = 0
        for config in configs_chunk:
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ç¢ºèªï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®å„è¨­å®šã§ç¢ºèªï¼‰
            # ProcessPoolExecutorå†…ã§ã¯ execution_id ã‚’å¼•æ•°ã‹ã‚‰å–å¾—
            check_execution_id = execution_id or getattr(self, 'current_execution_id', None)
            if check_execution_id:
                if self._should_cancel_execution(check_execution_id):
                    logger.info(f"Cancellation detected for {check_execution_id}, stopping chunk {chunk_id}")
                    break
            
            try:
                # è¨­å®šã®å‹ãƒã‚§ãƒƒã‚¯
                if not isinstance(config, dict):
                    logger.error(f"Config is not a dict: {type(config)} - {config}")
                    continue
                
                # configè¾æ›¸ã‹ã‚‰é©åˆ‡ãªã‚­ãƒ¼ã‚’å–å¾—
                if 'strategy' in config:
                    strategy = config['strategy']
                elif 'config' in config:
                    strategy = config['config']
                else:
                    strategy = 'Default'
                
                # æˆ¦ç•¥ã‚­ãƒ¼æ¤œè¨¼å¼·åŒ–
                if not strategy or strategy == 'Default':
                    logger.warning(f"Invalid or missing strategy in config: {config}")
                    continue
                
                # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
                if 'symbol' not in config or 'timeframe' not in config:
                    logger.error(f"Missing required keys in config: {config}")
                    continue
                
                # execution_idã‚’ãƒ­ã‚°å‡ºåŠ›
                logger.info(f"ğŸ” åˆ†æé–‹å§‹: {config['symbol']} {config['timeframe']} {strategy} (execution_id: {execution_id})")
                
                result, metrics = self._generate_single_analysis(
                    config['symbol'], 
                    config['timeframe'], 
                    strategy,
                    execution_id
                )
                if result:
                    processed += 1
                    
                    # é€²æ—ãƒ­ã‚¬ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€æˆ¦ç•¥å®Œäº†ã‚’ãƒ­ã‚°
                    # ğŸ› PickleåŒ–ã‚¨ãƒ©ãƒ¼ä¿®æ­£: ProcessPoolExecutorç’°å¢ƒã§ã¯é€²æ—ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
                    # if hasattr(self, 'progress_logger') and self.progress_logger:
                    #     try:
                    #         self.progress_logger.log_strategy_complete(
                    #             config['timeframe'], 
                    #             strategy,
                    #             metrics or {}
                    #         )
                    #     except Exception as log_error:
                    #         logger.warning(f"Progress logging error: {log_error}")
                    
                    if processed % 10 == 0:
                        logger.info(f"Chunk {chunk_id}: {processed}/{len(configs_chunk)} å®Œäº†")
            except Exception as e:
                logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼ {config}: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
        
        # ğŸ”§ å­ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†å¾Œ: ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°ãƒ­ã‚°ã‚’èª­ã¿å–ã‚Š
        try:
            import glob
            import json
            if execution_id:
                log_pattern = f"/tmp/analysis_log_{execution_id}_*.json"
                log_files = glob.glob(log_pattern)
                for log_file in log_files:
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            analysis_log = json.load(f)
                        
                        # è©³ç´°ãƒ­ã‚°ã‚’è¦ªãƒ—ãƒ­ã‚»ã‚¹ã§è¡¨ç¤º
                        logger.info(f"ğŸ“‹ å­ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ãƒ­ã‚°: {analysis_log['detailed_msg']}")
                        logger.info(f"ğŸ’¡ å­ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ãƒ­ã‚°: {analysis_log['user_msg']}")
                        if analysis_log.get('suggestions'):
                            logger.info(f"ğŸ¯ å­ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ãƒ­ã‚°: æ”¹å–„ææ¡ˆ: {'; '.join(analysis_log['suggestions'])}")
                        
                        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                        os.remove(log_file)
                    except Exception as read_error:
                        logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {read_error}")
        except Exception as cleanup_error:
            logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {cleanup_error}")
        
        return processed
    
    def _update_task_status(self, symbol, timeframe, config, status, error_message=None):
        """task_statusã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°"""
        execution_id = os.environ.get('CURRENT_EXECUTION_ID')
        logger.info(f"ğŸ”„ task_statusæ›´æ–°: {symbol} {timeframe} {config} â†’ {status}")
        logger.info(f"  ğŸ—ƒï¸ æ›´æ–°å…ˆDB: {self.db_path.absolute()}")
        logger.info(f"  ğŸ”‘ execution_id: {execution_id}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            try:
                if status == 'running':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, task_started_at = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, datetime.now(timezone.utc).isoformat(), symbol, timeframe, config, execution_id))
                elif status == 'failed':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, error_message = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, error_message, symbol, timeframe, config, execution_id))
                elif status == 'completed':
                    cursor.execute('''
                        UPDATE analyses 
                        SET task_status = ?, task_completed_at = ?
                        WHERE symbol = ? AND timeframe = ? AND config = ? AND execution_id = ?
                    ''', (status, datetime.now(timezone.utc).isoformat(), symbol, timeframe, config, execution_id))
                
                updated_rows = cursor.rowcount
                conn.commit()
                logger.info(f"âœ… task_statusæ›´æ–°æˆåŠŸ: {updated_rows}è¡Œæ›´æ–°")
                
            except Exception as e:
                logger.error(f"âŒ task_statusæ›´æ–°ã‚¨ãƒ©ãƒ¼: {symbol} {timeframe} {config}")
                logger.error(f"  ğŸ—ƒï¸ DB path: {self.db_path.absolute()}")
                logger.error(f"  ğŸ“ ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
                raise
    
    def _generate_single_analysis(self, symbol, timeframe, config, execution_id=None):
        """å˜ä¸€ã®åˆ†æã‚’ç”Ÿæˆï¼ˆãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆä½¿ç”¨ç‰ˆ + task_statusæ›´æ–°ï¼‰"""
        analysis_id = f"{symbol}_{timeframe}_{config}"
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
        if self._analysis_exists(analysis_id):
            return False, None
        
        # task_statusã‚’'running'ã«æ›´æ–°
        try:
            self._update_task_status(symbol, timeframe, config, 'running')
        except Exception as e:
            logger.warning(f"Failed to update task_status to running: {e}")
        
        # ãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒœãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸåˆ†æã‚’è©¦è¡Œ
        try:
            # execution_idã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"ğŸ¯ ãƒªã‚¢ãƒ«åˆ†æé–‹å§‹: {symbol} {timeframe} {config} (execution_id: {execution_id})")
            trades_data = self._generate_real_analysis(symbol, timeframe, config, execution_id=execution_id)
        except Exception as e:
            logger.error(f"Real analysis failed for {symbol} {timeframe} {config}: {e}")
            logger.error(f"Analysis terminated - no fallback to sample data")
            
            # task_statusã‚’'failed'ã«æ›´æ–°
            try:
                self._update_task_status(symbol, timeframe, config, 'failed', str(e))
            except Exception as update_error:
                logger.warning(f"Failed to update task_status to failed: {update_error}")
            
            return False, None
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self._calculate_metrics(trades_data)
        
        # ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ä¿å­˜
        compressed_path = self._save_compressed_data(analysis_id, trades_data)
        
        # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
        chart_path = None
        if self._should_generate_chart(metrics):
            chart_path = self._generate_lightweight_chart(analysis_id, trades_data, metrics)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆexecution_idä»˜ã + task_statusæ›´æ–°å«ã‚€ï¼‰
        # execution_idã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯å¼•æ•°ã‹ã‚‰å–å¾—ï¼ˆå¼•æ•°ã‚’å„ªå…ˆï¼‰
        final_execution_id = execution_id or os.environ.get('CURRENT_EXECUTION_ID')
        self._save_to_database(symbol, timeframe, config, metrics, chart_path, compressed_path, final_execution_id)
        
        return True, metrics
    
    def _get_exchange_from_config(self, config) -> str:
        """è¨­å®šã‹ã‚‰å–å¼•æ‰€ã‚’å–å¾—"""
        import json
        import os
        
        # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        try:
            if os.path.exists('exchange_config.json'):
                with open('exchange_config.json', 'r') as f:
                    exchange_config = json.load(f)
                    return exchange_config.get('default_exchange', 'hyperliquid').lower()
        except Exception as e:
            logger.warning(f"Failed to load exchange config: {e}")
        
        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿
        env_exchange = os.getenv('EXCHANGE_TYPE', '').lower()
        if env_exchange in ['hyperliquid', 'gateio']:
            return env_exchange
        
        # 3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Hyperliquid
        return 'hyperliquid'
    
    def _load_timeframe_config(self, timeframe):
        """æ™‚é–“è¶³è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            config_path = 'config/timeframe_conditions.json'
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    return config_data.get('timeframe_configs', {}).get(timeframe, {})
        except Exception as e:
            logger.warning(f"æ™‚é–“è¶³è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    def _calculate_period_with_history(self, custom_period_settings, timeframe):
        """ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ã«200æœ¬å‰ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€æœŸé–“ã‚’è¨ˆç®—"""
        try:
            from datetime import datetime, timedelta
            
            start_date = custom_period_settings.get('start_date')
            end_date = custom_period_settings.get('end_date')
            
            if not start_date or not end_date:
                logger.warning("ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šã«é–‹å§‹ãƒ»çµ‚äº†æ—¥æ™‚ãŒã‚ã‚Šã¾ã›ã‚“")
                return 90  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            # æ–‡å­—åˆ—ã‹ã‚‰æ—¥æ™‚ã«å¤‰æ›
            if isinstance(start_date, str):
                start_dt = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
            else:
                start_dt = start_date
                
            if isinstance(end_date, str):
                end_dt = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
            else:
                end_dt = end_date
            
            # æ™‚é–“è¶³ã«å¿œã˜ãŸ200æœ¬å‰ã®æœŸé–“ã‚’è¨ˆç®—
            timeframe_minutes = {
                '1m': 1, '3m': 3, '5m': 5, '15m': 15, '30m': 30, '1h': 60
            }
            
            minutes_per_candle = timeframe_minutes.get(timeframe, 60)
            history_period = timedelta(minutes=200 * minutes_per_candle)
            
            # é–‹å§‹æ—¥æ™‚ã‹ã‚‰200æœ¬å‰ã‚’è¨ˆç®—
            actual_start = start_dt - history_period
            
            # å¿…è¦ãªç·æœŸé–“ã‚’æ—¥æ•°ã§è¨ˆç®—
            total_period = end_dt - actual_start
            period_days = max(total_period.days + 1, 7)  # æœ€ä½7æ—¥
            
            logger.info(f"ğŸ“… æœŸé–“è¨ˆç®—: {start_dt} â†’ {end_dt}, 200æœ¬å‰å«ã‚€: {actual_start} ({period_days}æ—¥)")
            return period_days
            
        except Exception as e:
            logger.error(f"æœŸé–“è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 90  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _generate_real_analysis(self, symbol, timeframe, config, custom_period_days=None, execution_id=None):
        """æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ¬ãƒãƒ¬ãƒƒã‚¸åˆ†æ - å¸‚å ´æ¡ä»¶ã‚’æº€ãŸã—ãŸå ´åˆã®ã¿ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ"""
        # å¤‰æ•°åˆæœŸåŒ–ï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰
        custom_period_settings = None
        
        try:
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šã‚’èª­ã¿å–ã‚Š
            try:
                import os
                if 'CUSTOM_PERIOD_SETTINGS' in os.environ:
                    custom_period_settings = json.loads(os.environ['CUSTOM_PERIOD_SETTINGS'])
                    logger.info(f"ğŸ“… ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æœŸé–“è¨­å®šèª­ã¿å–ã‚Š: {custom_period_settings}")
            except Exception as e:
                logger.warning(f"æœŸé–“è¨­å®šç’°å¢ƒå¤‰æ•°èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                custom_period_settings = None
            
            # æ™‚é–“è¶³è¨­å®šã‹ã‚‰è©•ä¾¡æœŸé–“ã‚’å‹•çš„ã«å–å¾—
            tf_config = self._load_timeframe_config(timeframe)
            
            # æœŸé–“è¨­å®šã®å„ªå…ˆé †ä½: custom_period_days > custom_period_settings > è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
            if custom_period_days is not None:
                evaluation_period_days = custom_period_days
                logger.info(f"ğŸ“… ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡æœŸé–“: {evaluation_period_days}æ—¥")
            elif custom_period_settings and custom_period_settings.get('mode') == 'custom':
                # ã‚«ã‚¹ã‚¿ãƒ æœŸé–“ã®å ´åˆã¯200æœ¬å‰ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€æœŸé–“ã‚’è¨ˆç®—
                evaluation_period_days = self._calculate_period_with_history(custom_period_settings, timeframe)
                logger.info(f"ğŸ“… ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“+200æœ¬: {evaluation_period_days}æ—¥ ({timeframe}è¶³)")
            else:
                evaluation_period_days = tf_config.get('data_days', 90)  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
                logger.info(f"ğŸ“… æ™‚é–“è¶³åˆ¥è©•ä¾¡æœŸé–“: {evaluation_period_days}æ—¥ ({timeframe}è¶³è¨­å®š)")
            
            # æœ¬æ ¼çš„ãªæˆ¦ç•¥åˆ†æã®ãŸã‚ã€å®Ÿéš›ã®APIãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            from engines.high_leverage_bot_orchestrator import HighLeverageBotOrchestrator
            
            # å–å¼•æ‰€è¨­å®šã‚’å–å¾—
            exchange = self._get_exchange_from_config(config)
            
            logger.info(f"ğŸ¯ å®Ÿãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æˆ¦ç•¥åˆ†æã‚’é–‹å§‹: {symbol} {timeframe} {config} ({exchange})")
            logger.info("   â³ ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨MLåˆ†æã®ãŸã‚ã€å‡¦ç†ã«æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
            
            # ä¿®æ­£: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤å•é¡Œè§£æ±ºã®ãŸã‚ã€æ¯å›æ–°ã—ã„ãƒœãƒƒãƒˆã‚’ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ï¼‰
            # ç†ç”±: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®å†åˆ©ç”¨ã«ã‚ˆã‚Šã€å…¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã§åŒã˜ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãŒä½¿ç”¨ã•ã‚Œã‚‹å•é¡Œã‚’è§£æ±º
            bot = HighLeverageBotOrchestrator(use_default_plugins=True, exchange=exchange)
            logger.info(f"ğŸ”„ {symbol} æ–°è¦ãƒœãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (ä¾¡æ ¼å¤šæ§˜æ€§ç¢ºä¿ã®ãŸã‚)")
            
            # è¤‡æ•°å›åˆ†æã‚’å®Ÿè¡Œã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Œå…¨ãƒ­ã‚°æŠ‘åˆ¶ï¼‰
            trades = []
            import sys
            import os
            import contextlib
            import time
            
            # é€²æ—è¡¨ç¤ºç”¨
            logger.info(f"ğŸ”„ {symbol} {timeframe} {config}: æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æé–‹å§‹")
            
            # å®Œå…¨ã«ãƒ­ã‚°ã‚’æŠ‘åˆ¶ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ç„¡åŠ¹ï¼‰
            @contextlib.contextmanager
            def suppress_all_output():
                debug_mode = os.environ.get('SUPPORT_RESISTANCE_DEBUG', 'false').lower() == 'true'
                if debug_mode:
                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å‡ºåŠ›ã‚’æŠ‘åˆ¶ã—ãªã„
                    yield
                else:
                    with open(os.devnull, 'w') as devnull:
                        old_stdout = sys.stdout
                        old_stderr = sys.stderr
                        try:
                            sys.stdout = devnull
                            sys.stderr = devnull
                            yield
                        finally:
                            sys.stdout = old_stdout
                            sys.stderr = old_stderr
            
            # æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®ã‚·ã‚°ãƒŠãƒ«ç”ŸæˆæœŸé–“è¨­å®š
            if custom_period_settings and custom_period_settings.get('mode') == 'custom':
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“ã‚’ä½¿ç”¨
                start_time = datetime.fromisoformat(custom_period_settings['start_date'].replace('T', ' ')).replace(tzinfo=timezone.utc)
                end_time = datetime.fromisoformat(custom_period_settings['end_date'].replace('T', ' ')).replace(tzinfo=timezone.utc)
                logger.info(f"ğŸ“… ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“: {start_time.strftime('%Y-%m-%d %H:%M')} ï½ {end_time.strftime('%Y-%m-%d %H:%M')}")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“
                end_time = datetime.now(timezone.utc)
                start_time = end_time - timedelta(days=evaluation_period_days)
                logger.info(f"ğŸ“… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“: {start_time.strftime('%Y-%m-%d %H:%M')} ï½ {end_time.strftime('%Y-%m-%d %H:%M')}")
            
            # æ™‚é–“è¶³è¨­å®šã‹ã‚‰è©•ä¾¡é–“éš”ã‚’å–å¾—
            tf_config = self._load_timeframe_config(timeframe)
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©•ä¾¡é–“éš”ã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆ†å˜ä½ï¼‰
            evaluation_interval_minutes = tf_config.get('evaluation_interval_minutes')
            
            if evaluation_interval_minutes:
                evaluation_interval = timedelta(minutes=evaluation_interval_minutes)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–“éš”
                default_intervals = {
                    '1m': timedelta(minutes=5),   '3m': timedelta(minutes=15),
                    '5m': timedelta(minutes=30),  '15m': timedelta(hours=1),
                    '30m': timedelta(hours=2),    '1h': timedelta(hours=4),
                    '4h': timedelta(hours=12),    '1d': timedelta(days=1)
                }
                evaluation_interval = default_intervals.get(timeframe, timedelta(hours=4))
            
            # åˆæœŸåŒ–: effective_start_timeã‚’start_timeã«è¨­å®šï¼ˆå¾Œã§ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦èª¿æ•´ï¼‰
            effective_start_time = start_time
            
            # æ¡ä»¶ãƒ™ãƒ¼ã‚¹ã®åˆ†æå®Ÿè¡Œ
            current_time = effective_start_time
            total_evaluations = 0
            signals_generated = 0
            
            # é‡è¦å¤‰æ•°ã®åˆæœŸåŒ–ï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰
            result = {}
            ohlcv_df = None
            bot = None
            
            # ğŸ”§ OHLCVãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰å–å¾—ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ï¼‰
            try:
                # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
                from hyperliquid_api_client import MultiExchangeAPIClient
                api_client = MultiExchangeAPIClient(exchange_type=exchange)
                
                # OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè©•ä¾¡æœŸé–“ + æ”¯æŒç·šãƒ»æŠµæŠ—ç·šç”¨ã®å‰ãƒ‡ãƒ¼ã‚¿ï¼‰
                logger.info(f"ğŸ“Š {symbol} {timeframe} ã®OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                
                # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šè¨ˆç®—ç”¨ã«200æœ¬å‰ã‹ã‚‰å–å¾—
                lookback_days = 10  # 200æœ¬åˆ†ã®æ—¥æ•°ï¼ˆæ™‚é–“è¶³ã«ã‚ˆã‚Šèª¿æ•´ï¼‰
                data_start_time = start_time - timedelta(days=lookback_days)
                
                ohlcv_df = api_client.get_ohlcv_dataframe(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_time=data_start_time,
                    end_time=end_time
                )
                
                if ohlcv_df is not None and not ohlcv_df.empty:
                    logger.info(f"âœ… OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(ohlcv_df)}æœ¬")
                    
                    # RealPreparedDataã®ä½œæˆ
                    from engines.data_preparers import RealPreparedData
                    real_prepared_data = RealPreparedData(ohlcv_df)
                    
                    # FilteringFrameworkã®åˆæœŸåŒ–ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
                    self.filtering_framework = FilteringFramework(
                        prepared_data_factory=lambda: real_prepared_data,
                        strategy_factory=lambda: self._create_strategy_from_config(config)
                    )
                else:
                    logger.warning(f"âš ï¸ OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
                    ohlcv_df = None
            except Exception as e:
                logger.warning(f"âš ï¸ OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}ã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
                ohlcv_df = None
            
            # OHLCVãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡è¨­å®š
            if ohlcv_df is not None and not ohlcv_df.empty:
                # è©•ä¾¡å¯¾è±¡æœŸé–“ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®š
                evaluation_start_index = 0
                for i, row in ohlcv_df.iterrows():
                    if pd.to_datetime(row['timestamp']).replace(tzinfo=timezone.utc) >= start_time:
                        evaluation_start_index = i
                        break
                
                # å…¨OHLCVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©•ä¾¡å¯¾è±¡éƒ¨åˆ†ã®ã¿ã‚’è©•ä¾¡
                total_evaluations_planned = len(ohlcv_df) - evaluation_start_index
                logger.info(f"ğŸ” æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æ: {start_time.strftime('%Y-%m-%d %H:%M')} ã‹ã‚‰ {end_time.strftime('%Y-%m-%d %H:%M')}")
                logger.info(f"ğŸ“Š è©•ä¾¡å¯¾è±¡: {evaluation_start_index}æœ¬ç›®ï½{len(ohlcv_df)}æœ¬ç›® (è¨ˆ{total_evaluations_planned}æœ¬ã®{timeframe}è¶³)")
                print(f"ğŸ’¯ å…¨ãƒ‡ãƒ¼ã‚¿è©•ä¾¡: é–“å¼•ããªã—ã€åˆ¶é™ãªã—")
            else:
                logger.warning("âš ï¸ OHLCVãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return []
            
            # å…¨OHLCVãƒ‡ãƒ¼ã‚¿ã‚’é †æ¬¡è©•ä¾¡ï¼ˆåˆ¶é™ãªã—ï¼‰
            for current_index in range(evaluation_start_index, len(ohlcv_df)):
                current_row = ohlcv_df.iloc[current_index]
                current_time = pd.to_datetime(current_row['timestamp']).replace(tzinfo=timezone.utc)
                total_evaluations += 1
                
                # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
                # ç†ç”±: é‡è¤‡å‡¦ç†ã¨æ€§èƒ½åŠ£åŒ–å•é¡Œ - Stage 8ã§ååˆ†ãªåˆ†æå®Ÿè¡Œ
                
                try:
                    # å‡ºåŠ›æŠ‘åˆ¶ã§å¸‚å ´æ¡ä»¶ã®è©•ä¾¡ï¼ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ•ãƒ©ã‚°ä»˜ãï¼‰
                    with suppress_all_output():
                        # execution_idã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆåˆå›ã®ã¿ï¼‰
                        if total_evaluations == 1:
                            logger.info(f"ğŸ” ãƒœãƒƒãƒˆåˆ†æé–‹å§‹: execution_id={execution_id}")
                        
                        result = bot.analyze_symbol(symbol, timeframe, config, is_backtest=True, target_timestamp=current_time, custom_period_settings=custom_period_settings, execution_id=execution_id)
                    
                    # ğŸ” ProcessPoolExecutorç’°å¢ƒè¨ºæ–­: çµæœã®å‹ãƒ»å†…å®¹è©³ç´°èª¿æŸ»
                    logger.info(f"ğŸ” å­ãƒ—ãƒ­ã‚»ã‚¹çµæœè¨ºæ–­: {symbol} {timeframe} {config}")
                    logger.info(f"   çµæœã®å‹: {type(result)}")
                    if hasattr(result, 'early_exit'):
                        logger.info(f"   AnalysisResult detected - early_exit: {result.early_exit}")
                        if result.early_exit:
                            logger.info(f"   exit_stage: {result.exit_stage}")
                            logger.info(f"   exit_reason: {result.exit_reason}")
                    else:
                        logger.info(f"   çµæœå†…å®¹: {result}")
                        if isinstance(result, dict):
                            logger.info(f"   è¾æ›¸ã‚­ãƒ¼: {list(result.keys()) if result else 'None/Empty'}")
                    
                    # ğŸ¯ DISCORDé€šçŸ¥å‡¦ç†ï¼ˆProcessPoolExecutorå°‚ç”¨ãƒ»ç¢ºå®Ÿå®Ÿè¡Œç‰ˆï¼‰
                    self._handle_discord_notification_for_result(result, symbol, timeframe, config, execution_id)
                    
                    # ğŸ” AnalysisResultå¯¾å¿œ: Early Exitã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›ï¼ˆProcessPoolExecutorå¯¾å¿œå¼·åŒ–ç‰ˆï¼‰
                    from engines.analysis_result import AnalysisResult
                    import sys
                    if isinstance(result, AnalysisResult):
                        # å¿…ãšEarly Exitæ¤œå‡ºãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆé€šçŸ¥å‰ç¢ºèªç”¨ï¼‰
                        logger.info(f"âš¡ AnalysisResultå‡¦ç†é–‹å§‹: early_exit={result.early_exit}")
                        
                        if result.early_exit:
                            # ProcessPoolExecutorç’°å¢ƒã§ã®ç¢ºå®Ÿãªãƒ­ã‚°å‡ºåŠ›
                            detailed_msg = result.get_detailed_log_message()
                            user_msg = result.get_user_friendly_message()
                            
                            # å¼·åˆ¶çš„ãªãƒ­ã‚°å‡ºåŠ›ã¨ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼ˆProcessPoolExecutorå¯¾å¿œï¼‰
                            logger.info(f"ğŸ“‹ {detailed_msg}")
                            logger.info(f"ğŸ’¡ {user_msg}")
                            
                            # æ”¹å–„ææ¡ˆã‚‚å‡ºåŠ›
                            suggestions = result.get_suggestions()
                            if suggestions:
                                logger.info(f"ğŸ¯ æ”¹å–„ææ¡ˆ: {'; '.join(suggestions)}")
                            
                            # ProcessPoolExecutorç’°å¢ƒã§ã®ç¢ºå®Ÿãªå‡ºåŠ›ç¢ºä¿
                            sys.stdout.flush()
                            sys.stderr.flush()
                            
                            # ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®å¼·åˆ¶ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                            for handler in logger.handlers:
                                if hasattr(handler, 'flush'):
                                    handler.flush()
                            
                            # ğŸ¯ Discord webhooké€šçŸ¥: å­ãƒ—ãƒ­ã‚»ã‚¹ã®Early Exitè©³ç´°é€ä¿¡
                            try:
                                # åŒæœŸå®Ÿè¡Œï¼ˆProcessPoolExecutorç’°å¢ƒã§ã¯éåŒæœŸã¯ä½¿ç”¨ã§ããªã„ï¼‰
                                import requests
                                
                                # ProcessPoolExecutorç’°å¢ƒã§ã®ç’°å¢ƒå¤‰æ•°å†èª­ã¿è¾¼ã¿
                                try:
                                    from dotenv import load_dotenv
                                    load_dotenv()
                                except ImportError:
                                    pass
                                
                                # Discord webhook URL (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—)
                                webhook_url = os.environ.get('DISCORD_WEBHOOK_URL')
                                
                                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: Discordé€šçŸ¥è©¦è¡Œãƒ­ã‚°
                                logger.info(f"ğŸ¯ Discordé€šçŸ¥è©¦è¡Œ: {symbol} {timeframe} {config}")
                                logger.info(f"   webhook_urlè¨­å®š: {bool(webhook_url)}")
                                logger.info(f"   Early Exitè©³ç´°: {result.exit_stage}/{result.exit_reason}")
                                
                                if webhook_url:
                                    # embedä½œæˆ
                                    embed = {
                                        "title": f"ğŸš¨ Early Exit Analysis: {symbol}",
                                        "color": 0xFF4444,  # èµ¤è‰²
                                        "timestamp": datetime.now().isoformat(),
                                        "fields": [
                                            {"name": "Symbol", "value": symbol, "inline": True},
                                            {"name": "Timeframe", "value": timeframe, "inline": True},
                                            {"name": "Strategy", "value": config, "inline": True},
                                            {"name": "Exit Stage", "value": result.exit_stage.value if result.exit_stage else 'unknown', "inline": True},
                                            {"name": "Exit Reason", "value": result.exit_reason.value if result.exit_reason else 'unknown', "inline": True},
                                            {"name": "Execution ID", "value": f"`{execution_id}`", "inline": False},
                                            {"name": "Detailed Message", "value": detailed_msg[:1000], "inline": False},
                                            {"name": "User Message", "value": user_msg[:1000], "inline": False}
                                        ],
                                        "footer": {"text": "Long Trader - Early Exit Analysis"}
                                    }
                                    
                                    # æ”¹å–„ææ¡ˆã‚’è¿½åŠ 
                                    if suggestions:
                                        embed["fields"].append({
                                            "name": "ğŸ’¡ Suggestions",
                                            "value": "\n".join([f"â€¢ {s}" for s in suggestions[:5]])[:1000],
                                            "inline": False
                                        })
                                    
                                    # Discord APIã«é€ä¿¡
                                    payload = {
                                        "embeds": [embed],
                                        "username": "Long Trader Bot"
                                    }
                                    
                                    # æœ€å¤§3å›ã®ãƒªãƒˆãƒ©ã‚¤ï¼ˆProcessPoolExecutorç’°å¢ƒã§ã¯è»½é‡åŒ–ï¼‰
                                    for attempt in range(3):
                                        try:
                                            response = requests.post(webhook_url, json=payload, timeout=10)
                                            if response.status_code == 200:
                                                logger.info(f"âœ… Discordé€šçŸ¥é€ä¿¡æˆåŠŸ: {symbol} Early Exit")
                                                break
                                            elif response.status_code == 429:  # Rate limit
                                                retry_after = int(response.headers.get('Retry-After', 1))
                                                logger.warning(f"Discord rate limit, retrying after {retry_after}s")
                                                time.sleep(retry_after)
                                            else:
                                                logger.warning(f"Discord API error: {response.status_code}")
                                                break
                                        except Exception as e:
                                            if attempt == 2:  # æœ€å¾Œã®è©¦è¡Œ
                                                logger.error(f"âŒ Discordé€ä¿¡å¤±æ•—: {e}")
                                                break
                                            wait_time = 2 ** attempt
                                            logger.warning(f"Discordé€ä¿¡å¤±æ•— (attempt {attempt + 1}/3): {e}, retrying in {wait_time}s")
                                            time.sleep(wait_time)
                                else:
                                    logger.warning("âš ï¸ DISCORD_WEBHOOK_URL not set, skipping notification")
                                    
                            except Exception as discord_error:
                                logger.error(f"âŒ Discordé€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {discord_error}")
                                import traceback
                                logger.error(f"   ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
                            
                            # ğŸ”§ ProcessPoolExecutorç’°å¢ƒç”¨: AnalysisResultè©³ç´°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
                            try:
                                import tempfile
                                import json
                                analysis_log = {
                                    'timestamp': datetime.now().isoformat(),
                                    'execution_id': execution_id,
                                    'symbol': symbol,
                                    'timeframe': timeframe,
                                    'strategy': config,
                                    'detailed_msg': detailed_msg,
                                    'user_msg': user_msg,
                                    'suggestions': suggestions,
                                    'early_exit': True,
                                    'stage': result.exit_stage.value if result.exit_stage else 'unknown',
                                    'reason': result.exit_reason.value if result.exit_reason else 'unknown'
                                }
                                
                                log_file = f"/tmp/analysis_log_{execution_id}_{symbol}_{timeframe}_{config}.json"
                                with open(log_file, 'w', encoding='utf-8') as f:
                                    json.dump(analysis_log, f, ensure_ascii=False, indent=2)
                                
                                # è¦ªãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªç”¨
                                print(f"ğŸ“ å­ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°ãƒ­ã‚°å‡ºåŠ›: {log_file}", flush=True)
                            except Exception as log_error:
                                logger.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {log_error}")
                            
                            continue
                        elif result.completed and result.recommendation:
                            # æˆåŠŸæ™‚ã®ãƒ­ã‚°ã‚‚å‡ºåŠ›
                            success_msg = result.get_detailed_log_message()
                            logger.info(f"âœ… {success_msg}")
                            
                            # ProcessPoolExecutorç’°å¢ƒã§ã®ç¢ºå®Ÿãªå‡ºåŠ›ç¢ºä¿
                            sys.stdout.flush()
                            sys.stderr.flush()
                            for handler in logger.handlers:
                                if hasattr(handler, 'flush'):
                                    handler.flush()
                            
                            # AnalysisResultã‹ã‚‰è¾æ›¸å½¢å¼ã«å¤‰æ›ã—ã¦ãã®ã¾ã¾ä½¿ç”¨
                            result = result.recommendation
                        else:
                            # ä¸å®Œå…¨ãªçµæœã¯ã‚¹ã‚­ãƒƒãƒ—
                            logger.warning(f"âš ï¸ ä¸å®Œå…¨ãªAnalysisResult: {symbol} {timeframe}")
                            continue
                    
                    # ğŸ” analyze_symbolã®çµæœã‚’è©³ç´°ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if total_evaluations <= 3:  # æœ€åˆã®3å›ã®ã¿è©³ç´°ãƒ­ã‚°
                        logger.error(f"ğŸ” analyze_symbolçµæœè©³ç´° #{total_evaluations} ({symbol} {timeframe}):")
                        if result:
                            if isinstance(result, dict):
                                for key, value in result.items():
                                    logger.error(f"   {key}: {value} (å‹: {type(value)})")
                            else:
                                logger.error(f"   çµæœ: {result} (å‹: {type(result)})")
                        else:
                            logger.error(f"   çµæœ: None ã¾ãŸã¯ç©º")
                    
                    # Early Exitå¯¾å¿œ: Noneã¾ãŸã¯ç„¡åŠ¹ãªçµæœã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not result:
                        if total_evaluations <= 3:
                            logger.info(f"â­ï¸ Early Exit #{total_evaluations}: ã‚µãƒãƒ¬ã‚¸ãƒ¬ãƒ™ãƒ«ä¸è¶³ã«ã‚ˆã‚Šè©•ä¾¡æ™‚ç‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        continue
                    
                    if 'current_price' not in result:
                        if total_evaluations <= 3:
                            logger.error(f"ğŸš¨ analyze_symbolçµæœãŒç„¡åŠ¹ #{total_evaluations}: current_price missing")
                        continue
                    
                    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã®è©•ä¾¡
                    try:
                        should_enter = self._evaluate_entry_conditions(result, timeframe)
                    except Exception as e:
                        logger.error(f"ğŸš¨ ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ #{total_evaluations}:")
                        logger.error(f"   ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        logger.error(f"   åˆ†æçµæœ: {result}")
                        continue
                    
                    if not should_enter:
                        # æ¡ä»¶ã‚’æº€ãŸã•ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
                        if symbol == 'OP' and total_evaluations <= 5:  # æœ€åˆã®5å›ã®ã¿ãƒ­ã‚°
                            logger.error(f"ğŸš¨ OPæ¡ä»¶ä¸æº€è¶³ #{total_evaluations}: leverage={result.get('leverage')}, confidence={result.get('confidence')}, RR={result.get('risk_reward_ratio')}")
                        continue
                    
                    signals_generated += 1
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆæ¡ä»¶æº€è¶³æ™‚ï¼‰
                    if signals_generated % 5 == 0:
                        progress_pct = ((current_time - start_time).total_seconds() / 
                                      (end_time - start_time).total_seconds()) * 100
                        logger.info(f"ğŸ¯ {symbol} {timeframe}: ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ {signals_generated}ä»¶ (é€²æ—: {progress_pct:.1f}%)")
                    
                    # ãƒ¬ãƒãƒ¬ãƒƒã‚¸ã¨TP/SLä¾¡æ ¼ã‚’è¨ˆç®—
                    leverage = result.get('leverage', 5.0)
                    confidence = result.get('confidence', 70.0) / 100.0
                    risk_reward = result.get('risk_reward_ratio', 2.0)
                    current_price = result.get('current_price')
                    if current_price is None:
                        logger.error(f"No current_price in analysis result for {symbol}")
                        raise Exception(f"Missing current_price in analysis result for {symbol}")
                    
                    # TP/SLè¨ˆç®—æ©Ÿèƒ½ã‚’ä½¿ç”¨
                    from engines.stop_loss_take_profit_calculators import (
                        DefaultSLTPCalculator, ConservativeSLTPCalculator, AggressiveSLTPCalculator,
                        TraditionalSLTPCalculator, MLSLTPCalculator
                    )
                    from interfaces.data_types import MarketContext
                    
                    # æ¡ä»¶æº€è¶³æ™‚ã®å®Ÿéš›ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                    trade_time = current_time
                    
                    # æˆ¦ç•¥ã«å¿œã˜ãŸTP/SLè¨ˆç®—å™¨ã‚’é¸æŠ
                    if 'Conservative' in config:
                        sltp_calculator = ConservativeSLTPCalculator()
                    elif 'Aggressive_Traditional' in config:
                        sltp_calculator = TraditionalSLTPCalculator()
                    elif 'Aggressive' in config:
                        sltp_calculator = AggressiveSLTPCalculator()
                    elif 'Full_ML' in config:
                        sltp_calculator = MLSLTPCalculator()
                    else:
                        sltp_calculator = DefaultSLTPCalculator()
                    
                    # æ¨¡æ“¬çš„ãªå¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    market_context = MarketContext(
                        current_price=current_price,
                        volume_24h=1000000.0,
                        volatility=0.03,
                        trend_direction='BULLISH',
                        market_phase='MARKUP',
                        timestamp=trade_time
                    )
                    
                    # å®Ÿéš›ã®æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºï¼ˆæŸ”è»Ÿãªã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ç‰ˆï¼‰
                    try:
                        from engines.support_resistance_adapter import FlexibleSupportResistanceDetector
                        
                        # OHLCVãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸçš„ã«å–å¾—ï¼ˆãƒœãƒƒãƒˆå†…ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                        try:
                            # ãƒœãƒƒãƒˆãŒæ—¢ã«å–å¾—ã—ãŸOHLCVãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                                ohlcv_data = bot._cached_data
                            else:
                                # ãƒœãƒƒãƒˆã®fetch_market_dataãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼ˆã‚«ã‚¹ã‚¿ãƒ æœŸé–“è¨­å®šã‚’æ¸¡ã™ï¼‰
                                ohlcv_data = bot._fetch_market_data(symbol, timeframe, custom_period_settings)
                            
                            if ohlcv_data.empty:
                                raise Exception("OHLCVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                                
                        except Exception as ohlcv_error:
                            # OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                            raise Exception(f"OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {str(ohlcv_error)}")
                        
                        if len(ohlcv_data) < 50:
                            raise Exception(f"æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚{len(ohlcv_data)}æœ¬ï¼ˆæœ€ä½50æœ¬å¿…è¦ï¼‰")
                        
                        # å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€åˆã®æœ‰åŠ¹ãªè©•ä¾¡æ™‚åˆ»ã‚’æ±ºå®šï¼ˆåˆå›ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã®ã¿ï¼‰
                        if total_evaluations == 0:
                            data_start_time = pd.to_datetime(ohlcv_data.index[0], utc=True) if hasattr(ohlcv_data.index[0], 'tz_localize') else pd.to_datetime(ohlcv_data.index[0]).tz_localize('UTC')
                            
                            # è©•ä¾¡é–“éš”ã®å¢ƒç•Œã«åˆã†æœ€åˆã®æ™‚åˆ»ã‚’è¦‹ã¤ã‘ã‚‹
                            effective_start_time = self._find_first_valid_evaluation_time(data_start_time, evaluation_interval)
                            
                            if effective_start_time > start_time:
                                logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿åˆ¶ç´„ã«ã‚ˆã‚Šåˆ†æé–‹å§‹æ™‚åˆ»ã‚’èª¿æ•´: {start_time.strftime('%Y-%m-%d %H:%M')} â†’ {effective_start_time.strftime('%Y-%m-%d %H:%M')}")
                                current_time = effective_start_time  # current_timeã‚‚æ›´æ–°
                        
                        # æŸ”è»Ÿãªæ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
                        detector = FlexibleSupportResistanceDetector(
                            min_touches=2, 
                            tolerance_pct=0.01,
                            use_ml_enhancement=True
                        )
                        
                        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¡¨ç¤º
                        provider_info = detector.get_provider_info()
                        logger.info(f"       æ¤œå‡ºãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider_info['base_provider']}")
                        logger.info(f"       MLå¼·åŒ–: {provider_info['ml_provider']}")
                        
                        # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šã‚’æ¤œå‡ºï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã¯å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
                        logger.info(f"       ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ¼ãƒ‰: å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ {len(ohlcv_data)}æœ¬")
                        logger.info(f"       ğŸ” æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºé–‹å§‹ (è©•ä¾¡{total_evaluations}å›ç›®, æ™‚åˆ»: {current_time.strftime('%Y-%m-%d %H:%M')})")
                        support_levels, resistance_levels = detector.detect_levels(ohlcv_data, current_price)
                        
                        # æ¤œå‡ºçµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆProcessPoolExecutorå¯¾å¿œï¼‰
                        if support_levels or resistance_levels:
                            logger.info(f"   âœ… æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºæˆåŠŸ: æ”¯æŒç·š{len(support_levels)}å€‹, æŠµæŠ—ç·š{len(resistance_levels)}å€‹")
                            if support_levels:
                                for i, s in enumerate(support_levels[:3], 1):
                                    logger.info(f"      æ”¯æŒç·š{i}: ${s.price:.2f} (å¼·åº¦: {s.strength:.2f})")
                            if resistance_levels:
                                for i, r in enumerate(resistance_levels[:3], 1):
                                    logger.info(f"      æŠµæŠ—ç·š{i}: ${r.price:.2f} (å¼·åº¦: {r.strength:.2f})")
                        else:
                            logger.warning(f"   âš ï¸  æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                        
                        # ä¸Šä½ãƒ¬ãƒ™ãƒ«ã®ã¿é¸æŠï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
                        max_levels = 3
                        support_levels = support_levels[:max_levels]
                        resistance_levels = resistance_levels[:max_levels]
                        
                        if not support_levels and not resistance_levels:
                            # è©³ç´°ãƒ­ã‚°ã‚’è¿½åŠ 
                            data_stats = {
                                "data_points": len(ohlcv_data),
                                "price_range": f"{ohlcv_data['low'].min():.4f} - {ohlcv_data['high'].max():.4f}",
                                "current_price": current_price,
                                "volatility": ((ohlcv_data['high'] - ohlcv_data['low']) / ohlcv_data['close']).mean(),
                                "timeframe": timeframe
                            }
                            logger.error(f"ğŸ” {symbol} {timeframe} æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºå¤±æ•—è©³ç´°:")
                            logger.error(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ: {data_stats}")
                            logger.error(f"  âš™ï¸ æ¤œå‡ºè¨­å®š: min_touches={provider_info.get('min_touches', 'N/A')}, tolerance={provider_info.get('tolerance_pct', 'N/A')}")
                            logger.error(f"  ğŸ¤– MLä½¿ç”¨: {provider_info['ml_provider']}")
                            
                            raise Exception(f"æœ‰åŠ¹ãªæ”¯æŒç·šãƒ»æŠµæŠ—ç·šãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        
                        logger.info(f"   âœ… æ”¯æŒç·šãƒ»æŠµæŠ—ç·šæ¤œå‡ºæˆåŠŸ: æ”¯æŒç·š{len(support_levels)}å€‹, æŠµæŠ—ç·š{len(resistance_levels)}å€‹")
                        
                        # MLäºˆæ¸¬ã‚¹ã‚³ã‚¢æƒ…å ±ã‚‚è¡¨ç¤º
                        if provider_info['ml_provider'] != "Disabled":
                            if support_levels:
                                avg_ml_score = np.mean([getattr(s, 'ml_bounce_probability', 0) for s in support_levels])
                                logger.info(f"       æ”¯æŒç·šMLåç™ºäºˆæ¸¬: å¹³å‡{avg_ml_score:.2f}")
                            if resistance_levels:
                                avg_ml_score = np.mean([getattr(r, 'ml_bounce_probability', 0) for r in resistance_levels])
                                logger.info(f"       æŠµæŠ—ç·šMLåç™ºäºˆæ¸¬: å¹³å‡{avg_ml_score:.2f}")
                        else:
                            logger.info(f"       MLäºˆæ¸¬: ç„¡åŠ¹åŒ–")
                        
                        # TP/SLä¾¡æ ¼ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—
                        sltp_levels = sltp_calculator.calculate_levels(
                            current_price=current_price,
                            leverage=leverage,
                            support_levels=support_levels,
                            resistance_levels=resistance_levels,
                            market_context=market_context
                        )
                        
                    except Exception as e:
                        # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆã¯ã€ã“ã®è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã«é€²ã‚€
                        error_msg = f"æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºãƒ»åˆ†æã«å¤±æ•—: {str(e)}"
                        logger.warning(f"âš ï¸ {symbol} {timeframe} {config}: {error_msg} (è©•ä¾¡{total_evaluations}ã‚’ã‚¹ã‚­ãƒƒãƒ—)")
                        logger.info(f"   ğŸ“… ã‚¹ã‚­ãƒƒãƒ—ã—ãŸæ™‚åˆ»: {current_time.strftime('%Y-%m-%d %H:%M')} â†’ æ¬¡ã®è©•ä¾¡ã«ç¶™ç¶š")
                        logger.warning(f"Support/resistance analysis failed for {symbol} at {current_time}: {error_msg}")
                        # æ¬¡ã®è©•ä¾¡æ™‚ç‚¹ã«é€²ã‚€ï¼ˆcontinueå…ˆã§evaluation_intervalãŒåŠ ç®—ã•ã‚Œã‚‹ï¼‰
                        continue
                    
                    # ğŸ”§ é‡è¦ãªä¿®æ­£: å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ãƒˆãƒ¬ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ã‚’å–å¾—
                    # ç†ç”±: current_priceãŒå›ºå®šå€¤ã®ãŸã‚ã€å®Ÿéš›ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    entry_price = self._get_real_market_price(bot, symbol, timeframe, trade_time)
                    
                    # SL/TPä¾¡æ ¼ã‚’ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã§å†è¨ˆç®—
                    sltp_levels = sltp_calculator.calculate_levels(
                        current_price=entry_price,  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—
                        leverage=leverage,
                        support_levels=support_levels,
                        resistance_levels=resistance_levels,
                        market_context=market_context
                    )
                    
                    # å®Ÿéš›ã®TP/SLä¾¡æ ¼ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ï¼‰
                    tp_price = sltp_levels.take_profit_price
                    sl_price = sltp_levels.stop_loss_price
                    
                    # ğŸ”§ ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã®ä¾¡æ ¼è«–ç†ãƒã‚§ãƒƒã‚¯
                    if sl_price >= entry_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: æåˆ‡ã‚Šä¾¡æ ¼({sl_price:.4f})ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼({entry_price:.4f})ä»¥ä¸Š")
                        continue
                    if tp_price <= entry_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: åˆ©ç¢ºä¾¡æ ¼({tp_price:.4f})ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼({entry_price:.4f})ä»¥ä¸‹")
                        continue
                    if sl_price >= tp_price:
                        logger.error(f"é‡å¤§ã‚¨ãƒ©ãƒ¼: æåˆ‡ã‚Šä¾¡æ ¼({sl_price:.4f})ãŒåˆ©ç¢ºä¾¡æ ¼({tp_price:.4f})ä»¥ä¸Š")
                        continue
                    
                    # æˆ¦ç•¥åˆ†æã§ã¯ã€current_priceã‚‚entry_priceã¨åŒã˜ã«ã—ã¦æ•´åˆæ€§ã‚’ä¿ã¤
                    # ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®open vs closeã«ã‚ˆã‚‹ä¾¡æ ¼å·®ã‚’é˜²ã
                    current_price = entry_price
                    
                    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
                    price_consistency_result = self.price_validator.validate_price_consistency(
                        analysis_price=current_price,
                        entry_price=entry_price,
                        symbol=symbol,
                        context=f"{timeframe}_{config}_trade_{len(trades)+1}"
                    )
                    
                    if not price_consistency_result.is_consistent:
                        logger.warning(f"ä¾¡æ ¼æ•´åˆæ€§å•é¡Œæ¤œå‡º: {symbol} {timeframe} - {price_consistency_result.message}")
                        for recommendation in price_consistency_result.recommendations:
                            logger.warning(f"æ¨å¥¨å¯¾å¿œ: {recommendation}")
                        
                        # é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆã®å ´åˆã¯å–å¼•ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if price_consistency_result.inconsistency_level.value == 'critical':
                            logger.error(f"é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆã®ãŸã‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: {symbol} at {trade_time}")
                            continue
                    
                    # çµ±ä¸€ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                    unified_price_data = self.price_validator.create_unified_price_data(
                        analysis_price=current_price,
                        entry_price=entry_price,
                        symbol=symbol,
                        timeframe=timeframe,
                        market_timestamp=trade_time,
                        data_source=exchange
                    )
                    
                    # TP/SLåˆ°é”ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ã‚ºåˆ¤å®šï¼ˆå®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                    exit_time, exit_price, is_success = self._find_tp_sl_exit(
                        bot, symbol, timeframe, trade_time, entry_price, tp_price, sl_price
                    )
                    
                    # åˆ°é”åˆ¤å®šãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if exit_time is None:
                        # å–¶æ¥­æ™‚é–“å†…ï¼ˆå¹³æ—¥ã®9:00-21:00 JST = 0:00-12:00 UTCï¼‰ã«èª¿æ•´
                        if trade_time.weekday() >= 5:  # åœŸæ—¥ã¯æœˆæ›œã«ç§»å‹•
                            trade_time += timedelta(days=(7 - trade_time.weekday()))
                        # æ™‚é–“èª¿æ•´ï¼ˆ9:00-21:00 JST = 0:00-12:00 UTCï¼‰
                        hour = trade_time.hour
                        if hour < 0:  # JST 9:00 = UTC 0:00
                            trade_time = trade_time.replace(hour=0)
                        elif hour > 12:  # JST 21:00 = UTC 12:00
                            trade_time = trade_time.replace(hour=12)
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ™‚é–“è¶³ã«å¿œã˜ãŸæœŸé–“å¾Œã«å»ºå€¤æ±ºæ¸ˆ
                        exit_minutes = self._get_fallback_exit_minutes(timeframe)
                        exit_time = trade_time + timedelta(minutes=exit_minutes)
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¤å®šä¸èƒ½ã®ãŸã‚å»ºå€¤æ±ºæ¸ˆï¼ˆãƒ—ãƒ©ãƒã‚¤0ï¼‰
                        is_success = None  # åˆ¤å®šä¸èƒ½ã‚’ç¤ºã™
                        exit_price = entry_price  # å»ºå€¤æ±ºæ¸ˆ
                    
                    # PnLè¨ˆç®—
                    pnl_pct = (exit_price - entry_price) / entry_price
                    leveraged_pnl = pnl_pct * leverage
                    
                    # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®ç·åˆæ¤œè¨¼
                    backtest_validation = self.price_validator.validate_backtest_result(
                        entry_price=entry_price,
                        stop_loss_price=sl_price,
                        take_profit_price=tp_price,
                        exit_price=exit_price,
                        duration_minutes=int((exit_time - trade_time).total_seconds() / 60),
                        symbol=symbol
                    )
                    
                    if not backtest_validation['is_valid']:
                        logger.warning(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœç•°å¸¸æ¤œçŸ¥: {symbol} {timeframe}")
                        logger.warning(f"å•é¡Œ: {', '.join(backtest_validation['issues'])}")
                        
                        # é‡å¤§ãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯éŠ˜æŸ„è¿½åŠ è‡ªä½“ã‚’åœæ­¢
                        if backtest_validation['severity_level'] == 'critical':
                            logger.error(f"é‡å¤§ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç•°å¸¸ã®ãŸã‚éŠ˜æŸ„è¿½åŠ ã‚’åœæ­¢: {symbol} at {trade_time}")
                            logger.error(f"è©³ç´°: {', '.join(backtest_validation['issues'])}")
                            raise Exception(f"é‡å¤§ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç•°å¸¸æ¤œçŸ¥: {', '.join(backtest_validation['issues'])}")
                    
                    # æ—¥æœ¬æ™‚é–“ï¼ˆUTC+9ï¼‰ã§è¡¨ç¤º
                    jst_entry_time = trade_time + timedelta(hours=9)
                    jst_exit_time = exit_time + timedelta(hours=9)
                    
                    trades.append({
                        'entry_time': jst_entry_time.strftime('%Y-%m-%d %H:%M:%S JST'),
                        'exit_time': jst_exit_time.strftime('%Y-%m-%d %H:%M:%S JST'),
                        'entry_price': entry_price,
                        'exit_price': exit_price,
                        'take_profit_price': tp_price,
                        'stop_loss_price': sl_price,
                        'leverage': leverage,
                        'pnl_pct': leveraged_pnl,
                        'confidence': confidence,
                        'is_success': is_success,
                        'trade_type': 'breakeven' if is_success is None else ('profit' if is_success else 'loss'),
                        'strategy': config,
                        # ä¾¡æ ¼æ•´åˆæ€§æƒ…å ±ã®è¿½åŠ 
                        'price_consistency_score': unified_price_data.consistency_score,
                        'price_validation_level': price_consistency_result.inconsistency_level.value,
                        'backtest_validation_severity': backtest_validation['severity_level'],
                        'analysis_price': current_price  # ãƒ‡ãƒãƒƒã‚°ç”¨
                    })
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ (è©•ä¾¡{total_evaluations}): {str(e)[:100]}")
                    logger.warning(f"Analysis failed for {symbol} at {current_time}: {e}")
                
                # forãƒ«ãƒ¼ãƒ—ãªã®ã§è‡ªå‹•çš„ã«æ¬¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«é€²ã‚€
            
            # å…¨ãƒ‡ãƒ¼ã‚¿è©•ä¾¡å®Œäº†ã®ãƒ­ã‚°
            logger.info(f"âœ… {symbol} {timeframe} {config}: å…¨{total_evaluations}æœ¬ã®ãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡å®Œäº†")
            
            if not trades:
                print(f"â„¹ï¸ {symbol} {timeframe} {config}: è©•ä¾¡æœŸé–“ä¸­ã«æ¡ä»¶ã‚’æº€ãŸã™ã‚·ã‚°ãƒŠãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return []  # ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã«ã—ãªã„ï¼‰
            
            evaluation_rate = (signals_generated / total_evaluations * 100) if total_evaluations > 0 else 0
            logger.info(f"âœ… {symbol} {timeframe} {config}: æ¡ä»¶ãƒ™ãƒ¼ã‚¹åˆ†æå®Œäº†")
            print(f"   ğŸ“Š ç·è©•ä¾¡æ•°: {total_evaluations}, ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ: {signals_generated}ä»¶ ({evaluation_rate:.1f}%)")
            
            # ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            if trades:
                validation_summary = self.price_validator.get_validation_summary(hours=24)
                if validation_summary['total_validations'] > 0:
                    print(f"   ğŸ” ä¾¡æ ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {validation_summary['consistency_rate']:.1f}% æ•´åˆæ€§")
                    print(f"   ğŸ“ˆ å¹³å‡ä¾¡æ ¼å·®ç•°: {validation_summary['avg_difference_pct']:.2f}%")
                    if validation_summary['level_counts'].get('critical', 0) > 0:
                        logger.warning(f"   âš ï¸ é‡å¤§ãªä¾¡æ ¼ä¸æ•´åˆ: {validation_summary['level_counts']['critical']}ä»¶")
            
            return trades
            
        except Exception as e:
            logger.error(f"Condition-based analysis failed: {e}")
            raise
    
    def _evaluate_entry_conditions(self, analysis_result, timeframe):
        """
        ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’è©•ä¾¡ã—ã¦ã€ã‚·ã‚°ãƒŠãƒ«ç”ŸæˆãŒé©åˆ‡ã‹ã‚’åˆ¤å®š
        
        Args:
            analysis_result: ãƒã‚¤ãƒ¬ãƒãƒœãƒƒãƒˆã‹ã‚‰ã®åˆ†æçµæœ
            timeframe: æ™‚é–“è¶³
            
        Returns:
            bool: ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ã©ã†ã‹
        """
        
        # åŸºæœ¬çš„ãªæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        leverage = analysis_result.get('leverage', 0)
        confidence = analysis_result.get('confidence', 0) / 100.0  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‹ã‚‰æ¯”ç‡ã«å¤‰æ›
        risk_reward = analysis_result.get('risk_reward_ratio', 0)
        current_price = analysis_result.get('current_price', 0)
        
        # çµ±åˆè¨­å®šã‹ã‚‰æ¡ä»¶ã‚’å–å¾—ï¼ˆæˆ¦ç•¥ã‚‚è€ƒæ…®ï¼‰
        try:
            from config.unified_config_manager import UnifiedConfigManager
            config_manager = UnifiedConfigManager()
            
            # åˆ†æä¸­ã®æˆ¦ç•¥ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Balancedï¼‰
            strategy = analysis_result.get('strategy', 'Balanced')
            
            # çµ±åˆè¨­å®šã‹ã‚‰æ¡ä»¶ã‚’å–å¾—
            conditions = config_manager.get_entry_conditions(timeframe, strategy)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
            import os
            filter_params_env = os.getenv('FILTER_PARAMS')
            if filter_params_env:
                try:
                    import json
                    filter_params = json.loads(filter_params_env)
                    entry_conditions = filter_params.get('entry_conditions', {})
                    
                    if entry_conditions:
                        # WebUIã‹ã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨­å®šã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
                        original_conditions = conditions.copy()
                        if 'min_leverage' in entry_conditions:
                            conditions['min_leverage'] = entry_conditions['min_leverage']
                        if 'min_confidence' in entry_conditions:
                            conditions['min_confidence'] = entry_conditions['min_confidence']
                        if 'min_risk_reward' in entry_conditions:
                            conditions['min_risk_reward'] = entry_conditions['min_risk_reward']
                        
                        logger.info(f"ğŸ”§ ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶ã‚’WebUIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰:")
                        logger.info(f"   min_leverage: {original_conditions.get('min_leverage')} â†’ {conditions['min_leverage']}")
                        logger.info(f"   min_confidence: {original_conditions.get('min_confidence')} â†’ {conditions['min_confidence']}")
                        logger.info(f"   min_risk_reward: {original_conditions.get('min_risk_reward')} â†’ {conditions['min_risk_reward']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è§£æã‚¨ãƒ©ãƒ¼: {e}")
            
        except Exception as e:
            # è¨­å®šèª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯éŠ˜æŸ„è¿½åŠ ã‚’åœæ­¢
            error_msg = f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è¨­å®šãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}"
            print(f"âŒ è¨­å®šã‚¨ãƒ©ãƒ¼: {error_msg}")
            raise InsufficientConfigurationError(
                message=error_msg,
                error_type="entry_conditions_config_failed",
                missing_config="unified_entry_conditions"
            )
        
        # ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æ¤œè¨¼ã¨ãƒ­ã‚°æ©Ÿèƒ½
        logger.error(f"ğŸ” ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è©•ä¾¡é–‹å§‹:")
        logger.error(f"   åˆ†æçµæœã®ç”Ÿãƒ‡ãƒ¼ã‚¿:")
        logger.error(f"     leverage: {leverage} (å‹: {type(leverage)})")
        logger.error(f"     confidence: {confidence} (å‹: {type(confidence)}) [å…ƒã®å€¤: {analysis_result.get('confidence')}]")
        logger.error(f"     risk_reward_ratio: {risk_reward} (å‹: {type(risk_reward)})")
        logger.error(f"     current_price: {current_price} (å‹: {type(current_price)})")
        
        # Noneå€¤ãƒã‚§ãƒƒã‚¯ã¨è©³ç´°ã‚¨ãƒ©ãƒ¼å ±å‘Š
        validation_errors = []
        
        if leverage is None:
            validation_errors.append("leverage is None - åˆ†æçµæœã‹ã‚‰ãƒ¬ãƒãƒ¬ãƒƒã‚¸å€¤ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        if confidence is None:
            validation_errors.append("confidence is None - åˆ†æçµæœã‹ã‚‰ä¿¡é ¼åº¦ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        if risk_reward is None:
            validation_errors.append("risk_reward_ratio is None - åˆ†æçµæœã‹ã‚‰ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¯”ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        if current_price is None:
            validation_errors.append("current_price is None - åˆ†æçµæœã‹ã‚‰ç¾åœ¨ä¾¡æ ¼ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # Noneå€¤ãŒã‚ã‚‹å ´åˆã¯è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
        if validation_errors:
            logger.error(f"ğŸš¨ åˆ†æçµæœã«Noneå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™:")
            for error in validation_errors:
                logger.error(f"   âŒ {error}")
            logger.error(f"   ğŸ“Š åˆ†æçµæœã®å…¨å†…å®¹:")
            for key, value in analysis_result.items():
                logger.error(f"     {key}: {value} (å‹: {type(value)})")
            
            # Noneå€¤ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦ä¾‹å¤–ã‚’ç™ºç”Ÿ
            raise ValueError(f"åˆ†æçµæœã«Noneå€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {', '.join(validation_errors)}")
        
        # æ¡ä»¶è©•ä¾¡
        conditions_met = []
        
        try:
            # 1. ãƒ¬ãƒãƒ¬ãƒƒã‚¸æ¡ä»¶
            leverage_ok = leverage >= conditions['min_leverage']
            conditions_met.append(('leverage', leverage_ok, f"{leverage:.1f}x >= {conditions['min_leverage']}x"))
            
            # 2. ä¿¡é ¼åº¦æ¡ä»¶
            confidence_ok = confidence >= conditions['min_confidence']
            conditions_met.append(('confidence', confidence_ok, f"{confidence:.1%} >= {conditions['min_confidence']:.1%}"))
            
            # 3. ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¡ä»¶
            risk_reward_ok = risk_reward >= conditions['min_risk_reward']
            conditions_met.append(('risk_reward', risk_reward_ok, f"{risk_reward:.1f} >= {conditions['min_risk_reward']}"))
            
            # 4. ä¾¡æ ¼ã®æœ‰åŠ¹æ€§
            price_ok = current_price > 0
            conditions_met.append(('price', price_ok, f"price={current_price}"))
            
        except Exception as e:
            logger.error(f"ğŸš¨ ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            logger.error(f"   ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
            logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            logger.error(f"   å€¤ã®è©³ç´°:")
            logger.error(f"     leverage: {leverage} (å‹: {type(leverage)})")
            logger.error(f"     confidence: {confidence} (å‹: {type(confidence)})")
            logger.error(f"     risk_reward: {risk_reward} (å‹: {type(risk_reward)})")
            logger.error(f"     current_price: {current_price} (å‹: {type(current_price)})")
            raise ValueError(f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}") from e
        
        # å…¨ã¦ã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        all_conditions_met = all(condition[1] for condition in conditions_met)
        
        # æ¡ä»¶è©•ä¾¡çµæœã®è©³ç´°ãƒ­ã‚°
        logger.error(f"ğŸ¯ ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶è©•ä¾¡çµæœ:")
        for condition_name, result, description in conditions_met:
            status = "âœ… OK" if result else "âŒ NG"
            logger.error(f"   {condition_name}: {status} - {description}")
        logger.error(f"   æœ€çµ‚åˆ¤å®š: {'âœ… é€šé' if all_conditions_met else 'âŒ é™¤å¤–'}")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: OPã®æ¡ä»¶è©•ä¾¡è©³ç´°
        if 'OP' in str(analysis_result.get('symbol', '')):
            logger.error(f"ğŸ” OPæ¡ä»¶è©•ä¾¡è©³ç´°:")
            logger.error(f"   ãƒ¬ãƒãƒ¬ãƒƒã‚¸: {leverage} (å¿…è¦: {conditions['min_leverage']}) â†’ {leverage_ok}")
            logger.error(f"   ä¿¡é ¼åº¦: {confidence:.1%} (å¿…è¦: {conditions['min_confidence']:.1%}) â†’ {confidence_ok}")
            logger.error(f"   RRæ¯”: {risk_reward} (å¿…è¦: {conditions['min_risk_reward']}) â†’ {risk_reward_ok}")
            logger.error(f"   çµæœ: {all_conditions_met}")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°ï¼ˆæ¡ä»¶æº€è¶³æ™‚ã®ã¿è©³ç´°è¡¨ç¤ºï¼‰
        if all_conditions_met:
            print(f"   âœ… ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¡ä»¶æº€è¶³: L={leverage:.1f}x, C={confidence:.1%}, RR={risk_reward:.1f}")
        
        return all_conditions_met
    
    def _create_strategy_from_config(self, config: str):
        """è¨­å®šã‹ã‚‰æˆ¦ç•¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        class ConfigBasedStrategy:
            def __init__(self, config_name):
                self.name = config_name
                
                # åŸºæœ¬è¨­å®š
                self.min_volume_threshold = 500000
                self.max_spread_threshold = 0.05
                self.min_liquidity_score = 0.5
                
                # æˆ¦ç•¥åˆ¥ã®è¨­å®š
                if 'Conservative' in config_name:
                    self.min_ml_confidence = 0.8
                    self.min_support_strength = 0.7
                    self.min_resistance_strength = 0.7
                    self.max_volatility = 0.1
                elif 'Aggressive' in config_name:
                    self.min_ml_confidence = 0.6
                    self.min_support_strength = 0.5
                    self.min_resistance_strength = 0.5
                    self.max_volatility = 0.2
                else:
                    self.min_ml_confidence = 0.7
                    self.min_support_strength = 0.6
                    self.min_resistance_strength = 0.6
                    self.max_volatility = 0.15
                
                # è·é›¢æ¡ä»¶
                self.min_distance_from_support = 0.5
                self.max_distance_from_support = 5.0
                self.min_distance_from_resistance = 1.0
                self.max_distance_from_resistance = 8.0
                
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¡ä»¶
                self.min_volatility = 0.01
                self.max_atr_ratio = 0.05
                
                # MLæ¡ä»¶
                self.required_ml_signal = "BUY"
                self.min_ml_signal_strength = 0.6
        
        return ConfigBasedStrategy(config)
    
    # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
    # ç†ç”±: "è»½é‡äº‹å‰ãƒã‚§ãƒƒã‚¯"ã¨è¬³ã„ãªãŒã‚‰é‡ã„è¨ˆç®—ã‚’å®Ÿè¡Œã—ã€2.6å€ã®æ€§èƒ½åŠ£åŒ–
    # è©³ç´°: README.mdå‚ç…§
    
    # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
    
    # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
    
    # Stage 9ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ãƒ¢ãƒƒã‚¯æˆ¦ç•¥ä½œæˆãƒ¡ã‚½ãƒƒãƒ‰å‰Šé™¤æ¸ˆã¿ (2025å¹´6æœˆ29æ—¥)
    
    def _analysis_exists(self, analysis_id):
        """åˆ†æãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # ãƒãƒƒã‚·ãƒ¥IDã®å ´åˆã¯DBã‹ã‚‰ç›´æ¥æ¤œç´¢
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ã¾ãšãƒãƒƒã‚·ãƒ¥IDã§æ¤œç´¢ã‚’è©¦è¡Œ
            if len(analysis_id) == 32:  # MD5ãƒãƒƒã‚·ãƒ¥ã®å ´åˆ
                cursor.execute(
                    'SELECT COUNT(*) FROM analyses WHERE symbol || "_" || timeframe || "_" || config = ?',
                    (analysis_id,)
                )
            else:
                # å¾“æ¥ã®å½¢å¼ã®å ´åˆ
                try:
                    symbol, timeframe, config = analysis_id.split('_', 2)
                    cursor.execute(
                        'SELECT COUNT(*) FROM analyses WHERE symbol=? AND timeframe=? AND config=?',
                        (symbol, timeframe, config)
                    )
                except ValueError:
                    return False
            return cursor.fetchone()[0] > 0
    
    # TODO: ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆã¯å“è³ªå•é¡Œã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ (2024-06-18)
    # ç¾åœ¨ã¯ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ãŒã€å°†æ¥çš„ã«å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ–¹æ³•ã«ç½®ãæ›ãˆã‚‹å¿…è¦ã‚ã‚Š
    # def _generate_sample_trades(self, symbol, timeframe, config, num_trades=100):
    #     """ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè»½é‡ç‰ˆï¼‰"""
    #     np.random.seed(hash(f"{symbol}_{timeframe}_{config}") % 2**32)
    #     
    #     # åŸºæœ¬æ€§èƒ½è¨­å®š
    #     base_performance = {
    #         'Conservative_ML': {'sharpe': 1.2, 'win_rate': 0.65},
    #         'Aggressive_Traditional': {'sharpe': 1.8, 'win_rate': 0.55},
    #         'Full_ML': {'sharpe': 2.1, 'win_rate': 0.62}
    #     }.get(config, {'sharpe': 1.5, 'win_rate': 0.58})
    #     
    #     trades = []
    #     cumulative_return = 0
    #     
    #     for i in range(num_trades):
    #         # ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆ
    #         is_win = np.random.random() < base_performance['win_rate']
    #         
    #         if is_win:
    #             pnl_pct = np.random.exponential(0.03)
    #         else:
    #             pnl_pct = -np.random.exponential(0.015)
    #         
    #         leverage = np.random.uniform(2.0, 8.0)
    #         leveraged_pnl = pnl_pct * leverage
    #         cumulative_return += leveraged_pnl
    #         
    #         trades.append({
    #             'trade_id': i,
    #             'pnl_pct': leveraged_pnl,
    #             'leverage': leverage,
    #             'is_win': is_win,
    #             'cumulative_return': cumulative_return
    #         })
    #     
    #     return pd.DataFrame(trades)
    
    # æš«å®šå®Ÿè£…: ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
    def _generate_sample_trades(self, symbol, timeframe, config, num_trades=100):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰"""
        raise NotImplementedError(
            "ãƒ©ãƒ³ãƒ€ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‰ç”Ÿæˆã¯å“è³ªå•é¡Œã®ãŸã‚ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            "å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )
    
    def _calculate_metrics(self, trades_data):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # ãƒªã‚¹ãƒˆã®å ´åˆã¯DataFrameã«å¤‰æ›
        if isinstance(trades_data, list):
            if not trades_data:
                return {
                    'total_trades': 0,
                    'total_return': 0,
                    'win_rate': 0,
                    'sharpe_ratio': 0,
                    'max_drawdown': 0,
                    'avg_leverage': 0
                }
            
            # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã¨is_winã‚’è¨ˆç®—
            cumulative_return = 0
            for trade in trades_data:
                cumulative_return += trade['pnl_pct']
                trade['cumulative_return'] = cumulative_return
                
                # is_winè¨ˆç®—ã®æ”¹è‰¯ï¼ˆå»ºå€¤æ±ºæ¸ˆå¯¾å¿œï¼‰
                is_success = trade.get('is_success')
                if is_success is None:
                    # å»ºå€¤æ±ºæ¸ˆï¼ˆåˆ¤å®šä¸èƒ½ï¼‰ã®å ´åˆã¯å‹æ•—åˆ¤å®šå¯¾è±¡å¤–
                    trade['is_win'] = None
                else:
                    # é€šå¸¸ã®å‹æ•—åˆ¤å®š
                    trade['is_win'] = is_success if is_success is not None else (trade['pnl_pct'] > 0)
            
            trades_df = pd.DataFrame(trades_data)
        else:
            trades_df = trades_data
        
        if len(trades_df) == 0:
            return {
                'total_trades': 0,
                'total_return': 0,
                'win_rate': 0,
                'sharpe_ratio': 0,
                'max_drawdown': 0,
                'avg_leverage': 0
            }
        
        total_return = trades_df['cumulative_return'].iloc[-1] if len(trades_df) > 0 else 0
        
        # å‹ç‡è¨ˆç®—ã®æ”¹è‰¯ï¼ˆå»ºå€¤æ±ºæ¸ˆã‚’é™¤å¤–ï¼‰
        if len(trades_df) > 0:
            valid_trades = trades_df['is_win'].notna()  # Noneä»¥å¤–ï¼ˆå‹æ•—åˆ¤å®šå¯èƒ½ï¼‰
            win_rate = trades_df[valid_trades]['is_win'].mean() if valid_trades.sum() > 0 else 0
        else:
            win_rate = 0
        
        # Sharpeæ¯”ã®ç°¡æ˜“è¨ˆç®—
        returns = trades_df['pnl_pct'].values
        sharpe_ratio = np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0
        
        # æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
        cum_returns = trades_df['cumulative_return'].values
        peak = np.maximum.accumulate(cum_returns)
        drawdown = (cum_returns - peak) / peak
        max_drawdown = np.min(drawdown) if len(drawdown) > 0 else 0
        
        # ä¾¡æ ¼æ•´åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        price_consistency_metrics = {}
        if 'price_consistency_score' in trades_df.columns:
            price_consistency_metrics = {
                'avg_price_consistency': trades_df['price_consistency_score'].mean(),
                'critical_price_issues': len(trades_df[trades_df['price_validation_level'] == 'critical']),
                'critical_backtest_issues': len(trades_df[trades_df['backtest_validation_severity'] == 'critical'])
            }
        else:
            price_consistency_metrics = {
                'avg_price_consistency': 1.0,
                'critical_price_issues': 0,
                'critical_backtest_issues': 0
            }
        
        # å»ºå€¤æ±ºæ¸ˆçµ±è¨ˆã®è¨ˆç®—
        breakeven_stats = {}
        if len(trades_df) > 0:
            breakeven_count = trades_df['is_win'].isna().sum()
            total_decisive_trades = len(trades_df) - breakeven_count
            breakeven_stats = {
                'breakeven_trades': breakeven_count,
                'decisive_trades': total_decisive_trades,
                'breakeven_rate': (breakeven_count / len(trades_df)) if len(trades_df) > 0 else 0
            }
        else:
            breakeven_stats = {
                'breakeven_trades': 0,
                'decisive_trades': 0,
                'breakeven_rate': 0
            }
        
        base_metrics = {
            'total_trades': len(trades_df),
            'total_return': total_return,
            'win_rate': win_rate,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'avg_leverage': trades_df['leverage'].mean() if len(trades_df) > 0 else 0
        }
        
        # å»ºå€¤æ±ºæ¸ˆçµ±è¨ˆã‚’è¿½åŠ 
        base_metrics.update(breakeven_stats)
        
        # ä¾¡æ ¼æ•´åˆæ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’çµåˆ
        base_metrics.update(price_consistency_metrics)
        return base_metrics
    
    def _save_compressed_data(self, analysis_id, trades_df):
        """ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®ã—ã¦ä¿å­˜"""
        compressed_path = self.compressed_dir / f"{analysis_id}.pkl.gz"
        
        with gzip.open(compressed_path, 'wb') as f:
            pickle.dump(trades_df, f)
        
        return str(compressed_path)
    
    def _should_generate_chart(self, metrics):
        """ãƒãƒ£ãƒ¼ãƒˆç”ŸæˆãŒå¿…è¦ã‹ã©ã†ã‹åˆ¤å®šï¼ˆä¸Šä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ã¿ï¼‰"""
        return metrics['sharpe_ratio'] > 1.5  # Sharpe > 1.5ã®ã¿ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ
    
    def _generate_lightweight_chart(self, analysis_id, trades_df, metrics):
        """è»½é‡ç‰ˆãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ"""
        # å®Ÿè£…ã¯çœç•¥ï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
        chart_path = self.charts_dir / f"{analysis_id}_chart.html"
        
        # ç°¡æ˜“HTMLä½œæˆ
        html_content = f"""
        <html>
        <head><title>{analysis_id} Analysis</title></head>
        <body>
        <h1>{analysis_id}</h1>
        <p>Sharpe Ratio: {metrics['sharpe_ratio']:.2f}</p>
        <p>Win Rate: {metrics['win_rate']:.1%}</p>
        <p>Total Return: {metrics['total_return']:.1%}</p>
        </body>
        </html>
        """
        
        with open(chart_path, 'w') as f:
            f.write(html_content)
        
        return str(chart_path)
    
    def _save_to_database(self, symbol, timeframe, config, metrics, chart_path, compressed_path, execution_id=None):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆexecution_idå¯¾å¿œ + task_statusæ›´æ–° + æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼‰"""
        logger.info(f"ğŸ’¾ DBä¿å­˜é–‹å§‹: {symbol} {timeframe} {config}")
        logger.info(f"  ğŸ—ƒï¸ ä¿å­˜å…ˆDB: {self.db_path.absolute()}")
        logger.info(f"  ğŸ”‘ execution_id: {execution_id or os.environ.get('CURRENT_EXECUTION_ID', 'None')}")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # execution_idã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯å¼•æ•°ã‹ã‚‰å–å¾—
            if not execution_id:
                execution_id = os.environ.get('CURRENT_EXECUTION_ID')
            
            try:
                # ğŸ”¥ é‡è¦: INSERTã‹ã‚‰UPDATEã«å¤‰æ›´ï¼ˆpre-taskãƒ¬ã‚³ãƒ¼ãƒ‰æ›´æ–°ï¼‰
                cursor.execute('''
                    UPDATE analyses SET
                        total_trades=?, win_rate=?, total_return=?, 
                        sharpe_ratio=?, max_drawdown=?, avg_leverage=?, 
                        chart_path=?, compressed_path=?, 
                        status='completed', task_status='completed', task_completed_at=?
                    WHERE symbol=? AND timeframe=? AND config=? AND execution_id=?
                ''', (
                    metrics['total_trades'], metrics['win_rate'], metrics['total_return'],
                    metrics['sharpe_ratio'], metrics['max_drawdown'], metrics['avg_leverage'],
                    chart_path, compressed_path, 
                    datetime.now(timezone.utc).isoformat(),
                    symbol, timeframe, config, execution_id
                ))
                
                updated_rows = cursor.rowcount
                analysis_id = None
                
                if updated_rows == 0:
                    # Pre-taskãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å¾“æ¥é€šã‚ŠINSERT
                    logger.warning(f"âš ï¸ Pre-taskãƒ¬ã‚³ãƒ¼ãƒ‰ãªã— - INSERTå®Ÿè¡Œ: {symbol} {timeframe} {config}")
                    cursor.execute('''
                        INSERT INTO analyses 
                        (symbol, timeframe, config, total_trades, win_rate, total_return, 
                         sharpe_ratio, max_drawdown, avg_leverage, chart_path, compressed_path, status, 
                         task_status, task_completed_at, execution_id)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'completed', 'completed', ?, ?)
                    ''', (
                        symbol, timeframe, config,
                        metrics['total_trades'], metrics['win_rate'], metrics['total_return'],
                        metrics['sharpe_ratio'], metrics['max_drawdown'], metrics['avg_leverage'],
                        chart_path, compressed_path, 
                        datetime.now(timezone.utc).isoformat(),
                        execution_id
                    ))
                    analysis_id = cursor.lastrowid
                else:
                    # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã®IDã‚’å–å¾—
                    cursor.execute('''
                        SELECT id FROM analyses 
                        WHERE symbol=? AND timeframe=? AND config=? AND execution_id=?
                    ''', (symbol, timeframe, config, execution_id))
                    result = cursor.fetchone()
                    if result:
                        analysis_id = result[0]
                
                # æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
                if analysis_id and 'leverage_details' in metrics and metrics['leverage_details']:
                    logger.info(f"ğŸ“Š æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¿å­˜é–‹å§‹: {len(metrics['leverage_details'])}ä»¶")
                    
                    for i, detail in enumerate(metrics['leverage_details']):
                        cursor.execute('''
                            INSERT INTO leverage_calculation_details 
                            (analysis_id, trade_number, support_distance_pct, support_constraint_leverage,
                             risk_reward_ratio, risk_reward_constraint_leverage, confidence_pct, 
                             confidence_constraint_leverage, btc_correlation, btc_constraint_leverage,
                             volatility_pct, volatility_constraint_leverage, trend_strength,
                             trend_multiplier, min_constraint_leverage, safety_margin_pct, final_leverage)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            analysis_id, i + 1,
                            detail.get('support_distance_pct'),
                            detail.get('support_constraint_leverage'),
                            detail.get('risk_reward_ratio'),
                            detail.get('risk_reward_constraint_leverage'),
                            detail.get('confidence_pct'),
                            detail.get('confidence_constraint_leverage'),
                            detail.get('btc_correlation'),
                            detail.get('btc_constraint_leverage'),
                            detail.get('volatility_pct'),
                            detail.get('volatility_constraint_leverage'),
                            detail.get('trend_strength'),
                            detail.get('trend_multiplier'),
                            detail.get('min_constraint_leverage'),
                            detail.get('safety_margin_pct'),
                            detail.get('final_leverage')
                        ))
                    
                    logger.info(f"âœ… æ”¯æŒç·šãƒ»æŠµæŠ—ç·šãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {len(metrics['leverage_details'])}ä»¶")
                
                conn.commit()
                logger.info(f"âœ… DBä¿å­˜æˆåŠŸ: {symbol} {timeframe} {config} ({'UPDATE' if updated_rows > 0 else 'INSERT'})")
                
            except Exception as e:
                logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {symbol} {timeframe} {config}")
                logger.error(f"  ğŸ—ƒï¸ DB path: {self.db_path.absolute()}")
                logger.error(f"  ğŸ“ ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
                raise
    
    def query_analyses(self, filters=None, order_by='sharpe_ratio', limit=100):
        """åˆ†æçµæœã‚’ã‚¯ã‚¨ãƒª"""
        with sqlite3.connect(self.db_path) as conn:
            query = "SELECT * FROM analyses WHERE status='completed'"
            params = []
            
            if filters:
                if 'symbol' in filters:
                    if isinstance(filters['symbol'], list):
                        query += " AND symbol IN ({})".format(','.join(['?' for _ in filters['symbol']]))
                        params.extend(filters['symbol'])
                    else:
                        query += " AND symbol = ?"
                        params.append(filters['symbol'])
                
                if 'timeframe' in filters:
                    if isinstance(filters['timeframe'], list):
                        query += " AND timeframe IN ({})".format(','.join(['?' for _ in filters['timeframe']]))
                        params.extend(filters['timeframe'])
                    else:
                        query += " AND timeframe = ?"
                        params.append(filters['timeframe'])
                
                if 'config' in filters:
                    if isinstance(filters['config'], list):
                        query += " AND config IN ({})".format(','.join(['?' for _ in filters['config']]))
                        params.extend(filters['config'])
                    else:
                        query += " AND config = ?"
                        params.append(filters['config'])
                
                if 'min_sharpe' in filters:
                    query += " AND sharpe_ratio >= ?"
                    params.append(filters['min_sharpe'])
            
            query += f" ORDER BY {order_by} DESC LIMIT ?"
            params.append(limit)
            
            cursor = conn.cursor()
            cursor.execute(query, params)
            columns = [description[0] for description in cursor.description]
            rows = cursor.fetchall()
            
            # Create a list of dictionaries instead of pandas DataFrame
            result = []
            for row in rows:
                result.append(dict(zip(columns, row)))
            return result
    
    def get_statistics(self):
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) as total, status FROM analyses GROUP BY status")
            status_counts = cursor.fetchall()
            
            # æ€§èƒ½çµ±è¨ˆ
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_analyses,
                    AVG(sharpe_ratio) as avg_sharpe,
                    MAX(sharpe_ratio) as max_sharpe,
                    AVG(total_return) as avg_return,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    COUNT(DISTINCT config) as unique_configs
                FROM analyses WHERE status='completed'
            """)
            perf_stats = cursor.fetchone()
            
            return {
                'status_counts': dict(status_counts),
                'performance': {
                    'total_analyses': perf_stats[0],
                    'avg_sharpe': perf_stats[1],
                    'max_sharpe': perf_stats[2],
                    'avg_return': perf_stats[3],
                    'unique_symbols': perf_stats[4],
                    'unique_configs': perf_stats[5]
                }
            }
    
    def load_compressed_trades(self, symbol, timeframe, config):
        """åœ§ç¸®ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        analysis_id = f"{symbol}_{timeframe}_{config}"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‘ã‚¹ã‚’å–å¾—
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT compressed_path FROM analyses WHERE symbol=? AND timeframe=? AND config=?",
                (symbol, timeframe, config)
            )
            result = cursor.fetchone()
            
            if not result or not result[0]:
                logger.warning(f"åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {analysis_id}")
                return None
            
            compressed_path = result[0]
        
        # åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        try:
            with gzip.open(compressed_path, 'rb') as f:
                trades_df = pickle.load(f)
            logger.info(f"ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {analysis_id} ({len(trades_df)}ãƒˆãƒ¬ãƒ¼ãƒ‰)")
            return trades_df
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {analysis_id}: {e}")
            return None
    
    def load_multiple_trades(self, criteria=None):
        """è¤‡æ•°ã®åœ§ç¸®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬èª­ã¿è¾¼ã¿"""
        # ã‚¯ã‚¨ãƒªæ¡ä»¶ã«åŸºã¥ã„ã¦åˆ†æã‚’å–å¾—
        analyses_df = self.query_analyses(filters=criteria)
        
        all_trades = {}
        for _, analysis in analyses_df.iterrows():
            trades_df = self.load_compressed_trades(
                analysis['symbol'], 
                analysis['timeframe'], 
                analysis['config']
            )
            if trades_df is not None:
                key = f"{analysis['symbol']}_{analysis['timeframe']}_{analysis['config']}"
                all_trades[key] = trades_df
        
        logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(all_trades)}ä»¶ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿")
        return all_trades
    
    def export_to_csv(self, symbol, timeframe, config, output_path=None):
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        trades_df = self.load_compressed_trades(symbol, timeframe, config)
        
        if trades_df is None:
            return False
        
        if output_path is None:
            output_path = f"{symbol}_{timeframe}_{config}_trades_export.csv"
        
        trades_df.to_csv(output_path, index=False)
        logger.info(f"CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}")
        return True
    
    def test_discord_notification(self, test_type="early_exit"):
        """Discordé€šçŸ¥ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½"""
        try:
            from dotenv import load_dotenv
            load_dotenv()
        except ImportError:
            pass
        
        import os
        webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
        
        if not webhook_url:
            logger.error("âŒ DISCORD_WEBHOOK_URLç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        logger.info(f"ğŸ”” Discordé€šçŸ¥ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_type}")
        
        if test_type == "early_exit":
            # Early Exitæ¨¡æ“¬é€šçŸ¥
            from engines.analysis_result import AnalysisResult, AnalysisStage, ExitReason
            
            result = AnalysisResult(
                symbol="TEST",
                timeframe="1h", 
                strategy="Conservative_ML-1h",
                execution_id="test_discord_001"
            )
            result.mark_early_exit(
                stage=AnalysisStage.SUPPORT_RESISTANCE,
                reason=ExitReason.NO_SUPPORT_RESISTANCE,
                error_message="Discordé€šçŸ¥ãƒ†ã‚¹ãƒˆç”¨Early Exit"
            )
            
            return self._send_discord_notification_sync(
                symbol="TEST",
                timeframe="1h",
                strategy="Conservative_ML-1h", 
                execution_id="test_discord_001",
                result=result,
                webhook_url=webhook_url
            )
        
        elif test_type == "simple":
            # ã‚·ãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆé€šçŸ¥
            payload = {
                "content": "ğŸ§ª **Discordé€šçŸ¥ãƒ†ã‚¹ãƒˆ**\n\n"
                          "âœ… ProcessPoolExecutorç’°å¢ƒã‹ã‚‰ã®Discordé€šçŸ¥ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚\n"
                          f"â° ãƒ†ã‚¹ãƒˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            }
            
            import requests
            try:
                response = requests.post(webhook_url, json=payload, timeout=10)
                if response.status_code == 204:
                    logger.info("âœ… Discordé€šçŸ¥ãƒ†ã‚¹ãƒˆæˆåŠŸ")
                    return True
                else:
                    logger.error(f"âŒ Discordé€šçŸ¥å¤±æ•—: {response.status_code}")
                    return False
            except Exception as e:
                logger.error(f"âŒ Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        return False

    def get_analysis_details(self, symbol, timeframe, config):
        """åˆ†æã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ + ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼‰"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰åŸºæœ¬æƒ…å ±å–å¾—
        with sqlite3.connect(self.db_path) as conn:
            query = """
                SELECT * FROM analyses 
                WHERE symbol=? AND timeframe=? AND config=?
            """
            analysis_info = pd.read_sql_query(query, conn, params=[symbol, timeframe, config])
        
        if analysis_info.empty:
            return None
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
        trades_df = self.load_compressed_trades(symbol, timeframe, config)
        
        return {
            'info': analysis_info.iloc[0].to_dict(),
            'trades': trades_df
        }
    
    def _handle_discord_notification_for_result(self, result, symbol, timeframe, config, execution_id):
        """ProcessPoolExecutorå°‚ç”¨Discordé€šçŸ¥å‡¦ç†"""
        try:
            from engines.analysis_result import AnalysisResult
            
            # AnalysisResultã®Early Exitç¢ºèª
            if isinstance(result, AnalysisResult) and result.early_exit:
                logger.info(f"ğŸ¯ Discordé€šçŸ¥å‡¦ç†é–‹å§‹: {symbol} {timeframe} Early Exit")
                
                # ç’°å¢ƒå¤‰æ•°å†èª­ã¿è¾¼ã¿ï¼ˆProcessPoolExecutorå¯¾å¿œï¼‰
                try:
                    from dotenv import load_dotenv
                    load_dotenv()
                except ImportError:
                    pass
                
                webhook_url = os.environ.get('DISCORD_WEBHOOK_URL')
                
                if webhook_url:
                    # Discordé€šçŸ¥é€ä¿¡
                    self._send_discord_notification_sync(
                        symbol=symbol,
                        timeframe=timeframe,
                        strategy=config,
                        execution_id=execution_id,
                        result=result,
                        webhook_url=webhook_url
                    )
                else:
                    logger.warning("âš ï¸ DISCORD_WEBHOOK_URL not set in ProcessPoolExecutor")
            else:
                logger.debug(f"Discordé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: Early Exitç„¡ã— ({symbol} {timeframe})")
                
        except Exception as e:
            logger.error(f"âŒ Discordé€šçŸ¥å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _send_discord_notification_sync(self, symbol, timeframe, strategy, execution_id, result, webhook_url):
        """åŒæœŸDiscordé€šçŸ¥é€ä¿¡ï¼ˆProcessPoolExecutorå°‚ç”¨ï¼‰"""
        try:
            import requests
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
            detailed_msg = result.get_detailed_log_message()
            user_msg = result.get_user_friendly_message()
            suggestions = result.get_suggestions()
            
            # Embedä½œæˆ
            embed = {
                "title": f"ğŸš¨ Early Exit Analysis: {symbol}",
                "color": 0xFF4444,
                "timestamp": datetime.now().isoformat(),
                "fields": [
                    {"name": "Symbol", "value": symbol, "inline": True},
                    {"name": "Timeframe", "value": timeframe, "inline": True},
                    {"name": "Strategy", "value": strategy, "inline": True},
                    {"name": "Exit Stage", "value": result.exit_stage.value if result.exit_stage else 'unknown', "inline": True},
                    {"name": "Exit Reason", "value": result.exit_reason.value if result.exit_reason else 'unknown', "inline": True},
                    {"name": "Execution ID", "value": f"`{execution_id}`", "inline": False},
                    {"name": "Detailed Message", "value": detailed_msg[:1000], "inline": False},
                    {"name": "User Message", "value": user_msg[:1000], "inline": False}
                ],
                "footer": {"text": "Long Trader - ProcessPoolExecutor Early Exit"}
            }
            
            if suggestions:
                embed["fields"].append({
                    "name": "ğŸ’¡ Suggestions",
                    "value": "\n".join([f"â€¢ {s}" for s in suggestions[:5]])[:1000],
                    "inline": False
                })
            
            payload = {
                "embeds": [embed],
                "username": "Long Trader Bot"
            }
            
            # é€ä¿¡å®Ÿè¡Œ
            response = requests.post(webhook_url, json=payload, timeout=10)
            
            if response.status_code in [200, 204]:
                logger.info(f"âœ… Discordé€šçŸ¥é€ä¿¡æˆåŠŸ: {symbol} Early Exit")
                return True
            else:
                logger.warning(f"Discordé€šçŸ¥å¤±æ•—: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Discordé€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def send_discord_early_exit_notification(self, symbol: str, timeframe: str, strategy: str, execution_id: str, exit_stage: str, exit_reason: str, detailed_msg: str, user_msg: str, suggestions: List[str]):
        """Discord webhooké€šçŸ¥: å­ãƒ—ãƒ­ã‚»ã‚¹ã®Early Exitè©³ç´°é€ä¿¡"""
        try:
            # Discord webhook URL (ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—)
            webhook_url = os.environ.get('DISCORD_WEBHOOK_URL')
            if not webhook_url:
                self.logger.warning("DISCORD_WEBHOOK_URL not set, skipping notification")
                return
            
            # embedä½œæˆ
            embed = {
                "title": f"ğŸš¨ Early Exit Analysis: {symbol}",
                "color": 0xFF4444,  # èµ¤è‰²
                "timestamp": datetime.now().isoformat(),
                "fields": [
                    {"name": "Symbol", "value": symbol, "inline": True},
                    {"name": "Timeframe", "value": timeframe, "inline": True},
                    {"name": "Strategy", "value": strategy, "inline": True},
                    {"name": "Exit Stage", "value": exit_stage, "inline": True},
                    {"name": "Exit Reason", "value": exit_reason, "inline": True},
                    {"name": "Execution ID", "value": f"`{execution_id}`", "inline": False},
                    {"name": "Detailed Message", "value": detailed_msg[:1000], "inline": False},
                    {"name": "User Message", "value": user_msg[:1000], "inline": False}
                ],
                "footer": {"text": "Long Trader - Early Exit Analysis"}
            }
            
            # æ”¹å–„ææ¡ˆã‚’è¿½åŠ 
            if suggestions:
                embed["fields"].append({
                    "name": "ğŸ’¡ Suggestions",
                    "value": "\n".join([f"â€¢ {s}" for s in suggestions[:5]])[:1000],
                    "inline": False
                })
            
            # Discord APIã«é€ä¿¡
            payload = {
                "embeds": [embed],
                "username": "Long Trader Bot"
            }
            
            # æœ€å¤§8å›ã®ãƒªãƒˆãƒ©ã‚¤ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
            for attempt in range(8):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(webhook_url, json=payload) as response:
                            if response.status == 200:
                                self.logger.info(f"âœ… Discordé€šçŸ¥é€ä¿¡æˆåŠŸ: {symbol} Early Exit")
                                return
                            elif response.status == 429:  # Rate limit
                                retry_after = int(response.headers.get('Retry-After', 1))
                                self.logger.warning(f"Discord rate limit, retrying after {retry_after}s")
                                await asyncio.sleep(retry_after)
                            else:
                                self.logger.warning(f"Discord API error: {response.status}")
                                break
                                
                except Exception as e:
                    if attempt == 7:  # æœ€å¾Œã®è©¦è¡Œ
                        raise e
                    
                    # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• (1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s)
                    wait_time = 2 ** attempt
                    self.logger.warning(f"Discordé€ä¿¡å¤±æ•— (attempt {attempt + 1}/8): {e}, retrying in {wait_time}s")
                    await asyncio.sleep(wait_time)
                    
        except Exception as e:
            self.logger.error(f"Discordé€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    def cleanup_low_performers(self, min_sharpe=0.5):
        """ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # å‰Šé™¤å¯¾è±¡ã‚’å–å¾—
            cursor.execute(
                "SELECT chart_path, compressed_path FROM analyses WHERE sharpe_ratio < ?",
                (min_sharpe,)
            )
            to_delete = cursor.fetchall()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            deleted_files = 0
            for chart_path, data_path in to_delete:
                for file_path in [chart_path, data_path]:
                    if file_path and os.path.exists(file_path):
                        os.remove(file_path)
                        deleted_files += 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚‚å‰Šé™¤
            cursor.execute("DELETE FROM analyses WHERE sharpe_ratio < ?", (min_sharpe,))
            deleted_records = cursor.rowcount
            conn.commit()
            
            logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {deleted_records}ãƒ¬ã‚³ãƒ¼ãƒ‰, {deleted_files}ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤")
            return deleted_records, deleted_files
    
    def _get_real_market_price(self, bot, symbol, timeframe, trade_time):
        """
        å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŒ‡å®šæ™‚åˆ»ã®ä¾¡æ ¼ã‚’å–å¾—
        ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»ãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚ç¾å®Ÿçš„ï¼‰
        
        Args:
            bot: HighLeverageBotOrchestrator ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«  
            timeframe: æ™‚é–“è¶³
            trade_time: ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»
        
        Returns:
            float: å®Ÿéš›ã®å¸‚å ´ä¾¡æ ¼ï¼ˆè©²å½“ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ï¼‰
        """
        try:
            # ãƒœãƒƒãƒˆã‹ã‚‰å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                market_data = bot._cached_data
            else:
                market_data = bot._fetch_market_data(symbol, timeframe)
            
            if market_data.empty:
                raise Exception(f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™: {symbol}")
            
            # ãƒ‡ãƒ¼ã‚¿ã® timestamp ã‚«ãƒ©ãƒ ã‚’ç¢ºèªãƒ»ä½œæˆ
            if 'timestamp' not in market_data.columns:
                if market_data.index.name == 'timestamp' or pd.api.types.is_datetime64_any_dtype(market_data.index):
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å ´åˆ
                    market_data = market_data.reset_index()
                    if 'index' in market_data.columns:
                        market_data = market_data.rename(columns={'index': 'timestamp'})
                else:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ãªã„å ´åˆã¯ä½œæˆ
                    market_data['timestamp'] = pd.to_datetime(market_data.index, utc=True)
            
            # timestampã‚«ãƒ©ãƒ ã‚’datetimeå‹ã«ç¢ºå®Ÿã«å¤‰æ›
            market_data['timestamp'] = pd.to_datetime(market_data['timestamp'], utc=True)
            
            # trade_timeã‚’UTCã«å¤‰æ›
            if trade_time.tzinfo is None:
                target_time = trade_time
            else:
                target_time = trade_time.astimezone(timezone.utc)
            
            # trade_timeãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã‚’è¨ˆç®—
            candle_start_time = self._get_candle_start_time(target_time, timeframe)
            
            # è©²å½“ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã‚’ç‰¹å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è€ƒæ…®ï¼‰
            # market_dataã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’UTCã«çµ±ä¸€
            if market_data['timestamp'].dt.tz is None:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_localize('UTC')
            else:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_convert('UTC')
            
            # candle_start_timeã‚‚UTCã«çµ±ä¸€
            if candle_start_time.tzinfo is None:
                candle_start_time = candle_start_time.replace(tzinfo=timezone.utc)
            else:
                candle_start_time = candle_start_time.astimezone(timezone.utc)
            
            # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ®µéšçš„è¨±å®¹ç¯„å›²æ‹¡å¤§ï¼‰
            time_tolerance = timedelta(minutes=1)
            target_candles = market_data[
                abs(market_data['timestamp'] - candle_start_time) <= time_tolerance
            ]
            
            if target_candles.empty:
                # æ®µéšçš„ã«è¨±å®¹ç¯„å›²ã‚’æ‹¡å¤§ï¼ˆæœ€å¤§30åˆ†ã¾ã§ï¼‰
                for tolerance_minutes in [5, 15, 30]:
                    time_tolerance = timedelta(minutes=tolerance_minutes)
                    target_candles = market_data[
                        abs(market_data['timestamp'] - candle_start_time) <= time_tolerance
                    ]
                    if not target_candles.empty:
                        time_diff = abs(target_candles.iloc[0]['timestamp'] - candle_start_time)
                        logger.warning(
                            f"âš ï¸ {symbol} {timeframe}: {tolerance_minutes}åˆ†è¨±å®¹ç¯„å›²ã§æœ€å¯„ã‚Šãƒ­ãƒ¼ã‚½ã‚¯è¶³ä½¿ç”¨ "
                            f"(æ™‚å·®: {time_diff})"
                        )
                        break
                
                if target_candles.empty:
                    # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¯„ã‚Šã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã‚’ä½¿ç”¨
                    time_diffs = abs(market_data['timestamp'] - candle_start_time)
                    min_diff_idx = time_diffs.idxmin()
                    min_diff = time_diffs[min_diff_idx]
                    
                    if min_diff <= timedelta(hours=2):  # 2æ™‚é–“ä»¥å†…ãªã‚‰ä½¿ç”¨
                        target_candles = market_data.iloc[[min_diff_idx]]
                        logger.warning(
                            f"âš ï¸ {symbol} {timeframe}: æœ€å¯„ã‚Šãƒ­ãƒ¼ã‚½ã‚¯è¶³ä½¿ç”¨ (æ™‚å·®: {min_diff})"
                        )
                    else:
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
                        available_times = market_data['timestamp'].head(10).tolist()
                        raise Exception(
                            f"è©²å½“ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symbol} {timeframe} "
                            f"trade_time={target_time}, candle_start={candle_start_time}. "
                            f"åˆ©ç”¨å¯èƒ½ãªæœ€åˆã®10ä»¶: {available_times}. "
                            f"æœ€å°æ™‚å·®: {min_diff} (2æ™‚é–“ã‚’è¶…é)"
                        )
            
            # æœ€ã‚‚è¿‘ã„ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã‚’é¸æŠ
            target_candle = target_candles.iloc[0]
            
            # ãã®ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®openä¾¡æ ¼ã‚’è¿”ã™ï¼ˆæœ€ã‚‚ç¾å®Ÿçš„ãªã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ï¼‰
            open_price = float(target_candle['open'])
            
            return open_price
            
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ä½¿ç”¨ã›ãšã€ã‚¨ãƒ©ãƒ¼ã§æˆ¦ç•¥åˆ†æã‚’çµ‚äº†
            raise Exception(f"å®Ÿéš›ã®å¸‚å ´ä¾¡æ ¼å–å¾—ã«å¤±æ•—: {symbol} - {str(e)}")
    
    def _get_candle_start_time(self, trade_time, timeframe):
        """
        ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»ãŒå±ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã‚’è¨ˆç®—
        
        Args:
            trade_time: ãƒˆãƒ¬ãƒ¼ãƒ‰æ™‚åˆ»
            timeframe: æ™‚é–“è¶³ï¼ˆä¾‹: '15m', '1h', '4h'ï¼‰
        
        Returns:
            datetime: ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»
        """
        # æ™‚é–“è¶³ã‚’åˆ†å˜ä½ã«å¤‰æ›
        timeframe_minutes = {
            '1m': 1, '3m': 3, '5m': 5, '15m': 15, 
            '30m': 30, '1h': 60, '4h': 240, '1d': 1440
        }
        
        if timeframe not in timeframe_minutes:
            raise Exception(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„æ™‚é–“è¶³: {timeframe}")
        
        minutes_interval = timeframe_minutes[timeframe]
        
        # trade_timeã‚’è©²å½“ã™ã‚‹ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é–‹å§‹æ™‚åˆ»ã«ä¸¸ã‚ã‚‹
        if timeframe == '1d':
            # æ—¥è¶³ã®å ´åˆã¯00:00:00ã«ä¸¸ã‚ã‚‹
            candle_start = trade_time.replace(hour=0, minute=0, second=0, microsecond=0)
        else:
            # åˆ†è¶³ãƒ»æ™‚é–“è¶³ã®å ´åˆ
            total_minutes = trade_time.hour * 60 + trade_time.minute
            candle_minutes = (total_minutes // minutes_interval) * minutes_interval
            
            candle_hour = candle_minutes // 60
            candle_minute = candle_minutes % 60
            
            candle_start = trade_time.replace(
                hour=candle_hour, 
                minute=candle_minute, 
                second=0, 
                microsecond=0
            )
        
        return candle_start
    
    def _find_first_valid_evaluation_time(self, data_start_time, evaluation_interval):
        """
        å®Ÿéš›ã®OHLCVãƒ‡ãƒ¼ã‚¿é–‹å§‹æ™‚åˆ»ã«åŸºã¥ã„ã¦ã€è©•ä¾¡é–“éš”ã®å¢ƒç•Œã«åˆã†æœ€åˆã®æœ‰åŠ¹ãªè©•ä¾¡æ™‚åˆ»ã‚’è¦‹ã¤ã‘ã‚‹
        
        Args:
            data_start_time: å®Ÿéš›ã®OHLCVãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            evaluation_interval: è©•ä¾¡é–“éš”ï¼ˆtimedeltaï¼‰
            
        Returns:
            datetime: æœ€åˆã®æœ‰åŠ¹ãªè©•ä¾¡æ™‚åˆ»
        """
        # è©•ä¾¡é–“éš”ã‚’åˆ†å˜ä½ã«å¤‰æ›
        interval_minutes = int(evaluation_interval.total_seconds() / 60)
        
        # ãƒ‡ãƒ¼ã‚¿é–‹å§‹æ™‚åˆ»ã‚’è©•ä¾¡é–“éš”ã®å¢ƒç•Œã«åˆã‚ã›ã‚‹
        # ä¾‹: ãƒ‡ãƒ¼ã‚¿é–‹å§‹ãŒ06:30ã€è©•ä¾¡é–“éš”ãŒ60åˆ†ã®å ´åˆ â†’ 07:00ã‚’è¿”ã™
        
        # ç¾åœ¨ã®æ™‚åˆ»ã‹ã‚‰è©•ä¾¡é–“éš”ã®å¢ƒç•Œæ™‚åˆ»ã‚’è¨ˆç®—
        current = data_start_time
        
        # æ™‚é–“ãƒ»åˆ†ã‚’è©•ä¾¡é–“éš”ã®å¢ƒç•Œã«åˆã‚ã›ã‚‹
        if interval_minutes >= 60:
            # 1æ™‚é–“ä»¥ä¸Šã®é–“éš”ã®å ´åˆã€æ™‚é–“å¢ƒç•Œã«åˆã‚ã›ã‚‹
            hours_interval = interval_minutes // 60
            aligned_hour = (current.hour // hours_interval + 1) * hours_interval
            if aligned_hour >= 24:
                current = current.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            else:
                current = current.replace(hour=aligned_hour, minute=0, second=0, microsecond=0)
        else:
            # 1æ™‚é–“æœªæº€ã®é–“éš”ã®å ´åˆã€åˆ†å¢ƒç•Œã«åˆã‚ã›ã‚‹
            total_minutes = current.hour * 60 + current.minute
            aligned_minutes = ((total_minutes // interval_minutes) + 1) * interval_minutes
            
            if aligned_minutes >= 24 * 60:
                current = current.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(days=1)
            else:
                aligned_hour = aligned_minutes // 60
                aligned_minute = aligned_minutes % 60
                current = current.replace(hour=aligned_hour, minute=aligned_minute, second=0, microsecond=0)
        
        return current
    
    def _find_tp_sl_exit(self, bot, symbol, timeframe, entry_time, entry_price, tp_price, sl_price):
        """
        TP/SLåˆ°é”ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ã‚ºåˆ¤å®š
        
        Args:
            bot: HighLeverageBotOrchestrator ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«
            timeframe: æ™‚é–“è¶³
            entry_time: ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ™‚åˆ»
            entry_price: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼
            tp_price: åˆ©ç¢ºä¾¡æ ¼
            sl_price: æåˆ‡ã‚Šä¾¡æ ¼
            
        Returns:
            tuple: (exit_time, exit_price, is_success)
        """
        try:
            # ãƒœãƒƒãƒˆã‹ã‚‰å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if hasattr(bot, '_cached_data') and not bot._cached_data.empty:
                market_data = bot._cached_data
            else:
                market_data = bot._fetch_market_data(symbol, timeframe)
            
            if market_data.empty:
                return None, None, None
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚«ãƒ©ãƒ ã®æº–å‚™
            if 'timestamp' not in market_data.columns:
                if market_data.index.name == 'timestamp' or pd.api.types.is_datetime64_any_dtype(market_data.index):
                    market_data = market_data.reset_index()
                    if 'index' in market_data.columns:
                        market_data = market_data.rename(columns={'index': 'timestamp'})
                else:
                    market_data['timestamp'] = pd.to_datetime(market_data.index, utc=True)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’çµ±ä¸€
            market_data['timestamp'] = pd.to_datetime(market_data['timestamp'], utc=True)
            if market_data['timestamp'].dt.tz is None:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_localize('UTC')
            else:
                market_data['timestamp'] = market_data['timestamp'].dt.tz_convert('UTC')
            
            # entry_timeã‚’UTCã«å¤‰æ›
            if entry_time.tzinfo is None:
                entry_time_utc = entry_time.replace(tzinfo=timezone.utc)
            else:
                entry_time_utc = entry_time.astimezone(timezone.utc)
            
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            after_entry_data = market_data[market_data['timestamp'] > entry_time_utc].copy()
            
            if after_entry_data.empty:
                return None, None, None
            
            # å„ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã§TP/SLåˆ°é”ã‚’ãƒã‚§ãƒƒã‚¯
            for _, candle in after_entry_data.iterrows():
                # ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’æƒ³å®š
                candle_high = float(candle['high'])
                candle_low = float(candle['low'])
                
                # åˆ©ç¢ºãƒ©ã‚¤ãƒ³åˆ°é”ãƒã‚§ãƒƒã‚¯
                if candle_high >= tp_price:
                    exit_time = candle['timestamp']
                    exit_price = tp_price
                    is_success = True
                    return exit_time, exit_price, is_success
                
                # æåˆ‡ã‚Šãƒ©ã‚¤ãƒ³åˆ°é”ãƒã‚§ãƒƒã‚¯
                if candle_low <= sl_price:
                    exit_time = candle['timestamp']
                    exit_price = sl_price
                    is_success = False
                    return exit_time, exit_price, is_success
            
            # è©•ä¾¡æœŸé–“å†…ã«åˆ°é”ã—ãªã‹ã£ãŸå ´åˆ
            return None, None, None
            
        except Exception as e:
            logger.warning(f"TP/SLåˆ°é”åˆ¤å®šã‚¨ãƒ©ãƒ¼: {symbol} - {e}")
            return None, None, None
    
    def _get_fallback_exit_minutes(self, timeframe):
        """æ™‚é–“è¶³ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€€å‡ºæ™‚é–“ã‚’å–å¾—"""
        fallback_minutes = {
            '1m': 15,    # 15åˆ†å¾Œ
            '3m': 30,    # 30åˆ†å¾Œ
            '5m': 45,    # 45åˆ†å¾Œ
            '15m': 60,   # 1æ™‚é–“å¾Œ
            '30m': 90,   # 1.5æ™‚é–“å¾Œ
            '1h': 120    # 2æ™‚é–“å¾Œ
        }
        return fallback_minutes.get(timeframe, 60)

def generate_large_scale_configs(symbols_count=20, timeframes=4, configs=10):
    """å¤§è¦æ¨¡è¨­å®šã‚’ç”Ÿæˆ"""
    symbols = [f"TOKEN{i:03d}" for i in range(1, symbols_count + 1)]
    timeframes_list = ['1m', '3m', '5m', '15m', '30m', '1h'][:timeframes]
    configs_list = [f"Config_{i:02d}" for i in range(1, configs + 1)]
    
    batch_configs = []
    for symbol in symbols:
        for timeframe in timeframes_list:
            for config in configs_list:
                batch_configs.append({
                    'symbol': symbol,
                    'timeframe': timeframe,
                    'config': config
                })
    
    return batch_configs

def main():
    """å¤§è¦æ¨¡åˆ†æã®ãƒ‡ãƒ¢"""
    system = ScalableAnalysisSystem()
    
    # 1000ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨­å®šç”Ÿæˆ
    configs = generate_large_scale_configs(symbols_count=10, timeframes=4, configs=25)
    print(f"ç”Ÿæˆã•ã‚ŒãŸè¨­å®šæ•°: {len(configs)}")
    
    # ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ
    processed = system.generate_batch_analysis(configs, max_workers=4)
    print(f"å‡¦ç†å®Œäº†: {processed}ãƒ‘ã‚¿ãƒ¼ãƒ³")
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = system.get_statistics()
    print("\nçµ±è¨ˆ:")
    print(f"  ç·åˆ†ææ•°: {stats['performance']['total_analyses']}")
    print(f"  å¹³å‡Sharpe: {stats['performance']['avg_sharpe']:.2f}")
    print(f"  æœ€é«˜Sharpe: {stats['performance']['max_sharpe']:.2f}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯éŠ˜æŸ„æ•°: {stats['performance']['unique_symbols']}")
    
    # ä¸Šä½çµæœè¡¨ç¤º
    top_results = system.query_analyses(
        filters={'min_sharpe': 1.5},
        order_by='sharpe_ratio',
        limit=10
    )
    print(f"\nä¸Šä½10çµæœ:")
    print(top_results[['symbol', 'timeframe', 'config', 'sharpe_ratio', 'total_return']].to_string())

# ScalableAnalysisSystem ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
def add_get_timeframe_config_method():
    """ScalableAnalysisSystemã«è¨­å®šå–å¾—ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ """
    import types
    
    def get_timeframe_config(self, timeframe: str):
        """æ™‚é–“è¶³è¨­å®šã‚’å–å¾—ï¼ˆå¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
        try:
            from config.timeframe_config_manager import TimeframeConfigManager
            config_manager = TimeframeConfigManager()
            return config_manager.get_timeframe_config(timeframe)
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            default_configs = {
                '1m': {'max_evaluations': 100},
                '3m': {'max_evaluations': 80},
                '5m': {'max_evaluations': 120},
                '15m': {'max_evaluations': 100},
                '30m': {'max_evaluations': 80},
                '1h': {'max_evaluations': 100}
            }
            base_config = {
                'data_days': 90,
                'evaluation_interval_minutes': 240,
                'max_evaluations': 50,
                'min_leverage': 3.0,
                'min_confidence': 0.5,
                'min_risk_reward': 2.0
            }
            base_config.update(default_configs.get(timeframe, {}))
            return base_config
    
    # ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ 
    ScalableAnalysisSystem.get_timeframe_config = get_timeframe_config
    
    # ğŸ”§ å¤ã„é€²æ—ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã¯æ–°ã—ã„FileBasedProgressTrackerã«çµ±ä¸€æ¸ˆã¿
    # def _init_file_based_progress_tracker(self, execution_id: str, symbol: str):
    #     """æ—§: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹progress_trackeråˆæœŸåŒ–ï¼ˆFileBasedProgressTrackerã«çµ±åˆæ¸ˆã¿ï¼‰"""
    #     pass
    
    # def _update_file_based_progress_tracker(self, execution_id: str, completed_phase: str, next_phase: str):
    #     """æ—§: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹progress_trackeræ›´æ–°ï¼ˆFileBasedProgressTrackerã«çµ±åˆæ¸ˆã¿ï¼‰"""
    #     pass

# ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ ã‚’å®Ÿè¡Œ
add_get_timeframe_config_method()

if __name__ == "__main__":
    main()