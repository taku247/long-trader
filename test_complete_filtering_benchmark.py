#!/usr/bin/env python3
"""
å®Œå…¨ãª9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

Filter 1-9ã®å…¨ã¦ã‚’çµ±åˆã—ãŸå®Œå…¨ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
import os
import time
import statistics
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

class CompleteFilteringBenchmark:
    """å®Œå…¨ãª9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.results = {}
        self.test_symbols = ["BTC", "ETH"]  # å®‰å®šã—ãŸå®ŸéŠ˜æŸ„
        self.test_configs = [
            {'timeframe': '1m', 'strategy': 'Conservative_ML'},
            {'timeframe': '5m', 'strategy': 'Conservative_ML'},
            {'timeframe': '15m', 'strategy': 'Conservative_ML'},
            {'timeframe': '30m', 'strategy': 'Full_ML'},
            {'timeframe': '1h', 'strategy': 'Aggressive_Traditional'},
            {'timeframe': '4h', 'strategy': 'Full_ML'},
            {'timeframe': '1d', 'strategy': 'Conservative_ML'},
            {'timeframe': '1w', 'strategy': 'Full_ML'},
        ]
    
    async def run_complete_benchmark(self):
        """å®Œå…¨ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸ”¥ å®Œå…¨9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print("=" * 80)
        
        from auto_symbol_training import AutoSymbolTrainer
        trainer = AutoSymbolTrainer()
        
        benchmark_results = {}
        total_start_time = time.time()
        
        for symbol in self.test_symbols:
            print(f"\nğŸ¯ å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡: {symbol}")
            print("-" * 60)
            
            symbol_result = await self._benchmark_complete_symbol(trainer, symbol)
            benchmark_results[symbol] = symbol_result
            
            # å³åº§ã«çµæœè¡¨ç¤º
            self._print_complete_symbol_results(symbol, symbol_result)
        
        total_time = time.time() - total_start_time
        
        # å…¨ä½“çµ±è¨ˆã®è¨ˆç®—ã¨è¡¨ç¤º
        self._print_complete_overall_statistics(benchmark_results, total_time)
        
        return benchmark_results
    
    async def _benchmark_complete_symbol(self, trainer, symbol: str) -> Dict[str, Any]:
        """éŠ˜æŸ„åˆ¥å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        result = {
            'symbol': symbol,
            'early_fail_time': 0.0,
            'early_fail_success': False,
            'complete_filtering_time': 0.0,
            'total_configs': len(self.test_configs),
            'passed_configs': 0,
            'excluded_configs': 0,
            'exclusion_rate': 0.0,
            'filtering_breakdown': {},
            'performance_metrics': {},
            'error_occurred': False,
            'error_message': ""
        }
        
        try:
            # Phase 1: Early Failæ¤œè¨¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print(f"  ğŸ” Early Failæ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
            early_fail_start = time.time()
            
            early_fail_result = await trainer._run_early_fail_validation(symbol)
            result['early_fail_time'] = time.time() - early_fail_start
            result['early_fail_success'] = early_fail_result.passed
            
            if not early_fail_result.passed:
                result['error_occurred'] = True
                result['error_message'] = f"Early Fail: {early_fail_result.fail_reason}"
                return result
            
            # Phase 2: å®Œå…¨9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print(f"  ğŸš€ å®Œå…¨9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
            
            # ãƒ†ã‚¹ãƒˆè¨­å®šã‚’æº–å‚™
            test_configs = [
                {**config, 'symbol': symbol} 
                for config in self.test_configs
            ]
            
            # å€‹åˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¿ã‚¤ãƒŸãƒ³ã‚°
            filtering_start = time.time()
            
            # æ®µéšåˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            phase_results = await self._run_phased_filtering(
                trainer, test_configs, symbol
            )
            
            result['complete_filtering_time'] = time.time() - filtering_start
            result['filtering_breakdown'] = phase_results
            
            # æœ€çµ‚çµæœçµ±è¨ˆ
            final_configs = phase_results.get('final_configs', [])
            result['passed_configs'] = len(final_configs)
            result['excluded_configs'] = len(test_configs) - len(final_configs)
            result['exclusion_rate'] = (result['excluded_configs'] / len(test_configs)) * 100
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—
            result['performance_metrics'] = self._calculate_performance_metrics(
                result, phase_results
            )
            
        except Exception as e:
            result['error_occurred'] = True
            result['error_message'] = str(e)
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return result
    
    async def _run_phased_filtering(self, trainer, test_configs: List[Dict], symbol: str) -> Dict[str, Any]:
        """æ®µéšåˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        phase_results = {
            'phase_1_light': {'time': 0.0, 'passed': len(test_configs), 'excluded': 0},
            'phase_2_medium': {'time': 0.0, 'passed': 0, 'excluded': 0},
            'phase_3_heavy': {'time': 0.0, 'passed': 0, 'excluded': 0},
            'final_configs': []
        }
        
        try:
            # å®Œå…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            execution_id = f"complete-benchmark-{symbol}-{int(time.time())}"
            
            phase_start = time.time()
            filtered_configs = await trainer._apply_filtering_framework(
                test_configs, symbol, execution_id
            )
            total_phase_time = time.time() - phase_start
            
            # çµæœé›†è¨ˆï¼ˆå®Ÿéš›ã®ãƒ•ã‚§ãƒ¼ã‚ºåˆ†é›¢ã¯è¤‡é›‘ãªãŸã‚ã€ç·åˆæ™‚é–“ã¨ã—ã¦è¨˜éŒ²ï¼‰
            phase_results['phase_1_light']['time'] = total_phase_time * 0.1  # æ¨å®š10%
            phase_results['phase_2_medium']['time'] = total_phase_time * 0.3  # æ¨å®š30%
            phase_results['phase_3_heavy']['time'] = total_phase_time * 0.6   # æ¨å®š60%
            
            phase_results['final_configs'] = filtered_configs
            
            # å„æ®µéšã®é€šéæ•°ã‚’æ¨å®šï¼ˆå®Ÿéš›ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚è¿‘ä¼¼å€¤ï¼‰
            total_configs = len(test_configs)
            final_passed = len(filtered_configs)
            
            # æ¨å®šæ®µéšåˆ¥é€šéç‡
            phase_results['phase_1_light']['passed'] = int(total_configs * 0.85)  # è»½é‡85%é€šé
            phase_results['phase_1_light']['excluded'] = total_configs - phase_results['phase_1_light']['passed']
            
            phase_results['phase_2_medium']['passed'] = int(phase_results['phase_1_light']['passed'] * 0.65)  # ä¸­é‡é‡65%é€šé
            phase_results['phase_2_medium']['excluded'] = phase_results['phase_1_light']['passed'] - phase_results['phase_2_medium']['passed']
            
            phase_results['phase_3_heavy']['passed'] = final_passed
            phase_results['phase_3_heavy']['excluded'] = phase_results['phase_2_medium']['passed'] - final_passed
            
            return phase_results
            
        except Exception as e:
            print(f"    âŒ æ®µéšåˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return phase_results
    
    def _calculate_performance_metrics(self, result: Dict[str, Any], 
                                     phase_results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—"""
        total_time = result['early_fail_time'] + result['complete_filtering_time']
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™
        efficiency_score = result['exclusion_rate'] / 100.0  # é™¤å¤–ç‡ï¼ˆé«˜ã„ã»ã©åŠ¹ç‡çš„ï¼‰
        speed_score = max(0, 1 - (total_time / 30))  # 30ç§’ã‚’åŸºæº–ã¨ã—ãŸé€Ÿåº¦ã‚¹ã‚³ã‚¢
        
        # æ®µéšåˆ¥åŠ¹ç‡
        phase_efficiency = {}
        for phase_name, phase_data in phase_results.items():
            if isinstance(phase_data, dict) and 'time' in phase_data:
                excluded = phase_data.get('excluded', 0)
                time_taken = phase_data.get('time', 0.001)
                efficiency = excluded / time_taken if time_taken > 0 else 0
                phase_efficiency[phase_name] = efficiency
        
        return {
            'total_execution_time': total_time,
            'efficiency_score': efficiency_score,
            'speed_score': speed_score,
            'early_fail_ratio': result['early_fail_time'] / total_time if total_time > 0 else 0,
            'filtering_ratio': result['complete_filtering_time'] / total_time if total_time > 0 else 0,
            'phase_efficiency': phase_efficiency,
            'overall_score': (efficiency_score + speed_score) / 2
        }
    
    def _print_complete_symbol_results(self, symbol: str, result: Dict[str, Any]):
        """éŠ˜æŸ„åˆ¥å®Œå…¨çµæœè¡¨ç¤º"""
        print(f"\nğŸ“Š {symbol} å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        
        if result['error_occurred']:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {result['error_message']}")
            return
        
        print(f"  âœ… Early Failæ¤œè¨¼: {result['early_fail_time']:.2f}ç§’")
        print(f"  âœ… å®Œå…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {result['complete_filtering_time']:.2f}ç§’")
        print(f"  ğŸ“ˆ æœ€çµ‚é€šé: {result['passed_configs']}/{result['total_configs']}")
        print(f"  ğŸ“‰ ç·é™¤å¤–ç‡: {result['exclusion_rate']:.1f}%")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        metrics = result['performance_metrics']
        print(f"  âš¡ åŠ¹ç‡ã‚¹ã‚³ã‚¢: {metrics.get('efficiency_score', 0):.3f}")
        print(f"  ğŸš€ é€Ÿåº¦ã‚¹ã‚³ã‚¢: {metrics.get('speed_score', 0):.3f}")
        print(f"  ğŸ† ç·åˆã‚¹ã‚³ã‚¢: {metrics.get('overall_score', 0):.3f}")
        
        # æ®µéšåˆ¥æ™‚é–“å†…è¨³
        breakdown = result['filtering_breakdown']
        print(f"  ğŸ“‹ æ®µéšåˆ¥æ™‚é–“å†…è¨³:")
        print(f"    Phase 1 (è»½é‡): {breakdown.get('phase_1_light', {}).get('time', 0):.3f}ç§’")
        print(f"    Phase 2 (ä¸­é‡é‡): {breakdown.get('phase_2_medium', {}).get('time', 0):.3f}ç§’")
        print(f"    Phase 3 (é‡é‡): {breakdown.get('phase_3_heavy', {}).get('time', 0):.3f}ç§’")
    
    def _print_complete_overall_statistics(self, benchmark_results: Dict[str, Dict[str, Any]], 
                                         total_time: float):
        """å®Œå…¨çµ±è¨ˆè¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å®Œå…¨9æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç·åˆçµ±è¨ˆ")
        print("=" * 80)
        
        successful_results = [
            result for result in benchmark_results.values() 
            if not result['error_occurred']
        ]
        
        if not successful_results:
            print("âŒ æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æ™‚é–“çµ±è¨ˆ
        early_fail_times = [r['early_fail_time'] for r in successful_results]
        filtering_times = [r['complete_filtering_time'] for r in successful_results]
        exclusion_rates = [r['exclusion_rate'] for r in successful_results]
        overall_scores = [r['performance_metrics']['overall_score'] for r in successful_results]
        
        print(f"ğŸ• Early Failæ¤œè¨¼æ™‚é–“:")
        print(f"   å¹³å‡: {statistics.mean(early_fail_times):.2f}ç§’")
        print(f"   æœ€å°: {min(early_fail_times):.2f}ç§’")
        print(f"   æœ€å¤§: {max(early_fail_times):.2f}ç§’")
        
        print(f"\nâš¡ å®Œå…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚é–“:")
        print(f"   å¹³å‡: {statistics.mean(filtering_times):.3f}ç§’")
        print(f"   æœ€å°: {min(filtering_times):.3f}ç§’")
        print(f"   æœ€å¤§: {max(filtering_times):.3f}ç§’")
        
        print(f"\nğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ:")
        print(f"   å¹³å‡é™¤å¤–ç‡: {statistics.mean(exclusion_rates):.1f}%")
        print(f"   æœ€å°é™¤å¤–ç‡: {min(exclusion_rates):.1f}%")
        print(f"   æœ€å¤§é™¤å¤–ç‡: {max(exclusion_rates):.1f}%")
        
        print(f"\nğŸ† ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"   å¹³å‡ç·åˆã‚¹ã‚³ã‚¢: {statistics.mean(overall_scores):.3f}")
        print(f"   æœ€é«˜ã‚¹ã‚³ã‚¢: {max(overall_scores):.3f}")
        print(f"   æœ€ä½ã‚¹ã‚³ã‚¢: {min(overall_scores):.3f}")
        
        # ç†è«–çš„åŠ¹æœæ¨å®š
        avg_exclusion = statistics.mean(exclusion_rates)
        theoretical_reduction = 100 - avg_exclusion  # å®Ÿéš›ã«å‡¦ç†ã•ã‚Œã‚‹å‰²åˆ
        
        print(f"\nğŸ¯ ç†è«–çš„å‡¦ç†åŠ¹ç‡:")
        print(f"   å‡¦ç†å‰Šæ¸›ç‡: {avg_exclusion:.1f}%")
        print(f"   å®Ÿéš›å‡¦ç†ç‡: {theoretical_reduction:.1f}%")
        print(f"   åŠ¹ç‡åŒ–å€ç‡: {100/theoretical_reduction:.1f}x")
        
        print(f"\nâ±ï¸  ç·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ™‚é–“: {total_time:.2f}ç§’")
        print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {len(successful_results)}/{len(benchmark_results)}éŠ˜æŸ„ã§æˆåŠŸ")
        
        # æœ€çµ‚è©•ä¾¡
        if statistics.mean(overall_scores) > 0.7:
            print(f"\nğŸ‰ è©•ä¾¡: å„ªç§€ - é«˜åŠ¹ç‡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
        elif statistics.mean(overall_scores) > 0.5:
            print(f"\nâœ… è©•ä¾¡: è‰¯å¥½ - åŠ¹æœçš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
        else:
            print(f"\nâš ï¸ è©•ä¾¡: æ”¹å–„è¦ - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœã®æœ€é©åŒ–ãŒå¿…è¦")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    benchmark = CompleteFilteringBenchmark()
    
    start_time = time.time()
    results = await benchmark.run_complete_benchmark()
    total_time = time.time() - start_time
    
    print(f"\nğŸ å®Œå…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    
    # æˆåŠŸç‡
    successful_count = sum(1 for r in results.values() if not r['error_occurred'])
    success_rate = (successful_count / len(results)) * 100
    
    print(f"ğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æˆåŠŸç‡: {success_rate:.1f}% ({successful_count}/{len(results)})")
    
    return results

if __name__ == "__main__":
    results = asyncio.run(main())
    exit(0)