#!/usr/bin/env python3
"""
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœã®å®šé‡è©•ä¾¡
"""

import asyncio
import sys
import os
import time
import statistics
from datetime import datetime
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

class FilteringBenchmark:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.results = {}
        self.test_symbols = ["BTC", "ETH", "SOL"]  # å®‰å…¨ãªå®ŸéŠ˜æŸ„
        self.test_configs = [
            {'timeframe': '1m', 'strategy': 'Conservative_ML'},
            {'timeframe': '5m', 'strategy': 'Conservative_ML'},
            {'timeframe': '15m', 'strategy': 'Conservative_ML'},
            {'timeframe': '30m', 'strategy': 'Full_ML'},
            {'timeframe': '1h', 'strategy': 'Aggressive_Traditional'},
            {'timeframe': '4h', 'strategy': 'Full_ML'},
            {'timeframe': '1d', 'strategy': 'Conservative_ML'},
        ]
    
    async def run_benchmark(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸ”¥ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 80)
        
        from auto_symbol_training import AutoSymbolTrainer
        trainer = AutoSymbolTrainer()
        
        benchmark_results = {}
        
        for symbol in self.test_symbols:
            print(f"\nğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡: {symbol}")
            print("-" * 50)
            
            symbol_result = await self._benchmark_symbol(trainer, symbol)
            benchmark_results[symbol] = symbol_result
            
            # å³åº§ã«çµæœè¡¨ç¤º
            self._print_symbol_results(symbol, symbol_result)
        
        # å…¨ä½“çµ±è¨ˆã®è¨ˆç®—ã¨è¡¨ç¤º
        self._print_overall_statistics(benchmark_results)
        
        return benchmark_results
    
    async def _benchmark_symbol(self, trainer, symbol: str) -> Dict[str, Any]:
        """éŠ˜æŸ„åˆ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        result = {
            'symbol': symbol,
            'early_fail_time': 0.0,
            'early_fail_success': False,
            'filtering_time': 0.0,
            'total_configs': len(self.test_configs),
            'passed_configs': 0,
            'excluded_configs': 0,
            'exclusion_rate': 0.0,
            'error_occurred': False,
            'error_message': ""
        }
        
        try:
            # Phase 1: Early Failæ¤œè¨¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print(f"  ğŸ” Early Failæ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
            early_fail_start = time.time()
            
            early_fail_result = await trainer._run_early_fail_validation(symbol)
            result['early_fail_time'] = time.time() - early_fail_start
            result['early_fail_success'] = early_fail_result.passed
            
            if not early_fail_result.passed:
                result['error_occurred'] = True
                result['error_message'] = f"Early Fail: {early_fail_result.fail_reason}"
                return result
            
            # Phase 2: FilteringFrameworkã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print(f"  ğŸš€ FilteringFrameworkäº‹å‰æ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯...")
            
            # ãƒ†ã‚¹ãƒˆè¨­å®šã‚’æº–å‚™
            test_configs = [
                {**config, 'symbol': symbol} 
                for config in self.test_configs
            ]
            
            filtering_start = time.time()
            filtered_configs = await trainer._apply_filtering_framework(
                test_configs, symbol, f"benchmark-{symbol}-{int(time.time())}"
            )
            result['filtering_time'] = time.time() - filtering_start
            
            result['passed_configs'] = len(filtered_configs)
            result['excluded_configs'] = len(test_configs) - len(filtered_configs)
            result['exclusion_rate'] = (result['excluded_configs'] / len(test_configs)) * 100
            
        except Exception as e:
            result['error_occurred'] = True
            result['error_message'] = str(e)
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return result
    
    def _print_symbol_results(self, symbol: str, result: Dict[str, Any]):
        """éŠ˜æŸ„åˆ¥çµæœè¡¨ç¤º"""
        print(f"\nğŸ“Š {symbol} ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        
        if result['error_occurred']:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {result['error_message']}")
            return
        
        print(f"  âœ… Early Failæ¤œè¨¼: {result['early_fail_time']:.2f}ç§’")
        print(f"  âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {result['filtering_time']:.2f}ç§’")
        print(f"  ğŸ“ˆ é€šéè¨­å®š: {result['passed_configs']}/{result['total_configs']}")
        print(f"  ğŸ“‰ é™¤å¤–ç‡: {result['exclusion_rate']:.1f}%")
        print(f"  ğŸ’° æ¨å®šç¯€ç´„: ç´„{result['exclusion_rate']:.0f}%ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹")
    
    def _print_overall_statistics(self, benchmark_results: Dict[str, Dict[str, Any]]):
        """å…¨ä½“çµ±è¨ˆè¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å…¨ä½“çµ±è¨ˆ")
        print("=" * 80)
        
        successful_results = [
            result for result in benchmark_results.values() 
            if not result['error_occurred']
        ]
        
        if not successful_results:
            print("âŒ æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æ™‚é–“çµ±è¨ˆ
        early_fail_times = [r['early_fail_time'] for r in successful_results]
        filtering_times = [r['filtering_time'] for r in successful_results]
        exclusion_rates = [r['exclusion_rate'] for r in successful_results]
        
        print(f"ğŸ• Early Failæ¤œè¨¼æ™‚é–“:")
        print(f"   å¹³å‡: {statistics.mean(early_fail_times):.2f}ç§’")
        print(f"   æœ€å°: {min(early_fail_times):.2f}ç§’")
        print(f"   æœ€å¤§: {max(early_fail_times):.2f}ç§’")
        
        print(f"\nâš¡ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ™‚é–“:")
        print(f"   å¹³å‡: {statistics.mean(filtering_times):.3f}ç§’")
        print(f"   æœ€å°: {min(filtering_times):.3f}ç§’")
        print(f"   æœ€å¤§: {max(filtering_times):.3f}ç§’")
        
        print(f"\nğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ:")
        print(f"   å¹³å‡é™¤å¤–ç‡: {statistics.mean(exclusion_rates):.1f}%")
        print(f"   æœ€å°é™¤å¤–ç‡: {min(exclusion_rates):.1f}%")
        print(f"   æœ€å¤§é™¤å¤–ç‡: {max(exclusion_rates):.1f}%")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        avg_exclusion = statistics.mean(exclusion_rates)
        avg_filtering_time = statistics.mean(filtering_times)
        
        print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡:")
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ: {'å„ªç§€' if avg_exclusion > 20 else 'è‰¯å¥½' if avg_exclusion > 10 else 'è¦æ”¹å–„'}")
        print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é€Ÿåº¦: {'é«˜é€Ÿ' if avg_filtering_time < 0.1 else 'æ™®é€š' if avg_filtering_time < 1.0 else 'æ”¹å–„å¿…è¦'}")
        
        # æ¨å®šå‰Šæ¸›åŠ¹æœ
        if avg_exclusion > 0:
            estimated_time_savings = f"ç´„{avg_exclusion:.0f}%ã®å‡¦ç†æ™‚é–“å‰Šæ¸›"
            estimated_resource_savings = f"ç´„{avg_exclusion:.0f}%ã®ãƒªã‚½ãƒ¼ã‚¹ç¯€ç´„"
            print(f"   æ¨å®šåŠ¹æœ: {estimated_time_savings}")
            print(f"   ãƒªã‚½ãƒ¼ã‚¹åŠ¹æœ: {estimated_resource_savings}")
        
        print(f"\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {len(successful_results)}/{len(benchmark_results)}éŠ˜æŸ„ã§æˆåŠŸ")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    benchmark = FilteringBenchmark()
    
    start_time = time.time()
    results = await benchmark.run_benchmark()
    total_time = time.time() - start_time
    
    print(f"\nğŸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")
    
    # æˆåŠŸç‡
    successful_count = sum(1 for r in results.values() if not r['error_occurred'])
    success_rate = (successful_count / len(results)) * 100
    
    print(f"ğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æˆåŠŸç‡: {success_rate:.1f}% ({successful_count}/{len(results)})")
    
    return results

if __name__ == "__main__":
    results = asyncio.run(main())
    exit(0)