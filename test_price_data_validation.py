#!/usr/bin/env python3
"""
ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ä¾¡æ ¼æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤ã‚„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã‚’æ¤œå‡º

æ©Ÿèƒ½:
1. å…¨éŠ˜æŸ„ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¾¡æ ¼æ¤œè¨¼
2. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤æ¤œå‡º (100.0, 105.0, 97.62ç­‰)
3. ä¾¡æ ¼å¤‰å‹•ã®ç•°å¸¸æ¤œå‡º
4. å®Ÿéš›ã®APIä¾¡æ ¼ã¨ã®æ¯”è¼ƒ
5. ã‚¨ãƒ³ãƒˆãƒªãƒ¼/TP/SLä¾¡æ ¼ã®è«–ç†æ€§ãƒã‚§ãƒƒã‚¯
"""

import pickle
import gzip
import pandas as pd
import numpy as np
from pathlib import Path
import json
import asyncio
from datetime import datetime, timezone, timedelta
import argparse
from typing import Dict, List, Tuple, Any, Optional

class PriceDataValidator:
    """ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.compressed_dir = Path("large_scale_analysis/compressed")
        self.known_hardcoded_values = [100.0, 105.0, 97.62, 100.00, 105.00, 97.620]
        self.tolerance_pct = 0.001  # 0.1% - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤æ¤œå‡ºã®è¨±å®¹ç¯„å›²
        self.test_results = []
        
    def load_all_symbol_data(self) -> Dict[str, List[Tuple[str, str, str]]]:
        """å…¨éŠ˜æŸ„ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        symbol_files = {}
        
        if not self.compressed_dir.exists():
            print(f"âš ï¸  åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.compressed_dir}")
            return symbol_files
        
        for file_path in self.compressed_dir.glob("*.pkl.gz"):
            parts = file_path.stem.replace('.pkl', '').split('_')
            if len(parts) >= 3:
                symbol = parts[0]
                timeframe = parts[1]
                config = '_'.join(parts[2:])
                
                if symbol not in symbol_files:
                    symbol_files[symbol] = []
                symbol_files[symbol].append((timeframe, config, str(file_path)))
        
        return symbol_files
    
    def load_trade_data(self, file_path: str) -> Optional[pd.DataFrame]:
        """ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with gzip.open(file_path, 'rb') as f:
                trades_data = pickle.load(f)
            
            if isinstance(trades_data, list):
                return pd.DataFrame(trades_data)
            elif isinstance(trades_data, dict):
                return pd.DataFrame(trades_data)
            else:
                return trades_data
                
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return None
    
    def detect_hardcoded_values(self, df: pd.DataFrame, column: str) -> Dict[str, Any]:
        """ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤ã‚’æ¤œå‡º"""
        if column not in df.columns:
            return {'detected': False, 'reason': f'Column {column} not found'}
        
        values = pd.to_numeric(df[column], errors='coerce').dropna()
        if len(values) == 0:
            return {'detected': False, 'reason': 'No numeric values'}
        
        # æ—¢çŸ¥ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤ã‚’ãƒã‚§ãƒƒã‚¯
        hardcoded_detections = []
        for hardcoded_val in self.known_hardcoded_values:
            matching_count = sum(abs(val - hardcoded_val) / hardcoded_val < self.tolerance_pct for val in values)
            if matching_count > 0:
                hardcoded_detections.append({
                    'value': hardcoded_val,
                    'count': matching_count,
                    'percentage': matching_count / len(values) * 100
                })
        
        # çµ±è¨ˆçš„ç•°å¸¸æ¤œå‡º
        unique_count = len(values.unique())
        std_dev = values.std()
        mean_val = values.mean()
        
        anomalies = []
        
        # 1. å›ºæœ‰å€¤ãŒæ¥µç«¯ã«å°‘ãªã„
        if unique_count <= 3 and len(values) > 10:
            anomalies.append(f"å›ºæœ‰å€¤ãŒ{unique_count}å€‹ã®ã¿ ({len(values)}å€‹ä¸­)")
        
        # 2. æ¨™æº–åå·®ãŒæ¥µç«¯ã«å°ã•ã„
        if std_dev < mean_val * 0.001:  # 0.1%æœªæº€ã®å¤‰å‹•
            anomalies.append(f"å¤‰å‹•ãŒæ¥µå° (std={std_dev:.6f}, mean={mean_val:.6f})")
        
        # 3. åŒä¸€å€¤ãŒ50%ä»¥ä¸Š
        value_counts = values.value_counts()
        most_common_pct = value_counts.iloc[0] / len(values) * 100
        if most_common_pct > 50:
            anomalies.append(f"åŒä¸€å€¤ãŒ{most_common_pct:.1f}%ã‚’å ã‚ã‚‹ (å€¤: {value_counts.index[0]})")
        
        return {
            'detected': len(hardcoded_detections) > 0 or len(anomalies) > 0,
            'hardcoded_values': hardcoded_detections,
            'statistical_anomalies': anomalies,
            'stats': {
                'count': len(values),
                'unique_count': unique_count,
                'mean': float(mean_val),
                'std': float(std_dev),
                'min': float(values.min()),
                'max': float(values.max()),
                'sample_values': values.head(10).tolist()
            }
        }
    
    def validate_price_logic(self, df: pd.DataFrame) -> List[str]:
        """ä¾¡æ ¼ã®è«–ç†æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []
        
        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ãŒã‚ã‚‹å ´åˆã®ãƒã‚§ãƒƒã‚¯
        if 'entry_price' in df.columns and 'take_profit_price' in df.columns and 'stop_loss_price' in df.columns:
            entry_prices = pd.to_numeric(df['entry_price'], errors='coerce').dropna()
            tp_prices = pd.to_numeric(df['take_profit_price'], errors='coerce').dropna()
            sl_prices = pd.to_numeric(df['stop_loss_price'], errors='coerce').dropna()
            
            if len(entry_prices) > 0 and len(tp_prices) > 0 and len(sl_prices) > 0:
                # ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã®è«–ç†ãƒã‚§ãƒƒã‚¯
                invalid_tp = sum(tp <= entry for tp, entry in zip(tp_prices, entry_prices))
                invalid_sl = sum(sl >= entry for sl, entry in zip(sl_prices, entry_prices))
                
                if invalid_tp > 0:
                    issues.append(f"åˆ©ç¢ºä¾¡æ ¼ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ä»¥ä¸‹: {invalid_tp}ä»¶")
                
                if invalid_sl > 0:
                    issues.append(f"æåˆ‡ä¾¡æ ¼ãŒã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼ä»¥ä¸Š: {invalid_sl}ä»¶")
                
                # ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¯”ã®ç•°å¸¸
                rr_ratios = [(tp - entry) / (entry - sl) for tp, entry, sl in zip(tp_prices, entry_prices, sl_prices) if entry > sl]
                if rr_ratios:
                    avg_rr = np.mean(rr_ratios)
                    if avg_rr < 0.5:
                        issues.append(f"ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¯”ãŒä½ã„ (å¹³å‡: {avg_rr:.2f})")
                    elif avg_rr > 10:
                        issues.append(f"ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰æ¯”ãŒç•°å¸¸ã«é«˜ã„ (å¹³å‡: {avg_rr:.2f})")
        
        return issues
    
    async def get_current_market_price(self, symbol: str) -> Optional[float]:
        """ç¾åœ¨ã®å¸‚å ´ä¾¡æ ¼ã‚’å–å¾—"""
        try:
            from hyperliquid_api_client import MultiExchangeAPIClient
            
            client = MultiExchangeAPIClient()
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=1)
            
            data = await client.get_ohlcv_data(symbol, '1h', start_time, end_time)
            if not data.empty:
                return float(data['close'].iloc[-1])
        except Exception as e:
            print(f"å¸‚å ´ä¾¡æ ¼å–å¾—ã‚¨ãƒ©ãƒ¼ ({symbol}): {e}")
        
        return None
    
    def validate_symbol_data(self, symbol: str, file_configs: List[Tuple[str, str, str]]) -> Dict[str, Any]:
        """éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
        print(f"\nğŸ” {symbol} ã®æ¤œè¨¼é–‹å§‹...")
        
        symbol_result = {
            'symbol': symbol,
            'total_configs': len(file_configs),
            'config_results': [],
            'overall_issues': [],
            'current_price': None
        }
        
        # ç¾åœ¨ä¾¡æ ¼ã‚’å–å¾—
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            current_price = loop.run_until_complete(self.get_current_market_price(symbol))
            symbol_result['current_price'] = current_price
            loop.close()
            
            if current_price:
                print(f"  ğŸ“Š ç¾åœ¨ä¾¡æ ¼: {current_price:.6f}")
        except Exception as e:
            print(f"  âš ï¸  ç¾åœ¨ä¾¡æ ¼å–å¾—å¤±æ•—: {e}")
        
        total_anomalies = 0
        
        for timeframe, config, file_path in file_configs:
            print(f"  ğŸ“ {timeframe}_{config} ã‚’æ¤œè¨¼ä¸­...")
            
            df = self.load_trade_data(file_path)
            if df is None or df.empty:
                symbol_result['config_results'].append({
                    'timeframe': timeframe,
                    'config': config,
                    'status': 'failed',
                    'error': 'ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—'
                })
                continue
            
            config_result = {
                'timeframe': timeframe,
                'config': config,
                'total_trades': len(df),
                'price_validations': {},
                'logic_issues': [],
                'status': 'checked'
            }
            
            # ä¾¡æ ¼é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’æ¤œè¨¼
            price_columns = [col for col in df.columns 
                           if any(keyword in col.lower() for keyword in 
                                  ['price', 'entry', 'exit', 'profit', 'loss', 'target', 'stop'])]
            
            for col in price_columns:
                validation = self.detect_hardcoded_values(df, col)
                config_result['price_validations'][col] = validation
                
                if validation['detected']:
                    total_anomalies += 1
                    print(f"    âš ï¸  {col}: ç•°å¸¸æ¤œå‡º")
                    
                    if validation['hardcoded_values']:
                        for hv in validation['hardcoded_values']:
                            print(f"      - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤ {hv['value']}: {hv['count']}ä»¶ ({hv['percentage']:.1f}%)")
                    
                    for anomaly in validation['statistical_anomalies']:
                        print(f"      - {anomaly}")
                
                # ç¾åœ¨ä¾¡æ ¼ã¨ã®æ¯”è¼ƒ
                if current_price and validation.get('stats', {}).get('count', 0) > 0:
                    mean_price = validation['stats']['mean']
                    price_diff_pct = abs(mean_price - current_price) / current_price * 100
                    if price_diff_pct > 20:  # 20%ä»¥ä¸Šã®å·®
                        print(f"    ğŸ“ˆ {col}: ç¾åœ¨ä¾¡æ ¼ã¨ã®ä¹–é›¢ {price_diff_pct:.1f}% (å¹³å‡: {mean_price:.6f})")
            
            # ä¾¡æ ¼è«–ç†ãƒã‚§ãƒƒã‚¯
            logic_issues = self.validate_price_logic(df)
            config_result['logic_issues'] = logic_issues
            
            for issue in logic_issues:
                print(f"    âŒ è«–ç†ã‚¨ãƒ©ãƒ¼: {issue}")
            
            symbol_result['config_results'].append(config_result)
        
        # å…¨ä½“çš„ãªå•é¡Œã‚’ã¾ã¨ã‚
        if total_anomalies > 0:
            symbol_result['overall_issues'].append(f"ä¾¡æ ¼ç•°å¸¸ã‚’{total_anomalies}ä»¶æ¤œå‡º")
        
        print(f"  âœ… {symbol} æ¤œè¨¼å®Œäº†: {total_anomalies}ä»¶ã®ç•°å¸¸")
        return symbol_result
    
    def run_comprehensive_validation(self, target_symbols: Optional[List[str]] = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        print("ğŸ” Long Trader ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚¤ãƒ¼ãƒˆ")
        print("=" * 60)
        
        all_symbol_files = self.load_all_symbol_data()
        
        if not all_symbol_files:
            return {'error': 'æ¤œè¨¼å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
        
        # å¯¾è±¡éŠ˜æŸ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿
        if target_symbols:
            all_symbol_files = {s: f for s, f in all_symbol_files.items() 
                              if s.upper() in [t.upper() for t in target_symbols]}
        
        print(f"ğŸ“Š æ¤œè¨¼å¯¾è±¡: {len(all_symbol_files)}éŠ˜æŸ„")
        for symbol, configs in all_symbol_files.items():
            print(f"  - {symbol}: {len(configs)}è¨­å®š")
        
        validation_results = {
            'validation_timestamp': datetime.now(timezone.utc).isoformat(),
            'total_symbols': len(all_symbol_files),
            'symbol_results': [],
            'summary': {
                'symbols_with_issues': 0,
                'total_anomalies': 0,
                'critical_issues': []
            }
        }
        
        # å„éŠ˜æŸ„ã‚’æ¤œè¨¼
        for symbol, file_configs in all_symbol_files.items():
            symbol_result = self.validate_symbol_data(symbol, file_configs)
            validation_results['symbol_results'].append(symbol_result)
            
            # ã‚µãƒãƒªãƒ¼æ›´æ–°
            if symbol_result['overall_issues']:
                validation_results['summary']['symbols_with_issues'] += 1
            
            # å„è¨­å®šã®ç•°å¸¸ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for config_result in symbol_result['config_results']:
                for col, validation in config_result.get('price_validations', {}).items():
                    if validation['detected']:
                        validation_results['summary']['total_anomalies'] += 1
                        
                        # é‡è¦ãªç•°å¸¸ã‚’è¨˜éŒ²
                        if validation['hardcoded_values']:
                            for hv in validation['hardcoded_values']:
                                if hv['percentage'] > 50:  # 50%ä»¥ä¸ŠãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤
                                    validation_results['summary']['critical_issues'].append({
                                        'symbol': symbol,
                                        'timeframe': config_result['timeframe'],
                                        'config': config_result['config'],
                                        'column': col,
                                        'issue': f"ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤{hv['value']}ãŒ{hv['percentage']:.1f}%"
                                    })
        
        return validation_results
    
    def generate_validation_report(self, results: Dict[str, Any]) -> str:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ” Long Trader ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 80)
        report.append(f"æ¤œè¨¼æ—¥æ™‚: {results['validation_timestamp']}")
        report.append(f"å¯¾è±¡éŠ˜æŸ„æ•°: {results['total_symbols']}")
        report.append("")
        
        # ã‚µãƒãƒªãƒ¼
        summary = results['summary']
        report.append("ğŸ“Š æ¤œè¨¼ã‚µãƒãƒªãƒ¼")
        report.append("-" * 40)
        report.append(f"å•é¡Œã®ã‚ã‚‹éŠ˜æŸ„: {summary['symbols_with_issues']}/{results['total_symbols']}")
        report.append(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸: {summary['total_anomalies']}ä»¶")
        report.append(f"é‡è¦ãªå•é¡Œ: {len(summary['critical_issues'])}ä»¶")
        report.append("")
        
        # é‡è¦ãªå•é¡Œã®è©³ç´°
        if summary['critical_issues']:
            report.append("ğŸš¨ é‡è¦ãªå•é¡Œ")
            report.append("-" * 40)
            for issue in summary['critical_issues']:
                report.append(f"âŒ {issue['symbol']} {issue['timeframe']}_{issue['config']}")
                report.append(f"   {issue['column']}: {issue['issue']}")
            report.append("")
        
        # éŠ˜æŸ„åˆ¥è©³ç´°
        report.append("ğŸ“‹ éŠ˜æŸ„åˆ¥è©³ç´°")
        report.append("-" * 40)
        
        for symbol_result in results['symbol_results']:
            symbol = symbol_result['symbol']
            issues = len(symbol_result['overall_issues'])
            current_price = symbol_result.get('current_price')
            
            status_icon = "âœ…" if issues == 0 else "âš ï¸" 
            report.append(f"{status_icon} {symbol}")
            
            if current_price:
                report.append(f"   ç¾åœ¨ä¾¡æ ¼: {current_price:.6f}")
            
            if symbol_result['overall_issues']:
                for issue in symbol_result['overall_issues']:
                    report.append(f"   å•é¡Œ: {issue}")
            
            # å„è¨­å®šã®è©³ç´°
            for config_result in symbol_result['config_results']:
                config_name = f"{config_result['timeframe']}_{config_result['config']}"
                total_issues = sum(1 for v in config_result.get('price_validations', {}).values() if v['detected'])
                total_issues += len(config_result.get('logic_issues', []))
                
                if total_issues > 0:
                    report.append(f"   ğŸ“ {config_name}: {total_issues}ä»¶ã®å•é¡Œ")
                    
                    # ä¾¡æ ¼æ¤œè¨¼çµæœ
                    for col, validation in config_result.get('price_validations', {}).items():
                        if validation['detected']:
                            report.append(f"     - {col}:")
                            for hv in validation.get('hardcoded_values', []):
                                report.append(f"       ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤{hv['value']}: {hv['count']}ä»¶")
                            for anomaly in validation.get('statistical_anomalies', []):
                                report.append(f"       {anomaly}")
                    
                    # è«–ç†ã‚¨ãƒ©ãƒ¼
                    for logic_issue in config_result.get('logic_issues', []):
                        report.append(f"     - è«–ç†ã‚¨ãƒ©ãƒ¼: {logic_issue}")
            
            report.append("")
        
        # æ¨å¥¨äº‹é …
        report.append("ğŸ’¡ æ¨å¥¨äº‹é …")
        report.append("-" * 40)
        
        if summary['total_anomalies'] > 0:
            report.append("1. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å€¤ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’ç‰¹å®šã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆ")
            report.append("2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ã‚’å‰Šé™¤ã—ã€å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’ç™ºç”Ÿ")
            report.append("3. ä¾¡æ ¼è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¦‹ç›´ã—ã€å‹•çš„ãªä¾¡æ ¼ç”Ÿæˆã‚’ç¢ºä¿")
            report.append("4. ã‚¨ãƒ³ãƒˆãƒªãƒ¼/TP/SLä¾¡æ ¼ã®è¨ˆç®—å¼ã‚’ç¾åœ¨ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã«ä¿®æ­£")
        else:
            report.append("âœ… ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã«å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='Long Trader ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼')
    parser.add_argument('--symbols', nargs='*', help='æ¤œè¨¼å¯¾è±¡éŠ˜æŸ„ (æŒ‡å®šãªã—ã§å…¨éŠ˜æŸ„)')
    parser.add_argument('--output', help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--report-only', action='store_true', help='ãƒ¬ãƒãƒ¼ãƒˆã®ã¿ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    validator = PriceDataValidator()
    
    # æ¤œè¨¼å®Ÿè¡Œ
    print("ğŸš€ ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼é–‹å§‹...")
    results = validator.run_comprehensive_validation(args.symbols)
    
    if 'error' in results:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {results['error']}")
        return 1
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = validator.generate_validation_report(results)
    print("\n" + report)
    
    # çµæœä¿å­˜
    output_file = args.output or f"price_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    report_file = output_file.replace('.json', '_report.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ è©³ç´°çµæœ: {output_file}")
    print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    critical_issues = len(results['summary']['critical_issues'])
    if critical_issues > 0:
        print(f"\nâš ï¸  {critical_issues}ä»¶ã®é‡è¦ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        return 1
    else:
        print(f"\nâœ… æ¤œè¨¼å®Œäº†")
        return 0

if __name__ == "__main__":
    exit(main())